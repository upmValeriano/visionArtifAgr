
<!DOCTYPE html>


<html lang="en" >

  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.18.1: http://docutils.sourceforge.net/" />

    <title>Maqueta de red convolucional &#8212; visionArtifAgr</title>
  
  
  
  <script data-cfasync="false">
    document.documentElement.dataset.mode = localStorage.getItem("mode") || "";
    document.documentElement.dataset.theme = localStorage.getItem("theme") || "light";
  </script>
  
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/bootstrap.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
<link href="_static/styles/pydata-sphinx-theme.css?digest=365ca57ee442770a23c6" rel="stylesheet" />

  
  <link href="_static/vendor/fontawesome/6.1.2/css/all.min.css?digest=365ca57ee442770a23c6" rel="stylesheet" />
  <link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-solid-900.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-brands-400.woff2" />
<link rel="preload" as="font" type="font/woff2" crossorigin href="_static/vendor/fontawesome/6.1.2/webfonts/fa-regular-400.woff2" />

    <link rel="stylesheet" type="text/css" href="_static/pygments.css?v=b76e3c8a" />
    <link rel="stylesheet" type="text/css" href="_static/styles/sphinx-book-theme.css?v=384b581d" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css?v=13237357" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css?v=76b2166b" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.4510f1fc1dee50b3e5859aac5469c37c29e427902b24a333a5f9fcb2f0b3ac41.css?v=be8a1c11" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css?v=4fa983c6" />
    <link rel="stylesheet" type="text/css" href="_static/proof.css?v=ca93fcec" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.1e8bd061cd6da7fc9cf755528e8ffc24.min.css?v=0a3b3ea7" />
  
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6" />
<link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6" />
  <script src="_static/vendor/fontawesome/6.1.2/js/all.min.js?digest=365ca57ee442770a23c6"></script>

    <script src="_static/documentation_options.js?v=9eb32ce0"></script>
    <script src="_static/doctools.js?v=888ff710"></script>
    <script src="_static/sphinx_highlight.js?v=dc90522c"></script>
    <script src="_static/clipboard.min.js?v=a7894cd8"></script>
    <script src="_static/copybutton.js?v=f281be69"></script>
    <script src="_static/scripts/sphinx-book-theme.js?v=efea14e4"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js?v=4a39c7ea"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script src="_static/design-tabs.js?v=36754332"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script async="async" src="_static/sphinx-thebe.js?v=c100c467"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown';</script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"; const thebe_selector = ".thebe,.cell"; const thebe_selector_input = "pre"; const thebe_selector_output = ".output, .cell_output"</script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <script>DOCUMENTATION_OPTIONS.pagename = '03.9_RedesNeurConvolucional_Maqueta';</script>
    <link rel="icon" href="_static/EscUpm.jpg"/>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <link rel="next" title="PlantVillage. Cultivos con patologías" href="03.10_RedesNeurConv_PlantVillage.html" />
    <link rel="prev" title="Redes convoluciones con Pytorch" href="03.8_RedesNeurConvolucional_pytorch.html" />
  <meta name="viewport" content="width=device-width, initial-scale=1"/>
  <meta name="docsearch:language" content="en"/>
  </head>
  
  
  <body data-bs-spy="scroll" data-bs-target=".bd-toc-nav" data-offset="180" data-bs-root-margin="0px 0px -60%" data-default-mode="">

  
  
  <a class="skip-link" href="#main-content">Skip to main content</a>
  
  <div id="pst-scroll-pixel-helper"></div>

  
  <button type="button" class="btn rounded-pill" id="pst-back-to-top">
    <i class="fa-solid fa-arrow-up"></i>
    Back to top
  </button>

  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__primary"
          id="__primary"/>
  <label class="overlay overlay-primary" for="__primary"></label>
  
  <input type="checkbox"
          class="sidebar-toggle"
          name="__secondary"
          id="__secondary"/>
  <label class="overlay overlay-secondary" for="__secondary"></label>
  
  <div class="search-button__wrapper">
    <div class="search-button__overlay"></div>
    <div class="search-button__search-container">
<form class="bd-search d-flex align-items-center"
      action="search.html"
      method="get">
  <i class="fa-solid fa-magnifying-glass"></i>
  <input type="search"
         class="form-control"
         name="q"
         id="search-input"
         placeholder="Search this book..."
         aria-label="Search this book..."
         autocomplete="off"
         autocorrect="off"
         autocapitalize="off"
         spellcheck="false"/>
  <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd>K</kbd></span>
</form></div>
  </div>
  
    <nav class="bd-header navbar navbar-expand-lg bd-navbar">
    </nav>
  
  <div class="bd-container">
    <div class="bd-container__inner bd-page-width">
      
      <div class="bd-sidebar-primary bd-sidebar">
        

  
  <div class="sidebar-header-items sidebar-primary__section">
    
    
    
    
  </div>
  
    <div class="sidebar-primary-items__start sidebar-primary__section">
        <div class="sidebar-primary-item">
  

<a class="navbar-brand logo" href="introAA.html">
  
  
  
  
  
    
    
      
    
    
    <img src="_static/logoETSIAAB.png" class="logo__image only-light" alt="visionArtifAgr - Home"/>
    <script>document.write(`<img src="_static/logoETSIAAB.png" class="logo__image only-dark" alt="visionArtifAgr - Home"/>`);</script>
  
  
</a></div>
        <div class="sidebar-primary-item">
 <script>
 document.write(`
   <button class="btn navbar-btn search-button-field search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass"></i>
    <span class="search-button__default-text">Search</span>
    <span class="search-button__kbd-shortcut"><kbd class="kbd-shortcut__modifier">Ctrl</kbd>+<kbd class="kbd-shortcut__modifier">K</kbd></span>
   </button>
 `);
 </script></div>
        <div class="sidebar-primary-item"><nav class="bd-links bd-docs-nav" aria-label="Main">
    <div class="bd-toc-item navbar-nav active">
        
        <ul class="nav bd-sidenav bd-sidenav__home-link">
            <li class="toctree-l1">
                <a class="reference internal" href="introAA.html">
                    Bienvenida
                </a>
            </li>
        </ul>
        <ul class="current nav bd-sidenav">
<li class="toctree-l1 has-children"><a class="reference internal" href="01_IntroduccionIntro.html">Presentación</a><input class="toctree-checkbox" id="toctree-checkbox-1" name="toctree-checkbox-1" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-1"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="01.1_InstalacionJupyter.html">Instalación de Python y Cuadernos Jupyter</a></li>
<li class="toctree-l2"><a class="reference internal" href="01.2_EjemplosdePythonparaaprenderaprogramar.html">Repaso de programación en Python</a></li>
<li class="toctree-l2"><a class="reference internal" href="01.3_InstalacionLibrerias.html">Instalación de Librerías</a></li>
</ul>
</li>
<li class="toctree-l1 has-children"><a class="reference internal" href="02_ClasificacionIntro.html">Clasificación</a><input class="toctree-checkbox" id="toctree-checkbox-2" name="toctree-checkbox-2" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-2"><i class="fa-solid fa-chevron-down"></i></label><ul>
<li class="toctree-l2"><a class="reference internal" href="02.1_Introduccion_AA.html">El Aprendizaje Automático</a></li>
<li class="toctree-l2"><a class="reference internal" href="02.2_MetodosdeClasificacion_Ordinarios.html">Métodos de clasificación</a></li>
<li class="toctree-l2"><a class="reference internal" href="02.3_Clasificacion-EjercicioEntregaWINES.html">Ejercicio con entrega sobre variedades vínicolas</a></li>
</ul>
</li>
<li class="toctree-l1 current active has-children"><a class="reference internal" href="03_rnnIntro.html">Redes Neuronales</a><input checked="" class="toctree-checkbox" id="toctree-checkbox-3" name="toctree-checkbox-3" type="checkbox"/><label class="toctree-toggle" for="toctree-checkbox-3"><i class="fa-solid fa-chevron-down"></i></label><ul class="current">
<li class="toctree-l2"><a class="reference internal" href="03.1_RedesNeurDensas_Bicapa.html">Red densa simple</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.2_RedesNeurDensas_Bicapa_sklearn.html">Red densa simple. Perceptron de sk-learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.3_RedesNeurDensas_Multicapa.html">Red densa multicapa</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.4_RedesNeurDensas_Lote.html">Red densa. Procesamiento en lote</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.5_RedesNeurDensas_Multicapa_sklearn.html">Red densa multicapa. MLPClassifier de sk-learn</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.6_RedesNeurDensas_EjercicioEntregaWINES.html">Ejercicio con entrega sobre variedades vínicolas</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.7_RedesNeurConvolucional.html">Redes convolucionales</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.8_RedesNeurConvolucional_pytorch.html">Redes convoluciones con Pytorch</a></li>
<li class="toctree-l2 current active"><a class="current reference internal" href="#">Maqueta de red convolucional</a></li>
<li class="toctree-l2"><a class="reference internal" href="03.10_RedesNeurConv_PlantVillage.html">PlantVillage. Cultivos con patologías</a></li>
</ul>
</li>
<li class="toctree-l1"><a class="reference internal" href="ymas.html">Y más …</a></li>
<li class="toctree-l1"><a class="reference internal" href="Bibliografia.html">Bibliografía</a></li>
</ul>

    </div>
</nav></div>
    </div>
  
  
  <div class="sidebar-primary-items__end sidebar-primary__section">
  </div>
  
  <div id="rtd-footer-container"></div>


      </div>
      
      <main id="main-content" class="bd-main">
        
        

<div class="sbt-scroll-pixel-helper"></div>

          <div class="bd-content">
            <div class="bd-article-container">
              
              <div class="bd-header-article">
<div class="header-article-items header-article__inner">
  
    <div class="header-article-items__start">
      
        <div class="header-article-item"><label class="sidebar-toggle primary-toggle btn btn-sm" for="__primary" title="Toggle primary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
  <span class="fa-solid fa-bars"></span>
</label></div>
      
    </div>
  
  
    <div class="header-article-items__end">
      
        <div class="header-article-item">

<div class="article-header-buttons">





<div class="dropdown dropdown-launch-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Launch interactive content">
    <i class="fas fa-rocket"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://colab.research.google.com/github/upmValeriano/visionArtifAgr/blob/main/docs/03.9_RedesNeurConvolucional_Maqueta.ipynb" target="_blank"
   class="btn btn-sm dropdown-item"
   title="Launch onColab"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  
    <img src="_static/images/logo_colab.png">
  </span>
<span class="btn__text-container">Colab</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-source-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Source repositories">
    <i class="fab fa-github"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="https://github.com/upmValeriano/visionArtifAgr" target="_blank"
   class="btn btn-sm btn-source-repository-button dropdown-item"
   title="Source repository"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="btn__text-container">Repository</span>
</a>
</li>
      
      
      
      
      <li><a href="https://github.com/upmValeriano/visionArtifAgr/issues/new?title=Issue%20on%20page%20%2F03.9_RedesNeurConvolucional_Maqueta.html&body=Your%20issue%20content%20here." target="_blank"
   class="btn btn-sm btn-source-issues-button dropdown-item"
   title="Open an issue"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="btn__text-container">Open issue</span>
</a>
</li>
      
  </ul>
</div>






<div class="dropdown dropdown-download-buttons">
  <button class="btn dropdown-toggle" type="button" data-bs-toggle="dropdown" aria-expanded="false" aria-label="Download this page">
    <i class="fas fa-download"></i>
  </button>
  <ul class="dropdown-menu">
      
      
      
      <li><a href="_sources/03.9_RedesNeurConvolucional_Maqueta.ipynb" target="_blank"
   class="btn btn-sm btn-download-source-button dropdown-item"
   title="Download source file"
   data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file"></i>
  </span>
<span class="btn__text-container">.ipynb</span>
</a>
</li>
      
      
      
      
      <li>
<button onclick="window.print()"
  class="btn btn-sm btn-download-pdf-button dropdown-item"
  title="Print to PDF"
  data-bs-placement="left" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-file-pdf"></i>
  </span>
<span class="btn__text-container">.pdf</span>
</button>
</li>
      
  </ul>
</div>




<button onclick="toggleFullScreen()"
  class="btn btn-sm btn-fullscreen-button"
  title="Fullscreen mode"
  data-bs-placement="bottom" data-bs-toggle="tooltip"
>
  

<span class="btn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>


<script>
document.write(`
  <button class="btn btn-sm navbar-btn theme-switch-button" title="light/dark" aria-label="light/dark" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="theme-switch nav-link" data-mode="light"><i class="fa-solid fa-sun fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="dark"><i class="fa-solid fa-moon fa-lg"></i></span>
    <span class="theme-switch nav-link" data-mode="auto"><i class="fa-solid fa-circle-half-stroke fa-lg"></i></span>
  </button>
`);
</script>

<script>
document.write(`
  <button class="btn btn-sm navbar-btn search-button search-button__button" title="Search" aria-label="Search" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <i class="fa-solid fa-magnifying-glass fa-lg"></i>
  </button>
`);
</script>
<label class="sidebar-toggle secondary-toggle btn btn-sm" for="__secondary"title="Toggle secondary sidebar" data-bs-placement="bottom" data-bs-toggle="tooltip">
    <span class="fa-solid fa-list"></span>
</label>
</div></div>
      
    </div>
  
</div>
</div>
              
              

<div id="jb-print-docs-body" class="onlyprint">
    <h1>Maqueta de red convolucional</h1>
    <!-- Table of contents -->
    <div id="print-main-content">
        <div id="jb-print-toc">
            
            <div>
                <h2> Contents </h2>
            </div>
            <nav aria-label="Page">
                <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-convolucionales">Redes convolucionales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamento-teorico">Fundamento teórico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-proceso-de-retropropagacion">El proceso de retropropagación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#justificacion-de-la-retropropagacion-en-convolucion">Justificación de la retropropagación en convolución</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-y-cross-entropy">Softmax y cross-entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizacion-adam">Optimización ADAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#funciones-generales">Funciones generales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-densa">Clase para una capa densa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-convolucional">Clase para una capa convolucional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-de-agrupacion-por-maximo-max-pooling">Clase para una capa de agrupación por máximo (max-pooling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-de-aplanado">Clase de aplanado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-red-neuronal-convolucional">Clase red neuronal convolucional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-integral-con-la-arquitectura-lenet-5-y-el-conjunto-mnist">Prueba Integral con la arquitectura LeNet-5 y el conjunto MNIST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#se-define-el-modelo">Se define el modelo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#se-realiza-el-entrenamiento">Se realiza el entrenamiento</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serializacion-del-modelo-input-output-segun-estado">Serialización del modelo (input/output según estado)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-del-ratio-de-precision-en-el-conjunto-de-entrenamiento">Cálculo del ratio de precisión en el conjunto de entrenamiento</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejecucion-en-modo-perceptron">Ejecución en modo Perceptron</a></li>
</ul>
            </nav>
        </div>
    </div>
</div>

              
                
<div id="searchbox"></div>
                <article class="bd-article" role="main">
                  
  <section class="tex2jax_ignore mathjax_ignore" id="maqueta-de-red-convolucional">
<h1>Maqueta de red convolucional<a class="headerlink" href="#maqueta-de-red-convolucional" title="Link to this heading">#</a></h1>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">numpy</span> <span class="k">as</span> <span class="nn">np</span>
<span class="kn">from</span> <span class="nn">tqdm</span> <span class="kn">import</span> <span class="n">tqdm</span> 
<span class="kn">import</span> <span class="nn">pickle</span>
<span class="kn">import</span> <span class="nn">logging</span>
<span class="kn">from</span> <span class="nn">scipy</span> <span class="kn">import</span> <span class="n">signal</span> <span class="k">as</span> <span class="n">sg</span>
</pre></div>
</div>
</div>
</div>
<p><strong>Configuración del proceso</strong></p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">entrenamiento</span><span class="o">=</span><span class="kc">False</span>
</pre></div>
</div>
</div>
</div>
<section id="redes-convolucionales">
<h2>Redes convolucionales<a class="headerlink" href="#redes-convolucionales" title="Link to this heading">#</a></h2>
<a class="reference internal image-reference" href="_images/Densa_vs_ConvNet.png"><img alt="_images/Densa_vs_ConvNet.png" class="align-center" src="_images/Densa_vs_ConvNet.png" style="width: 800px;" /></a>
<p><strong>Izquierda</strong>: Una red neuronal normal de 3 capas. <strong>Derecha</strong>: Un ConvNet organiza sus neuronas en tres dimensiones (ancho, alto, profundidad), como se visualiza en una de las capas. Cada capa de un ConvNet transforma el volumen de entrada 3D en un volumen de salida 3D de activaciones neuronales. En este ejemplo, la capa de entrada roja contiene la imagen, por lo que su ancho y alto serían las dimensiones de la imagen, y la profundidad sería de 3 (canales rojo, verde, azul).</p>
<p>Una <strong>Arquitectura Convolucional</strong> tipo estará compuesta por las capas: [INPUT - CONV - RELU - POOL - FC]. Cuyo detalle es:</p>
<ul class="simple">
<li><p><strong>INPUT</strong> [bxhx3] contendrá los valores de píxeles raw de la imagen, en este caso una imagen de ancho b, altura h, y con tres canales de color R, G, B.</p></li>
<li><p>La capa <strong>CONV</strong> calculará la salida de las neuronas que están conectadas a las regiones locales en la entrada, cada una calculando un producto de punto entre sus pesos y una pequeña región a la que están conectadas en el volumen de entrada. Esto puede resultar en un volumen como [BxHxd] si decidimos usar d filtros. Los pesos que conectan los pixels localmente son compartidos y son <strong>aprendibles</strong>.</p></li>
<li><p>La capa <strong>RELU</strong> aplicará una función de activación por elementos, como el umbral <span class="math notranslate nohighlight">\((max(0,x))\)</span> en cero. Esto deja el tamaño del volumen sin cambios ([BxHxd]).</p></li>
<li><p>La capa <strong>POOL</strong> realizará una operación de <strong>downsampling</strong> a lo largo de las dimensiones espaciales (ancho, alto), lo que dará como resultado un volumen como [bxhxd], siendo (b,h) inferiores a (B,H). Esta capa no utiliza pesos aprendibles.</p></li>
<li><p>La capa <strong>FC o densa</strong>  (es decir, totalmente conectada) calculará los puntajes de clase, lo que resultará en un volumen de tamaño [1x1xK], donde cada uno de los K números corresponde a un puntaje de clase, entre las K categorías del conjunto. Al igual que con las redes neuronales ordinarias y como su nombre indica, cada neurona en esta capa estará conectada a todos los números en el volumen anterior.</p></li>
</ul>
</section>
<section id="fundamento-teorico">
<h2>Fundamento teórico<a class="headerlink" href="#fundamento-teorico" title="Link to this heading">#</a></h2>
<p>Una red neuronal se puede considerar (LeCun et al.; 1998) un sistema construido como una cascada de módulos, cada uno de los cuales implementa una función</p>
<div class="math notranslate nohighlight">
\[X_l = F_l(W_l, X_{l-1})\]</div>
<p>Se toma <span class="math notranslate nohighlight">\(l\)</span> como el índicador de número de capa (layer en inglés).</p>
<p>Dónde <span class="math notranslate nohighlight">\(X_l\)</span> es un vector que representa la salida del módulo, <span class="math notranslate nohighlight">\(W_l\)</span> es el vector de los parámetros entrenables del módulo (y que forma parte del conjunto total <span class="math notranslate nohighlight">\(W\)</span>) y <span class="math notranslate nohighlight">\(X_{l-1}\)</span> es el vector de entrada al módulo (así como la salida del módulo previo).</p>
<p>Se ha implementado una maqueta naíf que implementa 4 tipos de capas: densa o perceptrón, convolucional, agrupación máxima (max-pooling) y aplanado. La capa densa con una programación matricial en numpy se implementa de acuerdo a la documentación previa. La capa convolucional se restringe a convoluciones 2D sin relleno a ceros y con salto 1. La agrupación máxima usa un paso único en ambas direcciones de la imagen. El aplanado es un simple cambio de formato para pasar de las capas convolucionales a las densas.</p>
<p>Al final de la última capa se aplica una función softmax para obtener las probabilidades de cada clase y la pérdida se calcula con una función de croos-entropy.</p>
<p>También se <strong>incorpora la optimación Adam</strong> que provee un ratio de aprendizaje modulado (<span id="id1">[<a class="reference internal" href="Bibliografia.html#id9" title="Diederik P Kingma and Jimmy Ba. Adam: a method for stochastic optimization. arXiv preprint arXiv:1412.6980, 2014.">Kingma and Ba, 2014</a>]</span>).</p>
<p>El vector de datos de entrada <span class="math notranslate nohighlight">\(X_0\)</span> va a ser un tensor o matriz de 4 dimensiones <strong>(n, c, h, b)</strong>, donde <strong>n</strong> representará el número de observaciones en el lote tratado, <strong>c</strong> el número de canales, <strong>h</strong> es la altura de la imagen y <strong>b</strong> es el ancho de la imagen.</p>
<p>Esta estructura de los vectores <span class="math notranslate nohighlight">\(X_l\)</span> se mantiene durante los procesos convolucionales y de agrupación, mientras que las capas densas manejan tensores de la forma <strong>(n, m)</strong>, siendo <strong>n</strong> el número de observaciones tratadas y <strong>m</strong> las características de cada observación. Lógicamente, la capa de aplanado efectua reestructuración de las dimensiones con <strong>m = c * h * b</strong>.</p>
<p>En una capa convolucional los parámetros entrenables están formados por un filtro <span class="math notranslate nohighlight">\(W_l\)</span> cuyas dimensiones son <strong>(co, ci, f, f)</strong>, donde <strong>co</strong> es el número de canales de salida (los canales del tensor <span class="math notranslate nohighlight">\(X_l\)</span>), <strong>ci</strong> es el número de canales de entrada (los canales del tensor <span class="math notranslate nohighlight">\(X_{l-1}\)</span>) y <strong>f</strong> es el tamaño del filtro. Además se incluye un parámetro de bias <span class="math notranslate nohighlight">\(B_l\)</span>, que es un vector de dimensiones <strong>(co)</strong>.</p>
<p>Las capas de agrupación y aplanado no tienen parámetros entrenables.</p>
<p>Las capas densas tienen una matriz entrenable <span class="math notranslate nohighlight">\(W_l\)</span> de dos dimensiones <strong>(m,p)</strong>, siendo <strong>m</strong> el nº de carácteristicas de la salida de la capa (el vector <span class="math notranslate nohighlight">\(X_l\)</span>) y <strong>p</strong> el número de características de la entrada a la capa (el vector <span class="math notranslate nohighlight">\(X_{l-1}\)</span>)</p>
</section>
<section id="el-proceso-de-retropropagacion">
<h2>El proceso de retropropagación<a class="headerlink" href="#el-proceso-de-retropropagacion" title="Link to this heading">#</a></h2>
<p>En cuadernos anteriores se ha justificado el proceso de retropropagación de las capas densas que se incluye en la maqueta. Aquí se va a justificar las fórmulas usadas a partir de un ejemplo teórico de dimensión reducida en las condiciones de la convolución expuestas. A continuación se muestra un gráfico del proceso de entrenamiento de un lote de 32 imágenes con la arquitectura LetNet5. En el se indican los pasos forward de cada una de las capas, el final de proceso donde se obtiene las probabilidades de clase con la función softmax y el cálculo de la función de pérdida a partir de la entropía cruzada (<span class="math notranslate nohighlight">\(C\)</span>).</p>
<p>En este proceso final se obtiene el grandiente del coste con respecto a la combinación lineal de la última capa (<span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z ^L}\)</span>), este gradiente que en el gráfico y en las rutinas programadas utiliza la nomeclatura <strong>dA</strong> comparte la dimensión con los tensores <strong>Z</strong> y <strong>A</strong> de cada capa. El primer proceso de retropropagación (<strong>BAC1</strong> en el esquema) arranca con <strong>dA8</strong> que es el gradiente de coste de la capa 8. Hay que entender <strong>Softmax</strong> como una activación específica. Por eso la entrada a esta función es <strong>Z8</strong> la combinación lineal de la última capa. La salida de la función softmax para una observación <strong>x</strong> es un vector probabilidad <strong>p</strong> con la probabilidad de cada etiqueta. Para todo el lote es una matriz <strong>P</strong> donde cada fila es la probabilidad por observación. Más adelante comprobaremos que este gradiente de la capa final cuando se usa <strong>softmax</strong> unido a la función de coste de entropia cruzada resulta algo tan sintético como:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial Z ^L} = P - Y\]</div>
<p>Siendo <span class="math notranslate nohighlight">\(P = softmax(Z^L)\)</span> y <span class="math notranslate nohighlight">\(Y\)</span> una matriz <strong>one-hot</strong> de las etiquetas reales.</p>
<p>Por tanto los procesos de back-propagación siempre tienen la misma estructura:</p>
<ul class="simple">
<li><p>Partiendo del gradiente de coste con respecto a la ponderación lineal de su capa (<span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z ^l}\)</span>), obtener o propagar el de la capa anterior (<span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z ^{l-1}}\)</span>).</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial Z ^l} \rightarrow \frac{\partial C}{\partial Z ^{l-1}}\]</div>
<ul class="simple">
<li><p>Obtiener el gradiente del coste con respecto a los pesos y bias de su capa, siempre y cuando sean capas entrenables:</p></li>
</ul>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial W ^l}; \frac{\partial C}{\partial B ^l}\]</div>
<a class="reference internal image-reference" href="_images/Esquema_FWDBAC.png"><img alt="_images/Esquema_FWDBAC.png" class="align-center" src="_images/Esquema_FWDBAC.png" style="width: 800px;" /></a>
</section>
<section id="justificacion-de-la-retropropagacion-en-convolucion">
<h2>Justificación de la retropropagación en convolución<a class="headerlink" href="#justificacion-de-la-retropropagacion-en-convolucion" title="Link to this heading">#</a></h2>
<p>Para realizar esta justificación suponemos una única observación y un único canal <strong>(n = c = co = ci = 1)</strong>.</p>
<p>Además para simplificar las fórmulas al máximo se toma en <span class="math notranslate nohighlight">\(X_{n-1}\)</span> <strong>h=b=4</strong> en la entrada a la convolución y debido al filtro aplicado la salida se reduce en <span class="math notranslate nohighlight">\(X_n\)</span> a <strong>h=b=2</strong> (por no usar relleno a ceros). Además el tamaño del filtro es <strong>f=3</strong>.</p>
<p>Dada una capa convolucional <span class="math notranslate nohighlight">\(l\)</span> cuya entrada suponemos definida por una matriz</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
  x_{11} &amp; x_{12} &amp; x_{13} &amp; x_{14} \\
  x_{21} &amp; x_{22} &amp; x_{23} &amp; x_{24} \\
  x_{31} &amp; x_{32} &amp; x_{33} &amp; x_{34} \\
  x_{41} &amp; x_{42} &amp; x_{43} &amp; x_{44} \\
 \end{bmatrix}
\end{split}\]</div>
<p>Esta matriz puede estar representando los datos de una observación de la capa de entrada a la red neuronal o la salida de la activación de la capa <span class="math notranslate nohighlight">\(l-1\)</span>, que en su caso vendrá notada por <span class="math notranslate nohighlight">\(A^{l-1}\)</span>.</p>
<p>En la capa <span class="math notranslate nohighlight">\(l\)</span> tenemos definida una convolución simple (suponemos un canal de entrada y uno de salida) que requiere una matriz de pesos o filtro <span class="math notranslate nohighlight">\(W^l\)</span> y una matriz de bias <span class="math notranslate nohighlight">\(B^l\)</span>, suponemos un filtro 3x3 y dado que suponemos un sólo canal el bias es una matriz unidimensional:</p>
<div class="math notranslate nohighlight">
\[\begin{split}W^l = \begin{bmatrix}
  w_{11} &amp; w_{12} &amp; w_{13} \\
  w_{21} &amp; w_{22} &amp; w_{23} \\
  w_{31} &amp; w_{32} &amp; w_{33} \\
 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[B^l = \begin{bmatrix}
  b
 \end{bmatrix}
\]</div>
<p>Por comodidad sólo se usa superindice con el orden de la capa en las matrices, pero en los elementos de las matrices no se utiliza superíndice, si no sólo el subindice correspondiente. Así se nota la matriz <span class="math notranslate nohighlight">\(W^l\)</span> y sus elementos <span class="math notranslate nohighlight">\(w_{ij}\)</span> en lugar de <span class="math notranslate nohighlight">\(w^l_{ij}\)</span> por no complicar más la notación.</p>
<p>La matriz <span class="math notranslate nohighlight">\(Z^l = W^l \otimes A^{l-1} \oplus B^l\)</span>, resultado de aplicar el filtro convolucional y sumar el bias, viene dada por:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
x_{11} \cdot w_{11} + x_{12} \cdot w_{12}+x_{13} \cdot w_{13}+x_{21} \cdot w_{21}+x_{22} \cdot w_{22}+x_{23} \cdot w_{23}+x_{31} \cdot w_{31}+x_{32} \cdot w_{32}+x_{33} \cdot w_{33}+b &amp;
x_{12} \cdot w_{11} + x_{13} \cdot w_{12} + x_{14} \cdot w_{13} + x_{22} \cdot w_{21} + x_{23} \cdot w_{22} + x_{24} \cdot w_{23} + x_{32} \cdot w_{31} + x_{33} \cdot w_{32} + x_{34} \cdot w_{33} + b \\
x_{21} \cdot w_{11} + x_{22} \cdot w_{12} + x_{23} \cdot w_{13} + x_{31} \cdot w_{21} + x_{32} \cdot w_{22} + x_{33} \cdot w_{23} + x_{41} \cdot w_{31} + x_{42} \cdot w_{32} + x_{43} \cdot w_{33} + b &amp;
x_{22} \cdot w_{11} + x_{23} \cdot w_{12} + x_{24} \cdot w_{13} + x_{32} \cdot w_{21} + x_{33} \cdot w_{22} + x_{34} \cdot w_{23} + x_{42} \cdot w_{31} + x_{43} \cdot w_{32} + x_{44} \cdot w_{33} + b
 \end{bmatrix}
\end{split}\]</div>
<p>Si se aplica la función de activación resulta <span class="math notranslate nohighlight">\(A^l = f(Z^l) = \begin{bmatrix} a_{11} &amp; a_{12} \\ a_{21}  &amp; a_{22} \end{bmatrix} \)</span></p>
<p>En el proceso de retropropagación, al procesar la capa <span class="math notranslate nohighlight">\(l+1\)</span> se habrá retropropagado una matriz con el gradiente del coste con relación a la ponderación lineal <span class="math notranslate nohighlight">\(Z^l\)</span>, con igual dimensión que <span class="math notranslate nohighlight">\(Z^l\)</span> y <span class="math notranslate nohighlight">\(A^l\)</span> y que se nota:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial C}{\partial Z^l} = \begin{bmatrix} \delta_{11}  &amp; \delta_{12}  \\ \delta_{21}  &amp; \delta_{22} \end{bmatrix} =  
\begin{bmatrix} \frac{\partial C}{\partial z_{11}}  &amp; \frac{\partial C}{\partial z_{12}}  \\ \frac{\partial C}{\partial z_{21}}  &amp; \frac{\partial C}{\partial z_{22}} \end{bmatrix}
\end{split}\]</div>
<p>El objetivo del proceso de retropropagación de la capa <span class="math notranslate nohighlight">\(l\)</span> es obtener el gradiente del coste en relación a los pesos <span class="math notranslate nohighlight">\(W^l\)</span> y del bias <span class="math notranslate nohighlight">\(B^l\)</span>, estos gradientes tiene igual dimensión que <span class="math notranslate nohighlight">\(W^l\)</span> y <span class="math notranslate nohighlight">\(B^l\)</span>. Se observa en la notación que es una <strong>matriz jacobiana</strong> con las derivadas primeras del coste con respecto a cada peso:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial C}{\partial W^l} = \begin{bmatrix}
  \frac{\partial C}{\partial w_{11}} &amp; \frac{\partial C}{\partial w_{12}} &amp; \frac{\partial C}{\partial w_{13}} \\
  \frac{\partial C}{\partial w_{21}} &amp; \frac{\partial C}{\partial w_{22}} &amp; \frac{\partial C}{\partial w_{23}} \\
  \frac{\partial C}{\partial w_{31}} &amp; \frac{\partial C}{\partial w_{32}} &amp; \frac{\partial C}{\partial w_{33}} \\
 \end{bmatrix}
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial B^l} = \begin{bmatrix}
    \frac{\partial C}{\partial b}
  \end{bmatrix}
\]</div>
<p>Por la regla de la cadena, el término <span class="math notranslate nohighlight">\( \frac{\partial C}{\partial w_{11}}\)</span> se obtiene:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial w_{11}} = \frac{\partial C}{\partial a_{11}} \frac{\partial a_{11}}{\partial w_{11}} +
   \frac{\partial C}{\partial a_{12}} \frac{\partial a_{12}}{\partial w_{11}} + 
   \frac{\partial C}{\partial a_{21}} \frac{\partial a_{21}}{\partial w_{11}} + 
   \frac{\partial C}{\partial a_{22}} \frac{\partial a_{22}}{\partial w_{11}} 
\]</div>
<p>Como <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial a_{ij}} = \delta_{ij}\)</span>, la matriz <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial W^l}\)</span>, dónde el resultado anterior aparece en el término 1,1 es:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
  \delta_{11} \cdot x_{11} + \delta_{12} \cdot x_{12} + \delta_{21} \cdot x_{21} + \delta_{22} \cdot x_{22} &amp;
  \delta_{11} \cdot x_{12} + \delta_{12} \cdot x_{13} + \delta_{21} \cdot x_{22} + \delta_{22} \cdot x_{23} &amp;
  \delta_{11} \cdot x_{13} + \delta_{12} \cdot x_{14} + \delta_{21} \cdot x_{23} + \delta_{22} \cdot x_{24} \\
  \delta_{11} \cdot x_{21} + \delta_{12} \cdot x_{22} + \delta_{21} \cdot x_{31} + \delta_{22} \cdot x_{32} &amp;
  \delta_{11} \cdot x_{22} + \delta_{12} \cdot x_{23} + \delta_{21} \cdot x_{32} + \delta_{22} \cdot x_{33} &amp;
  \delta_{11} \cdot x_{23} + \delta_{12} \cdot x_{24} + \delta_{21} \cdot x_{33} + \delta_{22} \cdot x_{34} \\
  \delta_{11} \cdot x_{31} + \delta_{12} \cdot x_{32} + \delta_{21} \cdot x_{41} + \delta_{22} \cdot x_{42} &amp;
  \delta_{11} \cdot x_{32} + \delta_{12} \cdot x_{33} + \delta_{21} \cdot x_{42} + \delta_{22} \cdot x_{43} &amp;
  \delta_{11} \cdot x_{33} + \delta_{12} \cdot x_{34} + \delta_{21} \cdot x_{43} + \delta_{22} \cdot x_{44}
  \end{bmatrix}
\end{split}\]</div>
<p>El gradiente del coste con respecto a <span class="math notranslate nohighlight">\(B^l\)</span>, usando la regla de la cadena resulta:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial b} = \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial b} +
   \frac{\partial C}{\partial z_{12}} \frac{\partial z_{12}}{\partial b} + 
   \frac{\partial C}{\partial z_{21}} \frac{\partial z_{21}}{\partial b} + 
   \frac{\partial C}{\partial z_{22}} \frac{\partial z_{22}}{\partial b} 
\]</div>
<p>Y como <span class="math notranslate nohighlight">\(\frac{\partial a_{ij}}{\partial b} = 1\)</span> se tiene:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial B^l} = \delta_{11} + \delta_{12} + \delta_{21} + \delta_{22} \]</div>
<p><strong>Se puede observar</strong> que la matriz <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial W^l}\)</span> se construye recorriendo uno a uno los elementos de <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z^l}\)</span> a la vez que nos movemos usando las dimensiones del filtro <span class="math notranslate nohighlight">\(W^l\)</span> sobre la matriz <span class="math notranslate nohighlight">\(A^{l-1}\)</span> y los subproductos escalares entre <span class="math notranslate nohighlight">\(\delta_{ij}\)</span> y las submatrices (3x3) se van acumulando en <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial W^l}\)</span>. El primer subproducto se ve en la siguiente imagen:</p>
<a class="reference internal image-reference" href="_images/conv_gradW.png"><img alt="_images/conv_gradW.png" class="align-center" src="_images/conv_gradW.png" style="width: 300px;" /></a>
<p>Y el siguiente subproducto sería:</p>
<a class="reference internal image-reference" href="_images/conv_gradW2.png"><img alt="_images/conv_gradW2.png" class="align-center" src="_images/conv_gradW2.png" style="width: 300px;" /></a>
<p>El código programado aparece en la función <strong>backward</strong> de la clase <strong>conv2DLayer</strong> (ajustando la notación):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>             dW[co, :] += Aprev[i, :, _h:_h+f, _b:_b+f] * Delta[i, co, _h,_b]
</pre></div>
</div>
<p>La última operación del proceso de retropropagación de la capa <span class="math notranslate nohighlight">\(l\)</span> consiste en obtener la matriz jacobiana</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial Z^{l-1}} \]</div>
<p>Que es de la misma dimensión que <span class="math notranslate nohighlight">\(A^{l-1}\)</span>, en este ejemplo (4x4), siendo el elemento <span class="math notranslate nohighlight">\((i,j)\)</span> el resultado de obtener</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial x_{ij}} \]</div>
<p>Por la regla de la cadena, el término <span class="math notranslate nohighlight">\( \frac{\partial C}{\partial x_{11}}\)</span> se obtiene:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial x_{11}} = \frac{\partial C}{\partial z_{11}} \frac{\partial z_{11}}{\partial x_{11}} +
   \frac{\partial C}{\partial z_{12}} \frac{\partial z_{12}}{\partial x_{11}} + 
   \frac{\partial C}{\partial z_{21}} \frac{\partial z_{21}}{\partial x_{11}} + 
   \frac{\partial C}{\partial z_{22}} \frac{\partial z_{22}}{\partial x_{11}} = \delta_{11} \cdot w_{11}
\]</div>
<p>La matriz jacobiana completa</p>
<div class="math notranslate nohighlight">
\[\begin{split}\frac{\partial C}{\partial Z^{l-1}} = \begin{bmatrix}
  \frac{\partial C}{\partial x_{11}} &amp; \frac{\partial C}{\partial x_{12}} &amp; \frac{\partial C}{\partial x_{13}}  &amp; \frac{\partial C}{\partial x_{14}}\\
  \frac{\partial C}{\partial x_{21}} &amp; \frac{\partial C}{\partial x_{22}} &amp; \frac{\partial C}{\partial x_{23}} &amp; \frac{\partial C}{\partial x_{24}} \\
  \frac{\partial C}{\partial x_{31}} &amp; \frac{\partial C}{\partial x_{32}} &amp; \frac{\partial C}{\partial x_{33}}  &amp; \frac{\partial C}{\partial x_{34}}\\
   \frac{\partial C}{\partial x_{41}} &amp; \frac{\partial C}{\partial x_{42}} &amp; \frac{\partial C}{\partial x_{43}}  &amp; \frac{\partial C}{\partial x_{44}}\\
   \end{bmatrix}
\end{split}\]</div>
<p>Entonces <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z^{l-1}}\)</span> resulta:</p>
<div class="math notranslate nohighlight">
\[\begin{split}\begin{bmatrix}
w_{11} \cdot \delta_{11}  &amp;	w_{12} \cdot \delta_{11} + w_{11} \cdot \delta_{12} &amp;	w_{13} \cdot \delta_{11} + w_{12} \cdot \delta_{12} &amp;	w_{13} \cdot \delta_{12} \\
w_{21} \cdot \delta_{11} + w_{11} \cdot \delta_{21} &amp; w_{22} \cdot \delta_{11} + w_{21} \cdot \delta_{12} + w_{12} \cdot \delta_{21} + w_{11} \cdot \delta_{22} &amp;  w_{23} \cdot \delta_{11} + w_{22} \cdot \delta_{12} + w_{13} \cdot \delta_{21} + w_{12} \cdot \delta_{22} &amp; w_{23} \cdot \delta_{12} + w_{13} \cdot \delta_{22} \\
w_{31} \cdot \delta_{11} + w_{21} \cdot \delta_{22} &amp; w_{32} \cdot \delta_{11} + w_{31} \cdot \delta_{12} + w_{22} \cdot \delta_{21} + w_{21} \cdot \delta_{22} &amp; w_{33} \cdot \delta_{11} + w_{32} \cdot \delta_{12} + w_{23} \cdot \delta_{21} + w_{22} \cdot \delta_{22} &amp; w_{33} \cdot \delta_{12} + w_{23} \cdot \delta_{22} \\
w_{31} \cdot \delta_{21} &amp; w_{32} \cdot \delta_{21} + w_{31} \cdot \delta_{22} &amp; w_{33} \cdot \delta_{21} + w_{32} \cdot \delta_{22} &amp; w_{33} \cdot \delta_{22}
\end{bmatrix} \end{split}\]</div>
<p>La forma de obtener esta matriz es acumulando el producto escalar</p>
<div class="math notranslate nohighlight">
\[ \delta_{ij} \cdot W^l \]</div>
<p>En la matriz <span class="math notranslate nohighlight">\(\frac{\partial C}{\partial Z^{l-1}}\)</span> cuya dimensión coindicen con <span class="math notranslate nohighlight">\(A^{l-1}\)</span>, en este ejemplo 4x4, acumulando el resultado matricial, en este caso 3x3, en la submatriz de acuerdo al movimiento del filtro. El primer subproducto escalar se ve en la siguiente imagen:</p>
<a class="reference internal image-reference" href="_images/conv_gradA.png"><img alt="_images/conv_gradA.png" class="align-center" src="_images/conv_gradA.png" style="width: 800px;" /></a>
<p>Y el siguiente subproducto en:</p>
<a class="reference internal image-reference" href="_images/conv_gradA2.png"><img alt="_images/conv_gradA2.png" class="align-center" src="_images/conv_gradA2.png" style="width: 800px;" /></a>
<p>El código programado aparece en la función <strong>backward</strong> de la clase <strong>conv2DLayer</strong> (ajustando la notación):</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>             DeltaPrev[i, :, _h:_h+f, _b:_b+f] += self.W[co, :] * Delta[i, co, _h,_b]
</pre></div>
</div>
<p>El código anterior finaliza concatenado el <strong>producto de Hadamard de la derivada primera de la función de activación</strong> en <span class="math notranslate nohighlight">\(Z^{l-1}\)</span></p>
</section>
<section id="softmax-y-cross-entropy">
<h2>Softmax y cross-entropy<a class="headerlink" href="#softmax-y-cross-entropy" title="Link to this heading">#</a></h2>
<p>A la salida de la última capa se aplica la función softmax. Si el vector <span class="math notranslate nohighlight">\(z\)</span> tiene la transformación lineal de la última capa (antes de aplicar la función de activación) la probabilidad de la neurona <span class="math notranslate nohighlight">\(j\)</span> de salida es de acuerdo a la función softmax:</p>
<div class="math notranslate nohighlight">
\[p_j = \frac{e^{z_j}}{\sum_j{e^{z_j}}}\]</div>
<p>Finalmente la pérdida se obtiene con la función entropia cruzada que es:</p>
<div class="math notranslate nohighlight">
\[- \sum_j {y_j \cdot ln(p_j)}\]</div>
<p>Dónde <span class="math notranslate nohighlight">\(y_j\)</span> es el valor de la etiqueta en formato <strong>one-hot</strong> (vector con tantos dígitos como posibles etiquetas hay y que lleva todo ceros y un 1 en la posición que corresponde a la etiqueta).</p>
<p>El uso de softmax y cross-entropy a la salida de la última capa (<span class="math notranslate nohighlight">\(L\)</span>) como se indica hace que el gradiente del coste con respecto a la transformación lineal de esa última capa:</p>
<div class="math notranslate nohighlight">
\[ \frac{\partial C}{\partial z^l_i} =  p_i - y_i\]</div>
<p>Para comprobarlo en un caso sencillo, se supone que una observación que tiene una transformación lineal:</p>
<div class="math notranslate nohighlight">
\[z=(z_1, z_2, z_3)\]</div>
<p>Sabiendo que las etiquetas reales de esa observación en formato <strong>one-hot</strong> viene dado por</p>
<div class="math notranslate nohighlight">
\[y=(y_1, y_2, y_3)\]</div>
<p>La probabilidad usando la función <strong>softmax</strong> es:</p>
<div class="math notranslate nohighlight">
\[p=(p_1, p_2, p_3)\]</div>
<div class="math notranslate nohighlight">
\[p_1 = \frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}\]</div>
<div class="math notranslate nohighlight">
\[p_2 = \frac{e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3}}\]</div>
<div class="math notranslate nohighlight">
\[p_3 = \frac{e^{z_3}}{e^{z_1} + e^{z_2} + e^{z_3}}\]</div>
<p>La función de coste por entropia cruzada es:</p>
<div class="math notranslate nohighlight">
\[ C = -y1 \cdot ln(p_1) -y2 \cdot ln(p_2) -y3 \cdot ln(p_3) \]</div>
<p>El gradiente del coste con respecto a <span class="math notranslate nohighlight">\(z\)</span> será el vector</p>
<div class="math notranslate nohighlight">
\[\begin{pmatrix} \frac{\partial C}{\partial z_1}, \frac{\partial C}{\partial z_2}, \frac{\partial C}{\partial z_3} \end{pmatrix}\]</div>
<p>Desarrollando la primera componente:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial z_1} = -y1 \frac{1}{p_1}\frac{\partial p_1}{\partial z_1}  -y2 \frac{1}{p_2}\frac{\partial p_2}{\partial z_1}  -y3 \frac{1}{p_3}\frac{\partial p_3}{\partial z_1}\]</div>
<p>Quedando:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial z_1} = \frac{-y_1e^{z_2}-y_1e^{z_3}+y_2e^{z_1}+y_3e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}\]</div>
<p>Sumando y restando el término <span class="math notranslate nohighlight">\(y_1e^{z_1}\)</span> la fracción se puede poner</p>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial z_1} = \frac{-y_1 (e^{z_1} + e^{z_2} + e^{z_3}) + e^{z_1}(y_1+y_2+y_3)}{e^{z_1} + e^{z_2} + e^{z_3}}\]</div>
<p>Pero como el vector <span class="math notranslate nohighlight">\(y\)</span> es un formato <strong>one-hot</strong>, se cumple que <span class="math notranslate nohighlight">\(y_1+y_2+y_3=1\)</span> quedado la expresión:</p>
<div class="math notranslate nohighlight">
\[\frac{\partial C}{\partial z_1} = -y_1 + \frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}} = p_1 - y_1\]</div>
<p>Haciendo lo mismo con las otras 2 derivadas parciales se ve que el vector gradiente se obtiene así:</p>
<div class="math notranslate nohighlight">
\[\begin{pmatrix} \frac{\partial C}{\partial z_1}, \frac{\partial C}{\partial z_2}, \frac{\partial C}{\partial z_3} \end{pmatrix} = \begin{pmatrix}p_1 - y_1, &amp; p_2 - y_2, &amp; p_3 - y_3 \end{pmatrix} = p - y\]</div>
<p>Este gradiente es el que aparece en el esquema anterior de LetNet5 identificado como <strong>dA8</strong>, una matriz de 32 filas y 10 columnas para cada una de las 10 posibles etiquetas. Esta matriz jacobiana es la que arranca el proceso de retro-propagación de los gradientes del coste.</p>
</section>
<section id="optimizacion-adam">
<h2>Optimización ADAM<a class="headerlink" href="#optimizacion-adam" title="Link to this heading">#</a></h2>
<p>Al actualizar los pesos y bias con sus gradientes de coste multiplicados por la tasa de entrenamiento (<span class="math notranslate nohighlight">\(\eta\)</span>) en arquitecturas que mezclan capas de diferentes naturaleza surge el inconveniente de que las velocidades de crecimiento de los gradientes no son homogéneas y sería preciso utilizar valores de <span class="math notranslate nohighlight">\(\eta\)</span> diferentes en cada capa.</p>
<p>Como sería muy difícil estimar estos valores en cada capa, hay algoritmos, por ejemplo Root Mean Square Propagation (<strong>RMSprop</strong>) que utilizan una media móvil que promedia más los valores actuales que los antiguos. A la vez se utiliza un momento de segundo orden para normalizar los elementos del gradiente (pues la raiz cuadrada de de la estimación del momento del segundo orden equivale a la desviación estándar).</p>
<p><strong>ADAM</strong> (o estimación adaptativa del momento) añade a <strong>RMSprop</strong> el cálculo de un momento y es el método más utilizado. Su fórmula es:</p>
<div class="math notranslate nohighlight">
\[m_{t+1}=\beta \cdot m_t + (1 - \beta) \bigtriangledown f_i(w_t)\]</div>
<div class="math notranslate nohighlight">
\[v_{t+1}=\alpha \cdot v_t + (1 - \alpha) \bigtriangledown f_i(w_t) ^2\]</div>
<div class="math notranslate nohighlight">
\[w_{t+1} = w_t - \eta \frac{m_t}{\sqrt{v_t + 1} + \epsilon}\]</div>
<p>Donde <span class="math notranslate nohighlight">\(\epsilon\)</span> es un valor muy próximo a cero (<span class="math notranslate nohighlight">\(10^{-7}\)</span> por ejemplo) para evitar divisiones por cero</p>
</section>
<section id="funciones-generales">
<h2>Funciones generales<a class="headerlink" href="#funciones-generales" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">netFuntion</span><span class="p">(</span><span class="nb">object</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">):</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">random</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">RandomState</span><span class="p">(</span><span class="n">seed</span><span class="p">)</span>

        <span class="k">return</span> 
    
    <span class="k">def</span> <span class="nf">sigmoid</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="c1">#return 1.0/(1.0 + np.exp(-x))  ## versión básica con problemas de desbordamiento en valores x&lt;&lt;&lt;0</span>
        <span class="c1">#return np.where(x &lt; 0, np.exp(x)/(1.0 + np.exp(x)), 1.0/(1.0 + np.exp(-x)))</span>
        <span class="c1">#return 1. / (1. + np.exp(-np.clip(x, -250, 250)))</span>
        <span class="kn">from</span> <span class="nn">scipy.special</span> <span class="kn">import</span> <span class="n">expit</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">expit</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>  <span class="c1">##Función sigmoidea de scipy; algo más lenta</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;Error sigmoid en X=&#39;</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>
            <span class="k">return</span> <span class="n">x</span>

    <span class="k">def</span> <span class="nf">sigmoid_derivada</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="mf">1.0</span><span class="o">-</span><span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">x</span><span class="p">))</span>

    <span class="k">def</span> <span class="nf">tanh</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">tanh_derivada</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="mf">1.0</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">x</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>

    <span class="k">def</span> <span class="nf">ReLU</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">maximum</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">x</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">ReLU_derivada</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">where</span><span class="p">(</span><span class="n">x</span> <span class="o">&lt;=</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">0</span><span class="p">,</span> <span class="mi">1</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">raw_preds</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        pass raw predictions through softmax activation function</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">try</span><span class="p">:</span>
            <span class="n">out</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">raw_preds</span><span class="p">)</span>
            <span class="n">soft</span> <span class="o">=</span> <span class="n">out</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">out</span><span class="p">)</span> <span class="c1"># divide the exponentiated vector by its sum. All values in the output sum to 1.</span>
        <span class="k">except</span><span class="p">:</span>
            <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;ejecutada excepcion en softmax&#39;</span><span class="p">)</span>
            <span class="n">soft</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">stable_softmax</span><span class="p">(</span><span class="n">raw_preds</span><span class="p">)</span>
 
        <span class="k">return</span> <span class="n">soft</span>

    <span class="k">def</span> <span class="nf">stable_softmax</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">x</span><span class="p">):</span>
        <span class="n">z</span> <span class="o">=</span> <span class="n">x</span> <span class="o">-</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">numerator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">exp</span><span class="p">(</span><span class="n">z</span><span class="p">)</span>
        <span class="n">denominator</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">numerator</span><span class="p">,</span> <span class="n">axis</span><span class="o">=-</span><span class="mi">1</span><span class="p">,</span> <span class="n">keepdims</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
        <span class="n">softmax</span> <span class="o">=</span> <span class="n">numerator</span> <span class="o">/</span> <span class="n">denominator</span>
        <span class="k">return</span> <span class="n">softmax</span>

    <span class="k">def</span> <span class="nf">categoricalCrossEntropy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calcula la pérdida de entropía cruzada categórica de las predicciones</span>
<span class="sd">        Multiplica la etiqueta de salida deseada por el registro de la predicción, y se suman todos los valores en el vector        </span>
<span class="sd">        Se evita calcular el logaritmo de cero sumando un epsilon = 1e-7</span>
<span class="sd">        &#39;&#39;&#39;</span>
        
        <span class="n">entr</span> <span class="o">=</span> <span class="mi">0</span>
        <span class="k">for</span> <span class="n">v_pro</span><span class="p">,</span> <span class="n">v_lab</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">label</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_pro</span><span class="p">,</span> <span class="n">_lab</span> <span class="ow">in</span> <span class="nb">zip</span><span class="p">(</span><span class="n">v_pro</span><span class="p">,</span> <span class="n">v_lab</span><span class="p">):</span>
                <span class="n">entr</span> <span class="o">-=</span> <span class="n">_lab</span> <span class="o">*</span> <span class="n">np</span><span class="o">.</span><span class="n">log</span><span class="p">(</span><span class="n">_pro</span> <span class="o">+</span> <span class="mf">1e-7</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">entr</span>
        <span class="c1">#return  -np.sum(label * np.log(probs + 1e-7))</span>

    <span class="k">def</span> <span class="nf">listToString</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="nb">list</span><span class="p">):</span>
        <span class="n">cadena</span> <span class="o">=</span> <span class="s1">&#39;&#39;</span>
        <span class="k">for</span> <span class="n">ele</span> <span class="ow">in</span> <span class="nb">list</span><span class="p">:</span>
            <span class="n">cadena</span> <span class="o">=</span> <span class="n">cadena</span> <span class="o">+</span> <span class="n">ele</span> <span class="o">+</span> <span class="s1">&#39;; &#39;</span>
        <span class="k">return</span> <span class="n">cadena</span>
    
    <span class="k">def</span> <span class="nf">validarOpciones</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">valOpcion</span><span class="p">,</span> <span class="n">tipOpcion</span><span class="p">):</span>
        <span class="k">if</span> <span class="n">tipOpcion</span> <span class="o">==</span> <span class="s1">&#39;ACTIVACION&#39;</span><span class="p">:</span>
            <span class="n">lstOpciones</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;RELU&#39;</span><span class="p">,</span> <span class="s1">&#39;SIGMOID&#39;</span><span class="p">,</span> <span class="s1">&#39;TANH&#39;</span><span class="p">,</span> <span class="s1">&#39;IDENTITY&#39;</span><span class="p">,</span> <span class="s1">&#39;SOFTMAX&#39;</span><span class="p">]</span>
            <span class="n">literal</span> <span class="o">=</span> <span class="s1">&#39;Tipo de activación&#39;</span>
        <span class="k">elif</span> <span class="n">tipOpcion</span> <span class="o">==</span> <span class="s1">&#39;OPTIMIZADOR&#39;</span><span class="p">:</span>
            <span class="n">lstOpciones</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;ADAM&#39;</span><span class="p">,</span> <span class="s1">&#39;GRAD&#39;</span><span class="p">]</span>
            <span class="n">literal</span> <span class="o">=</span> <span class="s1">&#39;Optimizador&#39;</span>
        <span class="k">else</span><span class="p">:</span> <span class="c1">## &#39;PERDIDA&#39;</span>
            <span class="n">lstOpciones</span> <span class="o">=</span> <span class="p">[</span><span class="s1">&#39;CROSSENTROPY&#39;</span><span class="p">,</span> <span class="s1">&#39;ERRORCUADRATICO&#39;</span><span class="p">]</span>
            <span class="n">literal</span> <span class="o">=</span> <span class="s1">&#39;Función de pérdida&#39;</span>
        
        <span class="n">valOpcion</span> <span class="o">=</span> <span class="n">valOpcion</span><span class="o">.</span><span class="n">upper</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="n">valOpcion</span> <span class="ow">in</span> <span class="n">lstOpciones</span><span class="p">,</span> <span class="n">literal</span> <span class="o">+</span> <span class="s1">&#39; &lt;&#39;</span> <span class="o">+</span> <span class="n">valOpcion</span> <span class="o">+</span> <span class="s1">&#39;&gt; no contemplada. Usar: &#39;</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">listToString</span><span class="p">(</span><span class="n">lstOpciones</span><span class="p">)</span>
        
        <span class="k">return</span> <span class="n">valOpcion</span>
        
    
    <span class="k">def</span> <span class="nf">activacionPrima</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tipActiva</span><span class="p">,</span> <span class="n">A</span><span class="p">,</span> <span class="n">B</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;RELU&#39;</span><span class="p">):</span>
            <span class="n">A</span><span class="p">[</span><span class="n">B</span><span class="o">&lt;</span><span class="mi">0</span><span class="p">]</span> <span class="o">=</span> <span class="mi">0</span>
            <span class="k">return</span> <span class="n">A</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;SIGMOID&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">A</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid_derivada</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;TANH&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">A</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh_derivada</span><span class="p">(</span><span class="n">B</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>  <span class="c1">## No cambia si es Identity y Softmax (porque sólo se usa en la última capa)</span>
            <span class="k">return</span> <span class="n">A</span>

    <span class="k">def</span> <span class="nf">activacion</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">tipActiva</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;RELU&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">ReLU</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;SIGMOID&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">sigmoid</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;TANH&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="bp">self</span><span class="o">.</span><span class="n">tanh</span><span class="p">(</span><span class="n">X</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;SOFTMAX&#39;</span><span class="p">):</span>
            <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sp</span><span class="p">)</span> <span class="k">for</span> <span class="n">sp</span> <span class="ow">in</span> <span class="n">X</span><span class="p">])</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">return</span> <span class="n">X</span>

    <span class="k">def</span> <span class="nf">onehot</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">y</span><span class="p">,</span> <span class="n">n_classes</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Convierte las etiquetas en una representación de vectores de base canónica R^K siendo K el total de etiquetas</span>
<span class="sd">        Parameters</span>
<span class="sd">        ------------</span>
<span class="sd">        y : array, shape = [n_samples]</span>
<span class="sd">           Valores objetivo.</span>
<span class="sd">        Returns</span>
<span class="sd">        -----------</span>
<span class="sd">        onehot : array, shape = (n_samples, n_labels)</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="k">if</span> <span class="n">n_classes</span> <span class="o">==</span> <span class="mi">2</span><span class="p">:</span>
            <span class="n">onehot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="mi">1</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)):</span>
                <span class="n">ilabel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Clases_y</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                <span class="n">onehot</span><span class="p">[</span><span class="mi">0</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">ilabel</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">onehot</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n_classes</span><span class="p">,</span> <span class="n">y</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]))</span>
            <span class="k">for</span> <span class="n">idx</span><span class="p">,</span> <span class="n">val</span> <span class="ow">in</span> <span class="nb">enumerate</span><span class="p">(</span><span class="n">y</span><span class="o">.</span><span class="n">astype</span><span class="p">(</span><span class="nb">int</span><span class="p">)):</span>
                <span class="n">ilabel</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">Clases_y</span><span class="o">.</span><span class="n">tolist</span><span class="p">()</span><span class="o">.</span><span class="n">index</span><span class="p">(</span><span class="n">val</span><span class="p">)</span>
                <span class="n">onehot</span><span class="p">[</span><span class="n">ilabel</span><span class="p">,</span> <span class="n">idx</span><span class="p">]</span> <span class="o">=</span> <span class="mf">1.</span>
        <span class="k">return</span> <span class="n">onehot</span><span class="o">.</span><span class="n">T</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="clase-para-una-capa-densa">
<h2>Clase para una capa densa<a class="headerlink" href="#clase-para-una-capa-densa" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">linealLayer</span><span class="p">(</span><span class="n">netFuntion</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">neurInp</span><span class="p">,</span> <span class="n">neurOut</span><span class="p">,</span> <span class="n">tipActiva</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">neurInp</span> <span class="o">=</span> <span class="n">neurInp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">neurOut</span> <span class="o">=</span> <span class="n">neurOut</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validarOpciones</span><span class="p">(</span><span class="n">tipActiva</span><span class="p">,</span> <span class="s1">&#39;ACTIVACION&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">standard_normal</span><span class="p">(</span><span class="n">size</span><span class="o">=</span><span class="p">(</span><span class="n">neurOut</span><span class="p">,</span> <span class="n">neurInp</span><span class="p">))</span> <span class="o">*</span> <span class="mf">0.01</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">neurOut</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pesos</span><span class="o">=</span><span class="kc">True</span>
        
        <span class="k">return</span> 
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span>  <span class="n">X</span><span class="p">):</span> 
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calcula la transformación lineal por la matriz de pesos W</span>
<span class="sd">        Hay c_i canales de entrada. La dimensión de X ha de ser c_i mapas </span>

<span class="sd">        Argumentos:</span>
<span class="sd">        p -- Tamaño de Pool</span>
<span class="sd">        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada</span>

<span class="sd">        Retornos:</span>
<span class="sd">        H -- Salida agrupada con dimensión (c_i, h/p, b/p)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">2</span> <span class="ow">or</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada X deben ser listas de 2 o 4 dimensiones&quot;</span>

        <span class="c1"># Si la entrada es un mapa que ha devuelto una convolución se aplanan las dimensiones canal, alto, ancho en una</span>
        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span>
            <span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])))</span>

        <span class="c1">#print(&#39;forwardLineal - X.shape=&#39;, X.shape)</span>
        <span class="c1"># Se recogen las dimensiones de los mapas de entrada</span>
        <span class="p">(</span><span class="n">nf_x</span><span class="p">,</span> <span class="n">nc_x</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">(</span><span class="n">nf_w</span><span class="p">,</span> <span class="n">nc_w</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">(</span><span class="n">nc_b</span><span class="p">,)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span>

        <span class="k">assert</span> <span class="n">nc_x</span> <span class="o">==</span> <span class="n">nc_w</span><span class="p">,</span> <span class="s1">&#39;El numero de columnas de las matrices X y W deben ser iguales&#39;</span>
        <span class="k">assert</span> <span class="n">nc_b</span> <span class="o">==</span> <span class="n">nf_w</span><span class="p">,</span> <span class="s1">&#39;La dimensión del Bias y el nº de filas de W debe ser iguales nc_b=&#39;</span><span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nc_b</span><span class="p">)</span> <span class="o">+</span> <span class="s1">&#39;_nf_w=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">nf_w</span><span class="p">)</span>

        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">))</span> <span class="o">+</span> <span class="n">np</span><span class="o">.</span><span class="n">transpose</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>
        <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activacion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        El cálculo hacia atrás para una capa Densa</span>

<span class="sd">         Argumentos:</span>
<span class="sd">         dA: gradiente del costo con respecto a la salida de la capa densa, </span>
<span class="sd">             matriz numpy de forma (n_H, n_W) asumiendo canales = 1</span>
<span class="sd">         Apre -- Salida de la activación de la capa previa</span>
<span class="sd">         W -- Matriz de pesos de la capa actual</span>
<span class="sd">         l: identifica el nº de capa. Si l==0 es la primera capa oculta</span>

<span class="sd">         Retornos:</span>
<span class="sd">         dAprev: gradiente del costo con respecto a la entrada de la capa anterior. Gradiente que se propaga</span>
<span class="sd">         dW: gradiente de la matriz de peso de la capa actual</span>
<span class="sd">         dB: gradiente de la matriz de bias de la capa actual</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="k">if</span> <span class="nb">len</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">:</span>
            <span class="n">Aprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">Aprev</span><span class="p">,(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="nb">int</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]</span><span class="o">*</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]</span><span class="o">*</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">])))</span>

        <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dA</span><span class="o">.</span><span class="n">T</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">)</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>

        <span class="c1">## Propagación del delta dA sobre la capa previa. En la primera capa no tiene sentido la propagación</span>
        <span class="k">if</span> <span class="n">l</span><span class="o">==</span><span class="mi">0</span><span class="p">:</span>
            <span class="n">dAprev</span> <span class="o">=</span> <span class="kc">None</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">dAprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">dot</span><span class="p">(</span><span class="n">dA</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">dAprev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">dB</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="clase-para-una-capa-convolucional">
<h2>Clase para una capa convolucional<a class="headerlink" href="#clase-para-una-capa-convolucional" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">conv2DLayer</span><span class="p">(</span><span class="n">netFuntion</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">canalInp</span><span class="p">,</span> <span class="n">canalOut</span><span class="p">,</span> <span class="n">fSize</span><span class="p">,</span> <span class="n">tipActiva</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">canalInp</span> <span class="o">=</span> <span class="n">canalInp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">canalOut</span> <span class="o">=</span> <span class="n">canalOut</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">fSize</span> <span class="o">=</span> <span class="n">fSize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validarOpciones</span><span class="p">(</span><span class="n">tipActiva</span><span class="p">,</span> <span class="s1">&#39;ACTIVACION&#39;</span><span class="p">)</span>
        <span class="n">stddev</span> <span class="o">=</span> <span class="mf">1.0</span><span class="o">/</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">prod</span><span class="p">((</span><span class="n">canalOut</span><span class="p">,</span> <span class="n">canalInp</span><span class="p">,</span> <span class="n">fSize</span><span class="p">,</span> <span class="n">fSize</span><span class="p">)))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">normal</span><span class="p">(</span><span class="n">loc</span> <span class="o">=</span> <span class="mi">0</span><span class="p">,</span> <span class="n">scale</span> <span class="o">=</span> <span class="n">stddev</span><span class="p">,</span> <span class="n">size</span> <span class="o">=</span> <span class="p">(</span><span class="n">canalOut</span><span class="p">,</span> <span class="n">canalInp</span><span class="p">,</span> <span class="n">fSize</span><span class="p">,</span> <span class="n">fSize</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">canalOut</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pesos</span><span class="o">=</span><span class="kc">True</span>
        
        <span class="k">return</span> 
    
    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> 
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calcula una convolución suponiendo que no hay padding (relleno a ceros) y strike 1 (sin salto)</span>
<span class="sd">        El filtro es dimension f x f</span>
<span class="sd">        Por lo tanto el mapa de salida tiene una dimensión que se reduce en f-1 con respecto al de entrada</span>
<span class="sd">        Hay c_i canales de entrada y c_o canales de salida. La dimensión de X ha de ser c_i mapas </span>
<span class="sd">        Y los pesos han de ser c_o lista de c_i matrices de dimension f</span>

<span class="sd">        Argumentos:</span>
<span class="sd">        W -- Pesos, numpy array con dimension (c_o, c_i, f, f), siendo un filtro individual (f, f) </span>
<span class="sd">        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada</span>

<span class="sd">        Retornos:</span>
<span class="sd">        A -- Salida convolucionada, numpy array de tamaño (n, c_o, h, b), se asume salto 1 y siempre relleno a ceros</span>
<span class="sd">        &#39;&#39;&#39;</span>

        <span class="c1">#W = np.asarray(W)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada X deben ser listas de 4 dimensiones. X.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de pesos W deben ser listas de 4 dimensiones. W.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="c1"># Se recogen las dimensiones de los mapas de entrada</span>
        <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c_i</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Se recogen las dimensiones de los pesos </span>
        <span class="p">(</span><span class="n">c_o</span><span class="p">,</span> <span class="n">_c_i</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Se recoge las dimensiones de los Bias</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span><span class="o">==</span><span class="n">c_o</span><span class="p">,</span> <span class="s2">&quot;Bias debe ser de dimensión y con nº igual a canal salida=&quot;</span>  <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">assert</span> <span class="n">_c_i</span> <span class="o">==</span> <span class="n">c_i</span><span class="p">,</span> <span class="s2">&quot;El nº de canales de entrada en X debe ser igual que al nº canales de entrada en W&quot;</span>

        <span class="c1"># Inicializa la salida H con ceros</span>
        <span class="n">Z</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">c_o</span><span class="p">,</span> <span class="n">h</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">)))</span>

        <span class="c1"># Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida</span>
        <span class="c1">#for i in range(n):</span>
        <span class="c1">#    for co in range(c_o):</span>
        <span class="c1">#        for ci in range(c_i):</span>
        <span class="c1">#            for _h in range(h-(f-1)):</span>
        <span class="c1">#                for _b in range(b-(f-1)):</span>
        <span class="c1">#                    x_slice = X[i, ci, _h:_h+f, _b:_b+f]</span>
        <span class="c1">#                    Z[i, co, _h,_b] += np.sum(x_slice * self.W[co, ci]) + self.B[co]</span>

        <span class="c1"># Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida</span>
        <span class="k">for</span> <span class="n">co</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c_o</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">ci</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c_i</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                    <span class="k">for</span> <span class="n">_b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="o">-</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">)):</span>
                        <span class="n">x_slice</span> <span class="o">=</span> <span class="n">X</span><span class="p">[:,</span> <span class="n">ci</span><span class="p">,</span> <span class="n">_h</span><span class="p">:</span><span class="n">_h</span><span class="o">+</span><span class="n">f</span><span class="p">,</span> <span class="n">_b</span><span class="p">:</span><span class="n">_b</span><span class="o">+</span><span class="n">f</span><span class="p">]</span>
                        <span class="n">Z</span><span class="p">[:,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span><span class="n">_b</span><span class="p">]</span> <span class="o">+=</span> <span class="n">np</span><span class="o">.</span><span class="n">tensordot</span><span class="p">(</span><span class="n">x_slice</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">[</span><span class="n">co</span><span class="p">,</span> <span class="n">ci</span><span class="p">])</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">[</span><span class="n">co</span><span class="p">]</span>
        
        <span class="n">A</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activacion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span><span class="p">,</span> <span class="n">Z</span><span class="p">)</span>
        <span class="k">return</span> <span class="n">Z</span><span class="p">,</span> <span class="n">A</span>

    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        El cálculo hacia atrás para una función de convolución</span>

<span class="sd">         Argumentos:</span>
<span class="sd">         dA: gradiente del costo con respecto a la salida de la capa de conv (A), </span>
<span class="sd">             matriz numpy de forma (n_H, n_W) asumiendo canales = 1</span>
<span class="sd">         Aprev: Activación de la capa previa</span>
<span class="sd">         W, B: matrices de peso y bias de la capa actual</span>
<span class="sd">         shape_A : Tamaño de la activación en la capa actual</span>

<span class="sd">         Retornos:</span>
<span class="sd">         dAprev: gradiente del costo con respecto a la entrada de la capa anterior</span>
<span class="sd">         dW: gradiente del costo con respecto a los pesos del filtro de la capa actual </span>
<span class="sd">         dB: gradiente dle costo con respecto a los bias del filgro de la capa actual</span>
<span class="sd">         l: identifica el nº de capa. Si l==0 es la primera capa oculta</span>
<span class="sd">        &#39;&#39;&#39;</span>
        
        <span class="c1">#W = np.asarray(W)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dA</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada dA deben ser listas de 4 dimensiones. dA.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dA</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada Aprev deben ser listas de 4 dimensiones. Aprev.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de pesos W deben ser listas de 4 dimensiones=&quot;</span>  <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        
        <span class="c1"># Se recogen las dimensiones de los mapas de entrada</span>
        <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="n">dA</span><span class="o">.</span><span class="n">shape</span>
        <span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">c_i</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span> <span class="o">=</span> <span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span>
        <span class="c1"># Se recogen las dimensiones de los pesos </span>
        <span class="p">(</span><span class="n">c_o</span><span class="p">,</span> <span class="n">_c_i</span><span class="p">,</span> <span class="n">f</span><span class="p">,</span> <span class="n">f</span><span class="p">)</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">_c_i</span> <span class="o">==</span> <span class="n">c_i</span><span class="p">,</span> <span class="s2">&quot;El nº de canales de entrada en Aprev debe ser igual que al nº canales de entrada en W&quot;</span>

        <span class="k">assert</span> <span class="n">n</span><span class="o">==</span><span class="n">n2</span><span class="p">,</span> <span class="s2">&quot;El nº de observaciones en Aprev y dA debe ser igual n=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; n2=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n2</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">h2</span><span class="o">==</span><span class="n">h</span><span class="o">+</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;La altura del mapa previo y actual deben diferir en el filtro. h=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; h2=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">b2</span><span class="o">==</span><span class="n">b</span><span class="o">+</span><span class="p">(</span><span class="n">f</span><span class="o">-</span><span class="mi">1</span><span class="p">),</span> <span class="s2">&quot;La anchua del mapa previo y actual deben diferir en el filtro. b=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">h</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot; b2=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">h2</span><span class="p">)</span>

        <span class="c1"># Initializing dAprev, dW with the correct shapes</span>
        <span class="n">dAprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Aprev</span><span class="p">)</span>
        <span class="n">dW</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
        <span class="n">dB</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>

        <span class="c1"># Inicializamos dAprev con la dimensión de Aprev</span>
        <span class="n">dAprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Aprev</span><span class="p">)</span>

        <span class="c1"># Bucle sobre los ejes vertical(h) y la horizontal(b) de dA</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">co</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c_o</span><span class="p">):</span>
                            <span class="c1">##Full Convolution 180º rotate Filter</span>
                            <span class="n">dAprev</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">_h</span><span class="p">:</span><span class="n">_h</span><span class="o">+</span><span class="n">f</span><span class="p">,</span> <span class="n">_b</span><span class="p">:</span><span class="n">_b</span><span class="o">+</span><span class="n">f</span><span class="p">]</span> <span class="o">+=</span> <span class="bp">self</span><span class="o">.</span><span class="n">W</span><span class="p">[</span><span class="n">co</span><span class="p">,</span> <span class="p">:]</span> <span class="o">*</span> <span class="n">dA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span><span class="n">_b</span><span class="p">]</span>
                            <span class="c1">##Convolution</span>
                            <span class="n">dW</span><span class="p">[</span><span class="n">co</span><span class="p">,</span> <span class="p">:]</span> <span class="o">+=</span> <span class="n">Aprev</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="p">:,</span> <span class="n">_h</span><span class="p">:</span><span class="n">_h</span><span class="o">+</span><span class="n">f</span><span class="p">,</span> <span class="n">_b</span><span class="p">:</span><span class="n">_b</span><span class="o">+</span><span class="n">f</span><span class="p">]</span> <span class="o">*</span> <span class="n">dA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span><span class="n">_b</span><span class="p">]</span>
                            <span class="n">dB</span><span class="p">[</span><span class="n">co</span><span class="p">]</span> <span class="o">+=</span> <span class="n">dA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span> <span class="n">_b</span><span class="p">]</span>
        
        <span class="k">return</span> <span class="n">dAprev</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">dB</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="clase-para-una-capa-de-agrupacion-por-maximo-max-pooling">
<h2>Clase para una capa de agrupación por máximo (max-pooling)<a class="headerlink" href="#clase-para-una-capa-de-agrupacion-por-maximo-max-pooling" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">maxPooling</span><span class="p">(</span><span class="n">netFuntion</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">pSize</span><span class="p">,</span> <span class="n">tipActiva</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">pSize</span> <span class="o">=</span> <span class="n">pSize</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validarOpciones</span><span class="p">(</span><span class="n">tipActiva</span><span class="p">,</span> <span class="s1">&#39;ACTIVACION&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pesos</span><span class="o">=</span><span class="kc">False</span>
        
        <span class="k">return</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> 
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        Calcula la agrupación por pool, suponiendo un tamaño de pool p</span>
<span class="sd">        Hay c_i canales de entrada. La dimensión de X ha de ser c_i mapas </span>

<span class="sd">        Argumentos:</span>
<span class="sd">        p -- Tamaño de Pool</span>
<span class="sd">        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada</span>

<span class="sd">        Retornos:</span>
<span class="sd">        H -- Salida agrupada con dimensión (c_i, h/p, b/p)</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada X deben ser listas de 4 dimensiones&quot;</span>
        <span class="c1"># Se recogen las dimensiones de los mapas de entrada</span>
        <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c_i</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span>

        <span class="c1">## Se calcula la nueva dimensión una vez aplicado el pool</span>
        <span class="n">n_h</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">h</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">)</span>
        <span class="n">n_b</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">b</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">)</span>
        <span class="c1"># Inicializa la salida H con ceros</span>
        <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros</span><span class="p">((</span><span class="n">n</span><span class="p">,</span> <span class="n">c_i</span><span class="p">,</span> <span class="n">n_h</span><span class="p">,</span> <span class="n">n_b</span><span class="p">))</span>

        <span class="c1"># Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">ci</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c_i</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_h</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">_b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n_b</span><span class="p">):</span>
                        <span class="n">x_slice</span> <span class="o">=</span> <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="o">*</span><span class="n">_h</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="o">*</span><span class="p">(</span><span class="n">_h</span><span class="o">+</span><span class="mi">1</span><span class="p">),</span> <span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="o">*</span><span class="n">_b</span><span class="p">:</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="o">*</span><span class="p">(</span><span class="n">_b</span><span class="o">+</span><span class="mi">1</span><span class="p">)]</span>
                        <span class="n">H</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">ci</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span><span class="n">_b</span><span class="p">]</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">max</span><span class="p">(</span><span class="n">x_slice</span><span class="p">)</span>

        <span class="k">return</span> <span class="n">H</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">activacion</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span><span class="p">,</span> <span class="n">H</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&#39;&#39;&#39;</span>
<span class="sd">        El cálculo hacia atrás para una función de convolución</span>

<span class="sd">         Argumentos:</span>
<span class="sd">         dA: gradiente del costo con respecto a la salida de la capa de pool (A), </span>
<span class="sd">             matriz numpy de forma (n_H, n_W) asumiendo canales = 1</span>
<span class="sd">         Aprev, A: Activación de la capa previa y de la capa actual. Sus dimensiones permiten calcular el pooling</span>

<span class="sd">         Retornos:</span>
<span class="sd">         dAprev: gradiente del costo con respecto a la entrada de la capa de conv (Aprev), </span>
<span class="sd">             matriz numpy de forma (n_H_prev, n_W_prev) asumiendo canales = 1</span>
<span class="sd">         l: identifica el nº de capa. Si l==0 es la primera capa oculta</span>
<span class="sd">        &#39;&#39;&#39;</span>
        <span class="c1">##print(&quot;backwardPool - dA.shape=&quot; + str(dA.shape))</span>
        <span class="c1">##print(&quot;backwardPool - Aprev.shape=&quot; + str(Aprev.shape))</span>
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada Aprev deben ser listas de 4 dimensiones. Aprev.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">dA</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">4</span><span class="p">,</span> <span class="s2">&quot;La matriz de entrada dA deben ser listas de 4 dimensiones. dA.shape=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">dA</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>

        <span class="c1"># Se recoge las dimensiones del gradiente del costo con respecto a la salida, dA</span>
        <span class="p">(</span><span class="n">n</span><span class="p">,</span> <span class="n">c_o</span><span class="p">,</span> <span class="n">h</span><span class="p">,</span> <span class="n">b</span><span class="p">)</span> <span class="o">=</span> <span class="n">dA</span><span class="o">.</span><span class="n">shape</span>

        <span class="p">(</span><span class="n">n2</span><span class="p">,</span> <span class="n">c_o2</span><span class="p">,</span> <span class="n">h2</span><span class="p">,</span> <span class="n">b2</span><span class="p">)</span> <span class="o">=</span> <span class="n">Aprev</span><span class="o">.</span><span class="n">shape</span>
        <span class="k">assert</span> <span class="n">n</span><span class="o">==</span><span class="n">n2</span> <span class="ow">and</span> <span class="n">c_o</span><span class="o">==</span><span class="n">c_o2</span><span class="p">,</span> <span class="s2">&quot;El nº de observaciones y canales en dA y Aprev deben ser iguales. n=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">n</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;; n2=&quot;</span> 
                                     <span class="c1">#str(n2) + &quot;;c_o=&quot; + str(c_o) + &quot;;c_o2=&quot; + str(c_o2)</span>
        <span class="n">p</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">h2</span><span class="o">/</span><span class="n">h</span><span class="p">)</span>
        <span class="n">p2</span> <span class="o">=</span> <span class="nb">int</span><span class="p">(</span><span class="n">b2</span><span class="o">/</span><span class="n">b</span><span class="p">)</span>
        <span class="k">assert</span> <span class="n">p</span><span class="o">==</span><span class="n">p2</span> <span class="ow">and</span> <span class="n">p</span><span class="o">&gt;</span><span class="mi">1</span> <span class="ow">and</span> <span class="n">p</span><span class="o">==</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">,</span> <span class="s2">&quot;Padding incorrecto p=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">p</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;;p2=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">p2</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;;self.pSize=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">)</span>

        <span class="c1"># Initializing dX, dW with the correct shapes</span>
        <span class="n">dAprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="n">Aprev</span><span class="p">)</span>

        <span class="c1"># Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida</span>
        <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">n</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">co</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">c_o</span><span class="p">):</span>
                <span class="k">for</span> <span class="n">_h</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">h</span><span class="p">):</span>
                    <span class="k">for</span> <span class="n">_b</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">b</span><span class="p">):</span>
                        <span class="n">mapa</span> <span class="o">=</span> <span class="n">Aprev</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">:(</span><span class="n">_h</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">,</span> <span class="n">_b</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">:(</span><span class="n">_b</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span><span class="p">]</span>
                        <span class="n">idx</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">nanargmax</span><span class="p">(</span><span class="n">mapa</span><span class="p">)</span>
                        <span class="p">(</span><span class="n">hm</span><span class="p">,</span> <span class="n">bm</span><span class="p">)</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unravel_index</span><span class="p">(</span><span class="n">idx</span><span class="p">,</span> <span class="n">mapa</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
                        <span class="n">dAprev</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span> <span class="o">+</span> <span class="n">hm</span><span class="p">,</span> <span class="n">_b</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">pSize</span> <span class="o">+</span> <span class="n">bm</span><span class="p">]</span> <span class="o">=</span> <span class="n">dA</span><span class="p">[</span><span class="n">i</span><span class="p">,</span> <span class="n">co</span><span class="p">,</span> <span class="n">_h</span><span class="p">,</span> <span class="n">_b</span><span class="p">]</span>
                        <span class="c1">## Forma alternativa de hacerlo</span>
                        <span class="c1">###dAprev[i, co, _h*p:(_h+1)*p, _b*p:(_b+1)*p] = (1/p**2)*dA[i, co, _h, _b]</span>


        <span class="k">return</span> <span class="n">dAprev</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="clase-de-aplanado">
<h2>Clase de aplanado<a class="headerlink" href="#clase-de-aplanado" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">flatten</span><span class="p">(</span><span class="n">netFuntion</span><span class="p">):</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">shapeInp</span><span class="p">,</span> <span class="n">sizeOut</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">()</span>
        
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">shapeInp</span><span class="p">)</span> <span class="o">==</span> <span class="mi">3</span><span class="p">,</span> <span class="s1">&#39;shapeInp debe ser de longitud 3. len(shapeInp)=&#39;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="nb">len</span><span class="p">(</span><span class="n">shapeInp</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shapeInp</span> <span class="o">=</span> <span class="n">shapeInp</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">sizeOut</span> <span class="o">=</span> <span class="n">sizeOut</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">tipActiva</span> <span class="o">=</span> <span class="s1">&#39;IDENTITY&#39;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">pesos</span><span class="o">=</span><span class="kc">False</span>
        
        <span class="k">return</span> 

    <span class="k">def</span> <span class="nf">forward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span> 
        
        <span class="n">H</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">sizeOut</span><span class="p">))</span>
        
        <span class="k">return</span> <span class="n">H</span><span class="p">,</span> <span class="n">H</span>
    
    <span class="k">def</span> <span class="nf">backward</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">Aprev</span><span class="p">,</span> <span class="n">l</span><span class="p">):</span>
        
        <span class="n">dAprev</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">dA</span><span class="p">,(</span><span class="n">dA</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapeInp</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapeInp</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="bp">self</span><span class="o">.</span><span class="n">shapeInp</span><span class="p">[</span><span class="mi">2</span><span class="p">]))</span>
        
        <span class="k">return</span> <span class="n">dAprev</span><span class="p">,</span> <span class="kc">None</span><span class="p">,</span> <span class="kc">None</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="clase-red-neuronal-convolucional">
<h2>Clase red neuronal convolucional<a class="headerlink" href="#clase-red-neuronal-convolucional" title="Link to this heading">#</a></h2>
<div class="cell tag_hide-cell docutils container">
<details class="hide above-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell content</span>
<span class="expanded">Hide code cell content</span>
</summary>
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">class</span> <span class="nc">CNN</span><span class="p">(</span><span class="n">netFuntion</span><span class="p">):</span>
<span class="w">    </span><span class="sd">&quot;&quot;&quot;MultiLayer LInear NEuron classifier.</span>
<span class="sd">    Parametros</span>
<span class="sd">    ------------</span>
<span class="sd">    eta : float Ratio de aprendizaje (entre 0.0 y 1.0)</span>
<span class="sd">    n_iter : int Pasos sobre el conjunto de datos de entrenamiento.</span>
<span class="sd">    capas: capas[0] las neuronas de entrada, capas[1] las neuronas de salida</span>
<span class="sd">    random_state : int Generador de semillas de números aleatorios para inicializar los pesos.</span>
<span class="sd">    shuffle : boolean. Hace una mezcla o barajado aleatorio del conjunto de entrenamiento en los minibatch</span>
<span class="sd">    minibatch_size: tamaño del minibatch o subconjuntos en que se divide el conjunto de entrada para el entrenamiento</span>
<span class="sd">    hiddenLayers: Neuronas en cada capa oculta. Aquí se excluye la capa de entrada y la de salida que la obtiene</span>
<span class="sd">                  automáticamente el proceso fit() de X e y</span>
<span class="sd">    Atributos</span>
<span class="sd">    -----------</span>
<span class="sd">    w_ : Array de dimensión 1 con los pesos después del ajuste.</span>
<span class="sd">    cost_ : lista de Suma-de-cuadrados de los valores de la función coste en cada Paso del algoritmo.</span>
<span class="sd">    &quot;&quot;&quot;</span>
    <span class="k">def</span> <span class="fm">__init__</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">epocas</span><span class="o">=</span><span class="mi">50</span><span class="p">,</span> <span class="n">shuffle</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="o">=</span><span class="mi">0</span><span class="p">,</span> <span class="n">seed</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">optimizador</span><span class="o">=</span><span class="s1">&#39;ADAM&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;CrossEntropy&#39;</span><span class="p">):</span>
        
        <span class="nb">super</span><span class="p">()</span><span class="o">.</span><span class="fm">__init__</span><span class="p">(</span><span class="n">seed</span><span class="o">=</span><span class="n">seed</span><span class="p">)</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">activarTrazaTensor</span><span class="o">=</span><span class="kc">False</span> <span class="c1">## Activa o inhibe las trazas de la función trazaTensor</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">=</span> <span class="n">eta</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">epocas</span> <span class="o">=</span> <span class="n">epocas</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span> <span class="o">=</span> <span class="n">shuffle</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="n">minibatch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">optimizador</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validarOpciones</span><span class="p">(</span><span class="n">optimizador</span><span class="p">,</span> <span class="s1">&#39;OPTIMIZADOR&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">loss</span>  <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">validarOpciones</span><span class="p">(</span><span class="n">loss</span><span class="p">,</span> <span class="s1">&#39;PERDIDA&#39;</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">capas</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">=</span> <span class="mi">0</span>
        
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activarTrazaTensor</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">epocas</span> <span class="o">=</span> <span class="mi">1</span> <span class="c1">#Se fuerzan condiciones de mínima actividad</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="nb">min</span><span class="p">(</span><span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">)</span> <span class="c1">#Se fuerzan condiciones de mínima actividad</span>
            <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;trazaCNN.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;w&#39;</span><span class="p">)</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s1">&#39;Inicio de la traza</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>

    <span class="k">def</span> <span class="nf">addLayer</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">layer</span><span class="p">):</span>
        <span class="k">assert</span> <span class="nb">type</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span> <span class="ow">in</span> <span class="p">[</span><span class="n">linealLayer</span><span class="p">,</span> <span class="n">conv2DLayer</span><span class="p">,</span> <span class="n">maxPooling</span><span class="p">,</span> <span class="n">flatten</span><span class="p">],</span> <span class="s2">&quot;Objeto layer no permitido&quot;</span>
        
        <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">layer</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">L</span> <span class="o">+=</span> <span class="mi">1</span>
        
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">validaLayers</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">,</span> <span class="s2">&quot;Debe tener alguna capa definida en la red neuronal&quot;</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tipActiva</span> <span class="o">==</span> <span class="s1">&#39;SOFTMAX&#39;</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizador</span> <span class="o">==</span> <span class="s1">&#39;ADAM&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;CROSSENTROPY&#39;</span><span class="p">,</span> <span class="s2">&quot;Optimizidor Adam debe ir con perdida CrossEntropy y en la última capa softmax&quot;</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="k">assert</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizador</span> <span class="o">==</span> <span class="s1">&#39;GRAD&#39;</span> <span class="ow">and</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;ERRORCUADRATICO&#39;</span><span class="p">,</span> <span class="s2">&quot;Optimizidor GRAD (gradiente descenso) debe ir con perdida errorCuadratico y en la última capa distinto de softmax&quot;</span>
        
        <span class="k">return</span>


    <span class="k">def</span> <span class="nf">fit</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">,</span> <span class="n">y_inicial</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot; Ajuste con los datos de entrenamiento.</span>
<span class="sd">        Parametros</span>
<span class="sd">        ----------</span>
<span class="sd">        X : {Tipo array}, shape = [n_ejemplo, n_caracteristicas]</span>
<span class="sd">        Vectores de entrenamiento, donde n_ejemplo es el numero de ejemplos y </span>
<span class="sd">        n_caracteristicas es el número de características.</span>
<span class="sd">        y : tipo array, shape = [n_ejemplo] Valores Objetivo.</span>
<span class="sd">        Retorno</span>
<span class="sd">        -------</span>
<span class="sd">        self : objecto</span>
<span class="sd">        &quot;&quot;&quot;</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">X</span><span class="p">)</span> <span class="k">if</span> <span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">==</span> <span class="mi">0</span><span class="p">)</span> <span class="k">else</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">Clases_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y_inicial</span><span class="p">)</span>
        <span class="n">y</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">onehot</span><span class="p">(</span><span class="n">y_inicial</span><span class="p">,</span> <span class="nb">len</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">Clases_y</span><span class="p">))</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">coste</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">validaLayers</span><span class="p">()</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizador</span> <span class="o">==</span> <span class="s1">&#39;ADAM&#39;</span><span class="p">:</span> <span class="bp">self</span><span class="o">.</span><span class="n">initAdam</span><span class="p">()</span>
                
        <span class="k">for</span> <span class="n">t</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">epocas</span><span class="p">):</span>
            <span class="c1"># iterate over minibatches</span>
            <span class="n">cost</span><span class="o">=</span><span class="mi">0</span>
            <span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">shuffle</span><span class="p">:</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">random</span><span class="o">.</span><span class="n">shuffle</span><span class="p">(</span><span class="n">indices</span><span class="p">)</span>
            <span class="n">lstIndices</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">)])</span>
            <span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="n">lstIndices</span><span class="p">:</span>
                <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span> <span class="o">+</span> <span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">Z</span> <span class="o">=</span> <span class="p">[]</span>  <span class="c1">## lista donde guarda las salidas del sumatorio en cada capa</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">A</span> <span class="o">=</span> <span class="p">[</span><span class="n">X</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]]</span> <span class="c1">## lista para guardar la activación de cada capa. En la capa 1 es la X, en resto f(W(X))</span>
                <span class="c1">### Primero se hace el avance hacia adelante</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;IMAGEN INICIAL&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
                    <span class="n">sumatorioPesos</span><span class="p">,</span> <span class="n">activacion</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">])</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">sumatorioPesos</span><span class="p">)</span>  <span class="c1">## Se guarda la lista de sumatorios en A</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">activacion</span><span class="p">)</span>  <span class="c1">## La activación de esta capa es la entrada de la próxima</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;ACTIVACION[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                <span class="c1">## Se obtiene la matriz Delta en la capa L con la tasa de cambio del error</span>
                
                <span class="c1">#probs = np.asarray([self.softmax(sp) for sp in sumatorioPesos]) # predict class probabilities with the softmax activation function</span>
                <span class="n">probs</span> <span class="o">=</span> <span class="n">activacion</span>
                <span class="n">errors</span> <span class="o">=</span> <span class="n">probs</span> <span class="o">-</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span>   <span class="c1">## Los errores y - la activacion de la ultima capa</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">loss</span> <span class="o">==</span> <span class="s1">&#39;CROSSENTROPY&#39;</span><span class="p">:</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">categoricalCrossEntropy</span><span class="p">(</span><span class="n">probs</span><span class="p">,</span> <span class="n">y</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span> <span class="c1"># categorical cross-entropy loss</span>
                    <span class="n">delta</span><span class="o">=</span> <span class="n">errors</span>  <span class="c1">## Delta de la función CrossEntropy + SoftMax</span>
                <span class="k">else</span><span class="p">:</span> <span class="c1">## &#39;errorCuadratico&#39;</span>
                    <span class="n">loss</span> <span class="o">=</span> <span class="p">(</span><span class="n">errors</span><span class="o">**</span><span class="mi">2</span><span class="p">)</span><span class="o">.</span><span class="n">sum</span><span class="p">()</span> <span class="o">/</span> <span class="p">(</span><span class="mf">2.0</span> <span class="o">*</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
                    <span class="n">delta</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">activacionPrima</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tipActiva</span><span class="p">,</span> <span class="n">errors</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;PROBS&quot;</span><span class="p">,</span> <span class="n">probs</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;LOSS&quot;</span><span class="p">,</span> <span class="n">loss</span><span class="p">)</span>
                
                <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;DELTA ULTIMA&quot;</span><span class="p">,</span> <span class="n">delta</span><span class="p">)</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">deltas</span> <span class="o">=</span> <span class="p">[</span><span class="n">delta</span><span class="p">]</span> <span class="c1">## delta de la capa última, la capa L</span>

                <span class="c1">### Retropropagación. Calcula desde la capa L-1 hacia atrás la matriz Delta </span>
                <span class="c1">## propagando la tasa de cambio del error obtenido en la capa L</span>
                <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">reversed</span><span class="p">(</span><span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">)):</span>
                    <span class="bp">self</span><span class="o">.</span><span class="n">backwardProceso</span><span class="p">(</span><span class="n">l</span><span class="p">,</span> <span class="n">t</span><span class="o">+</span><span class="mi">1</span><span class="p">)</span>
                    
                <span class="n">cost</span> <span class="o">=</span> <span class="n">loss</span> <span class="o">/</span> <span class="n">X</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">]</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span>
                <span class="n">lstIndices</span><span class="o">.</span><span class="n">set_description</span><span class="p">(</span><span class="s2">&quot;Época: </span><span class="si">%d</span><span class="s2"> Coste: </span><span class="si">%.4f</span><span class="s2">&quot;</span> <span class="o">%</span> <span class="p">(</span><span class="n">t</span><span class="p">,</span> <span class="n">cost</span><span class="p">))</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">coste</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">cost</span><span class="p">)</span>
                <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activarTrazaTensor</span> <span class="o">==</span> <span class="kc">True</span><span class="p">:</span>
                    <span class="k">return</span> <span class="c1">## Se fuerzan condiciones de mínima actividad</span>
                
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">backwardProceso</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">l</span><span class="p">,</span> <span class="n">t</span><span class="p">):</span>
        <span class="n">dA</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">dB</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">backward</span><span class="p">(</span><span class="n">dA</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">deltas</span><span class="p">[</span><span class="o">-</span><span class="mi">1</span><span class="p">],</span> <span class="n">Aprev</span><span class="o">=</span><span class="bp">self</span><span class="o">.</span><span class="n">A</span><span class="p">[</span><span class="n">l</span><span class="p">],</span> <span class="n">l</span><span class="o">=</span><span class="n">l</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;dA[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;dW[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="n">dW</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;dB[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="n">dB</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
        <span class="k">if</span> <span class="n">l</span><span class="o">&gt;</span><span class="mi">0</span><span class="p">:</span>  <span class="c1">## Se añade la activación derivada de la capa anterior para completar la retro-progración de dA</span>
            <span class="n">dA</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">activacionPrima</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">]</span><span class="o">.</span><span class="n">tipActiva</span><span class="p">,</span> <span class="n">dA</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">Z</span><span class="p">[</span><span class="n">l</span><span class="o">-</span><span class="mi">1</span><span class="p">])</span> <span class="c1">## self.Z[l-1] es Zprev</span>
        <span class="bp">self</span><span class="o">.</span><span class="n">deltas</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">dA</span><span class="p">)</span>

        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">pesos</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>  <span class="c1">## La capa tiene pesos (W y B) que actualizar</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">optimizador</span> <span class="o">==</span> <span class="s1">&#39;ADAM&#39;</span><span class="p">:</span> <span class="c1">## Optimización por el algorimo de Adam</span>
                <span class="n">beta1</span><span class="o">=</span><span class="mf">0.95</span>
                <span class="n">beta2</span><span class="o">=</span><span class="mf">0.99</span>
                <span class="n">epsilon</span><span class="o">=</span><span class="mf">1e-7</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_moment</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">dW</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_RMS</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_RMS</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dW</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_moment</span> <span class="o">=</span> <span class="n">beta1</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_moment</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta1</span><span class="p">)</span><span class="o">*</span><span class="n">dB</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_RMS</span> <span class="o">=</span> <span class="n">beta2</span><span class="o">*</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_RMS</span> <span class="o">+</span> <span class="p">(</span><span class="mi">1</span><span class="o">-</span><span class="n">beta2</span><span class="p">)</span><span class="o">*</span><span class="p">(</span><span class="n">dB</span><span class="o">/</span><span class="bp">self</span><span class="o">.</span><span class="n">minibatch_size</span><span class="p">)</span><span class="o">**</span><span class="mi">2</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_moment</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_RMS</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_moment</span><span class="o">/</span><span class="p">(</span><span class="n">np</span><span class="o">.</span><span class="n">sqrt</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_RMS</span><span class="p">)</span><span class="o">+</span><span class="n">epsilon</span><span class="p">)</span>
            <span class="k">else</span><span class="p">:</span>  <span class="c1">## Gradiente Descenso estándar</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">dW</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span> <span class="o">-=</span> <span class="bp">self</span><span class="o">.</span><span class="n">eta</span> <span class="o">*</span> <span class="n">dB</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;UPDATED W[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>
            <span class="bp">self</span><span class="o">.</span><span class="n">trazaTensor</span><span class="p">(</span><span class="s2">&quot;UPDATED B[&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">l</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;]&quot;</span><span class="p">,</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">4</span><span class="p">)</span>

    <span class="k">def</span> <span class="nf">initAdam</span><span class="p">(</span><span class="bp">self</span><span class="p">):</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">pesos</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>  <span class="c1">## La capa tiene pesos (W y B) que actualizar</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_moment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W_RMS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="p">)</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_moment</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span><span class="p">)</span> 
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B_RMS</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">zeros_like</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span><span class="p">)</span> 
        
        <span class="k">return</span>
    
    <span class="k">def</span> <span class="nf">predict</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return class label after unit step&quot;&quot;&quot;</span>
        <span class="c1"># Se aplica un forward a través de todas las capas</span>
        <span class="n">A_</span><span class="o">=</span><span class="n">X</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="n">Z</span><span class="p">,</span> <span class="n">A_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">A_</span><span class="p">)</span>

        <span class="n">i_labels</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">argmax</span><span class="p">(</span><span class="n">A_</span><span class="p">,</span> <span class="n">axis</span><span class="o">=</span><span class="mi">1</span><span class="p">)</span>
        <span class="n">y_pred</span> <span class="o">=</span> <span class="p">[</span><span class="bp">self</span><span class="o">.</span><span class="n">Clases_y</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="n">i_labels</span><span class="p">]</span> 
        
        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">array</span><span class="p">(</span><span class="n">y_pred</span><span class="p">)</span>
    
    <span class="k">def</span> <span class="nf">predict_proba</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">X</span><span class="p">):</span>
<span class="w">        </span><span class="sd">&quot;&quot;&quot;Return la probabilidad de cada clase&quot;&quot;&quot;</span>
        <span class="c1"># Se aplica un forward a través de todas las capas</span>
        <span class="n">A_</span><span class="o">=</span><span class="n">X</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="n">Z</span><span class="p">,</span> <span class="n">A_</span> <span class="o">=</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">forward</span><span class="p">(</span><span class="n">A_</span><span class="p">)</span>
        
        <span class="n">probs</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span><span class="bp">self</span><span class="o">.</span><span class="n">softmax</span><span class="p">(</span><span class="n">sp</span><span class="p">)</span> <span class="k">for</span> <span class="n">sp</span> <span class="ow">in</span> <span class="n">Z</span><span class="p">])</span>
        
        <span class="k">return</span> <span class="n">probs</span>
    <span class="k">def</span> <span class="nf">accuracy</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">ypred</span><span class="p">,</span> <span class="n">yok</span><span class="p">):</span>
        
        <span class="k">assert</span> <span class="nb">len</span><span class="p">(</span><span class="n">ypred</span><span class="p">)</span><span class="o">==</span><span class="nb">len</span><span class="p">(</span><span class="n">yok</span><span class="p">)</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">ypred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span> <span class="ow">and</span> <span class="nb">len</span><span class="p">(</span><span class="n">yok</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span><span class="o">==</span><span class="mi">1</span><span class="p">,</span> <span class="s2">&quot;ypre, yok no son array 1D de igual longitud. ypred=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">ypred</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> <span class="o">+</span> <span class="s2">&quot;yok=&quot;</span> <span class="o">+</span> <span class="nb">str</span><span class="p">(</span><span class="n">yok</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span> 

        <span class="k">return</span> <span class="n">np</span><span class="o">.</span><span class="n">sum</span><span class="p">(</span><span class="n">ypred</span><span class="o">==</span><span class="n">yok</span><span class="p">)</span><span class="o">/</span><span class="nb">len</span><span class="p">(</span><span class="n">ypred</span><span class="p">)</span>   
    
    <span class="k">def</span> <span class="nf">writeModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span><span class="p">):</span>
        <span class="n">W</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="n">B</span> <span class="o">=</span> <span class="p">[]</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">pesos</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>  <span class="c1">## La capa tiene pesos (W y B) que actualizar</span>
                <span class="n">W</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span><span class="p">)</span>
                <span class="n">B</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span><span class="p">)</span>
        
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;wb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="n">pickle</span><span class="o">.</span><span class="n">dump</span><span class="p">([</span><span class="n">W</span><span class="p">,</span><span class="n">B</span><span class="p">],</span> <span class="n">file</span><span class="p">)</span>
        
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">readModel</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">fileName</span><span class="p">):</span>
        <span class="k">with</span> <span class="nb">open</span><span class="p">(</span><span class="n">fileName</span><span class="p">,</span> <span class="s1">&#39;rb&#39;</span><span class="p">)</span> <span class="k">as</span> <span class="n">file</span><span class="p">:</span>
            <span class="p">[</span><span class="n">W</span><span class="p">,</span><span class="n">B</span><span class="p">]</span> <span class="o">=</span> <span class="n">pickle</span><span class="o">.</span><span class="n">load</span><span class="p">(</span><span class="n">file</span><span class="p">)</span>

        <span class="n">i</span><span class="o">=</span><span class="mi">0</span>
        <span class="k">for</span> <span class="n">l</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="bp">self</span><span class="o">.</span><span class="n">L</span><span class="p">):</span>
            <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">pesos</span><span class="o">==</span><span class="kc">True</span><span class="p">:</span>  <span class="c1">## La capa tiene pesos (W y B) que actualizar</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">W</span> <span class="o">=</span> <span class="n">W</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="bp">self</span><span class="o">.</span><span class="n">capas</span><span class="p">[</span><span class="n">l</span><span class="p">]</span><span class="o">.</span><span class="n">B</span> <span class="o">=</span> <span class="n">B</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
                <span class="n">i</span><span class="o">+=</span><span class="mi">1</span>
        
        <span class="k">return</span>

    <span class="k">def</span> <span class="nf">trazaTensor</span><span class="p">(</span><span class="bp">self</span><span class="p">,</span> <span class="n">literal</span><span class="p">,</span> <span class="n">T</span><span class="p">,</span> <span class="n">nDecimal</span> <span class="o">=</span> <span class="mi">2</span><span class="p">):</span>
        <span class="k">if</span> <span class="bp">self</span><span class="o">.</span><span class="n">activarTrazaTensor</span> <span class="o">==</span> <span class="kc">False</span><span class="p">:</span>
            <span class="k">return</span>
        
        <span class="n">f</span> <span class="o">=</span> <span class="nb">open</span><span class="p">(</span><span class="s1">&#39;trazaCNN.txt&#39;</span><span class="p">,</span> <span class="s1">&#39;a&#39;</span><span class="p">)</span>
        
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="n">literal</span> <span class="o">+</span> <span class="s1">&#39;</span><span class="se">\n</span><span class="s1">&#39;</span><span class="p">)</span>
        
        <span class="k">if</span> <span class="ow">not</span> <span class="nb">type</span><span class="p">(</span><span class="n">T</span><span class="p">)</span>  <span class="ow">in</span> <span class="p">[</span><span class="n">np</span><span class="o">.</span><span class="n">ndarray</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float64</span><span class="p">,</span> <span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">]:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">T</span><span class="p">))</span>
            <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
            <span class="k">return</span>
        
        <span class="n">n</span> <span class="o">=</span> <span class="nb">len</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
        <span class="k">if</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">4</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>
                        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;[&quot;</span><span class="p">)</span>
                        <span class="k">for</span> <span class="n">m</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">3</span><span class="p">]):</span>
                            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">][</span><span class="n">m</span><span class="p">],</span> <span class="n">nDecimal</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
                        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;] </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;-----------------------------------------</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">3</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;[&quot;</span><span class="p">)</span>
                    <span class="k">for</span> <span class="n">k</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]):</span>
                        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">][</span><span class="n">k</span><span class="p">],</span> <span class="n">nDecimal</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;] </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;-----------------------------------------</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">2</span><span class="p">):</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;[&quot;</span><span class="p">)</span>
                <span class="k">for</span> <span class="n">j</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">]):</span>
                    <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">][</span><span class="n">j</span><span class="p">],</span> <span class="n">nDecimal</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;] </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">elif</span> <span class="p">(</span><span class="n">n</span><span class="o">==</span><span class="mi">1</span><span class="p">):</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;[&quot;</span><span class="p">)</span>
            <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">T</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]):</span>
                <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="nb">round</span><span class="p">(</span><span class="n">T</span><span class="p">[</span><span class="n">i</span><span class="p">],</span> <span class="n">nDecimal</span><span class="p">))</span> <span class="o">+</span> <span class="s2">&quot; &quot;</span><span class="p">)</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;] </span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="k">else</span><span class="p">:</span>
            <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="nb">str</span><span class="p">(</span><span class="n">T</span><span class="p">))</span>
        
        <span class="n">f</span><span class="o">.</span><span class="n">write</span><span class="p">(</span><span class="s2">&quot;==========================================</span><span class="se">\n</span><span class="s2">&quot;</span><span class="p">)</span>
        <span class="n">f</span><span class="o">.</span><span class="n">close</span><span class="p">()</span>
        <span class="k">return</span>
</pre></div>
</div>
</div>
</details>
</div>
</section>
<section id="prueba-integral-con-la-arquitectura-lenet-5-y-el-conjunto-mnist">
<h2>Prueba Integral con la arquitectura LeNet-5 y el conjunto MNIST<a class="headerlink" href="#prueba-integral-con-la-arquitectura-lenet-5-y-el-conjunto-mnist" title="Link to this heading">#</a></h2>
<p>Se implementa la arquitectura LeNet-5 (LeCun et al.; 1998)</p>
<a class="reference internal image-reference" href="_images/LeNet5.png"><img alt="_images/LeNet5.png" class="align-center" src="_images/LeNet5.png" style="width: 750px;" /></a>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="kn">import</span> <span class="nn">torch</span>
<span class="kn">from</span> <span class="nn">torchvision</span> <span class="kn">import</span> <span class="n">datasets</span><span class="p">,</span> <span class="n">transforms</span>
<span class="kn">import</span> <span class="nn">torchvision.transforms.functional</span> <span class="k">as</span> <span class="nn">fn</span>

<span class="c1"># Download and load the test data</span>
<span class="n">w_dataset</span> <span class="o">=</span> <span class="n">datasets</span><span class="o">.</span><span class="n">MNIST</span><span class="p">(</span><span class="s1">&#39;data&#39;</span><span class="p">,</span> <span class="n">download</span><span class="o">=</span><span class="kc">True</span><span class="p">,</span> <span class="n">train</span><span class="o">=</span><span class="kc">False</span><span class="p">,</span> <span class="n">transform</span><span class="o">=</span><span class="n">transforms</span><span class="o">.</span><span class="n">ToTensor</span><span class="p">())</span>
<span class="n">dataResized</span> <span class="o">=</span> <span class="n">fn</span><span class="o">.</span><span class="n">resize</span><span class="p">(</span><span class="n">w_dataset</span><span class="o">.</span><span class="n">data</span><span class="p">,</span> <span class="mi">32</span><span class="p">,</span><span class="n">antialias</span><span class="o">=</span><span class="kc">True</span><span class="p">)</span>
<span class="n">dataCifar</span> <span class="o">=</span> <span class="n">dataResized</span><span class="o">.</span><span class="n">data</span><span class="o">.</span><span class="n">numpy</span><span class="p">()</span>
<span class="n">dataCifar</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(10000, 32, 32)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="c1">## Se añade una dimensión para el canal</span>
<span class="n">nTotal</span> <span class="o">=</span> <span class="mi">2000</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">empty</span><span class="p">((</span><span class="n">nTotal</span><span class="p">,</span> <span class="n">dataCifar</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">1</span><span class="p">],</span> <span class="n">dataCifar</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">2</span><span class="p">]),</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="c1">#X = np.empty(dataCifar.shape)</span>


<span class="c1">## Se crea la matriz de labels</span>
<span class="n">y</span> <span class="o">=</span> <span class="p">[]</span>
<span class="c1">#for i in range(w_dataset.data.size()[0]):</span>
<span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="n">nTotal</span><span class="p">):</span>
    <span class="n">_x</span><span class="p">,</span> <span class="n">t</span> <span class="o">=</span> <span class="n">w_dataset</span><span class="o">.</span><span class="fm">__getitem__</span><span class="p">(</span><span class="n">i</span><span class="p">)</span>
    <span class="n">y</span><span class="o">.</span><span class="n">append</span><span class="p">(</span><span class="n">t</span><span class="p">)</span>
    <span class="n">X</span><span class="p">[</span><span class="n">i</span><span class="p">]</span> <span class="o">=</span> <span class="n">dataCifar</span><span class="p">[</span><span class="n">i</span><span class="p">]</span>
<span class="n">y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="n">X</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">asarray</span><span class="p">([</span> <span class="p">[</span><span class="n">_x</span><span class="p">]</span> <span class="k">for</span> <span class="n">_x</span> <span class="ow">in</span> <span class="n">X</span> <span class="p">],</span> <span class="n">dtype</span><span class="o">=</span><span class="n">np</span><span class="o">.</span><span class="n">float32</span><span class="p">)</span>
<span class="nb">print</span><span class="p">(</span><span class="s1">&#39;X.shape=&#39;</span><span class="p">,</span> <span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">)</span>
<span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>X.shape= (2000, 1, 32, 32)
</pre></div>
</div>
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])
</pre></div>
</div>
</div>
</div>
<section id="se-define-el-modelo">
<h3>Se define el modelo<a class="headerlink" href="#se-define-el-modelo" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">(</span><span class="n">minibatch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epocas</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">optimizador</span><span class="o">=</span><span class="s1">&#39;Adam&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;crossEntropy&#39;</span><span class="p">)</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">conv2DLayer</span><span class="p">(</span><span class="n">canalInp</span><span class="o">=</span><span class="mi">1</span><span class="p">,</span> <span class="n">canalOut</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">fSize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">maxPooling</span><span class="p">(</span><span class="n">pSize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">conv2DLayer</span><span class="p">(</span><span class="n">canalInp</span><span class="o">=</span><span class="mi">6</span><span class="p">,</span> <span class="n">canalOut</span><span class="o">=</span><span class="mi">16</span><span class="p">,</span> <span class="n">fSize</span><span class="o">=</span><span class="mi">5</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">maxPooling</span><span class="p">(</span><span class="n">pSize</span><span class="o">=</span><span class="mi">2</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">flatten</span><span class="p">(</span><span class="n">shapeInp</span><span class="o">=</span><span class="p">(</span><span class="mi">16</span><span class="p">,</span><span class="mi">5</span><span class="p">,</span><span class="mi">5</span><span class="p">),</span> <span class="n">sizeOut</span><span class="o">=</span><span class="mi">400</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">400</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">120</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">84</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;ReLU&#39;</span><span class="p">))</span>
<span class="n">model</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">84</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;softmax&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
</section>
<section id="se-realiza-el-entrenamiento">
<h3>Se realiza el entrenamiento<a class="headerlink" href="#se-realiza-el-entrenamiento" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">entrenamiento</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;modo de entrenamiento no activo&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>modo de entrenamiento no activo
</pre></div>
</div>
</div>
</div>
</section>
<section id="serializacion-del-modelo-input-output-segun-estado">
<h3>Serialización del modelo (input/output según estado)<a class="headerlink" href="#serializacion-del-modelo-input-output-segun-estado" title="Link to this heading">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">entrenamiento</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">writeModel</span><span class="p">(</span><span class="s1">&#39;./data/modeloLetNet5.pt&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;modelo guardado en fichero&#39;</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="n">model</span><span class="o">.</span><span class="n">readModel</span><span class="p">(</span><span class="s1">&#39;./data/modeloLetNet5.pt&#39;</span><span class="p">)</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;modelo recuperado del fichero&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>modelo recuperado del fichero
</pre></div>
</div>
</div>
</div>
</section>
<section id="calculo-del-ratio-de-precision-en-el-conjunto-de-entrenamiento">
<h3>Cálculo del ratio de precisión en el conjunto de entrenamiento<a class="headerlink" href="#calculo-del-ratio-de-precision-en-el-conjunto-de-entrenamiento" title="Link to this heading">#</a></h3>
<div class="cell tag_hide-output docutils container">
<div class="cell_input above-output-prompt docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">minibatch_size</span> <span class="o">=</span> <span class="mi">20</span>
<span class="n">indices</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">arange</span><span class="p">(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">])</span>
<span class="n">first</span><span class="o">=</span><span class="kc">True</span>
<span class="n">lstIndices</span> <span class="o">=</span> <span class="n">tqdm</span><span class="p">([</span><span class="n">i</span> <span class="k">for</span> <span class="n">i</span> <span class="ow">in</span> <span class="nb">range</span><span class="p">(</span><span class="mi">0</span><span class="p">,</span> <span class="n">indices</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">]</span> <span class="o">-</span> <span class="n">minibatch_size</span> <span class="o">+</span> <span class="mi">1</span><span class="p">,</span> <span class="n">minibatch_size</span><span class="p">)])</span>
<span class="n">model</span><span class="o">.</span><span class="n">Clases_y</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">unique</span><span class="p">(</span><span class="n">y</span><span class="p">)</span>
<span class="k">for</span> <span class="n">start_idx</span> <span class="ow">in</span> <span class="n">lstIndices</span><span class="p">:</span>
    <span class="n">batch_idx</span> <span class="o">=</span> <span class="n">indices</span><span class="p">[</span><span class="n">start_idx</span><span class="p">:</span><span class="n">start_idx</span> <span class="o">+</span> <span class="n">minibatch_size</span><span class="p">]</span>
    <span class="n">y_pred2</span> <span class="o">=</span> <span class="n">model</span><span class="o">.</span><span class="n">predict</span><span class="p">(</span><span class="n">X</span><span class="p">[</span><span class="n">batch_idx</span><span class="p">])</span>
    <span class="n">y_pred</span> <span class="o">=</span> <span class="n">y_pred2</span> <span class="k">if</span> <span class="n">first</span> <span class="k">else</span> <span class="n">np</span><span class="o">.</span><span class="n">concatenate</span><span class="p">((</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y_pred2</span><span class="p">),</span><span class="n">axis</span><span class="o">=</span><span class="mi">0</span><span class="p">)</span>
    <span class="n">first</span><span class="o">=</span><span class="kc">False</span>
</pre></div>
</div>
</div>
<details class="hide below-input">
<summary aria-label="Toggle hidden content">
<span class="collapsed">Show code cell output</span>
<span class="expanded">Hide code cell output</span>
</summary>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  0%|                                                                                                                                                    | 0/100 [00:00&lt;?, ?it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  1%|█▍                                                                                                                                          | 1/100 [00:00&lt;00:35,  2.76it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  2%|██▊                                                                                                                                         | 2/100 [00:00&lt;00:34,  2.81it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  3%|████▏                                                                                                                                       | 3/100 [00:01&lt;00:34,  2.80it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  4%|█████▌                                                                                                                                      | 4/100 [00:01&lt;00:34,  2.79it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  5%|███████                                                                                                                                     | 5/100 [00:01&lt;00:34,  2.77it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  6%|████████▍                                                                                                                                   | 6/100 [00:02&lt;00:34,  2.72it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  7%|█████████▊                                                                                                                                  | 7/100 [00:02&lt;00:38,  2.43it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  8%|███████████▏                                                                                                                                | 8/100 [00:03&lt;00:41,  2.21it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>  9%|████████████▌                                                                                                                               | 9/100 [00:03&lt;00:43,  2.07it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 10%|█████████████▉                                                                                                                             | 10/100 [00:04&lt;00:44,  2.04it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 11%|███████████████▎                                                                                                                           | 11/100 [00:04&lt;00:44,  2.02it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 12%|████████████████▋                                                                                                                          | 12/100 [00:05&lt;00:44,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 13%|██████████████████                                                                                                                         | 13/100 [00:05&lt;00:45,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 14%|███████████████████▍                                                                                                                       | 14/100 [00:06&lt;00:44,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 15%|████████████████████▊                                                                                                                      | 15/100 [00:06&lt;00:44,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 16%|██████████████████████▏                                                                                                                    | 16/100 [00:07&lt;00:42,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 17%|███████████████████████▋                                                                                                                   | 17/100 [00:07&lt;00:41,  2.00it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 18%|█████████████████████████                                                                                                                  | 18/100 [00:08&lt;00:41,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 19%|██████████████████████████▍                                                                                                                | 19/100 [00:08&lt;00:42,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 20%|███████████████████████████▊                                                                                                               | 20/100 [00:09&lt;00:42,  1.89it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 21%|█████████████████████████████▏                                                                                                             | 21/100 [00:10&lt;00:41,  1.88it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 22%|██████████████████████████████▌                                                                                                            | 22/100 [00:10&lt;00:41,  1.88it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 23%|███████████████████████████████▉                                                                                                           | 23/100 [00:11&lt;00:39,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 24%|█████████████████████████████████▎                                                                                                         | 24/100 [00:11&lt;00:41,  1.85it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 25%|██████████████████████████████████▊                                                                                                        | 25/100 [00:12&lt;00:39,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 26%|████████████████████████████████████▏                                                                                                      | 26/100 [00:12&lt;00:39,  1.89it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 27%|█████████████████████████████████████▌                                                                                                     | 27/100 [00:13&lt;00:39,  1.85it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 28%|██████████████████████████████████████▉                                                                                                    | 28/100 [00:13&lt;00:37,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 29%|████████████████████████████████████████▎                                                                                                  | 29/100 [00:14&lt;00:36,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 30%|█████████████████████████████████████████▋                                                                                                 | 30/100 [00:14&lt;00:36,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 31%|███████████████████████████████████████████                                                                                                | 31/100 [00:15&lt;00:35,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 32%|████████████████████████████████████████████▍                                                                                              | 32/100 [00:15&lt;00:34,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 33%|█████████████████████████████████████████████▊                                                                                             | 33/100 [00:16&lt;00:35,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 34%|███████████████████████████████████████████████▎                                                                                           | 34/100 [00:16&lt;00:34,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 35%|████████████████████████████████████████████████▋                                                                                          | 35/100 [00:17&lt;00:34,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 36%|██████████████████████████████████████████████████                                                                                         | 36/100 [00:17&lt;00:33,  1.89it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 37%|███████████████████████████████████████████████████▍                                                                                       | 37/100 [00:18&lt;00:33,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 38%|████████████████████████████████████████████████████▊                                                                                      | 38/100 [00:18&lt;00:32,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 39%|██████████████████████████████████████████████████████▏                                                                                    | 39/100 [00:19&lt;00:31,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 40%|███████████████████████████████████████████████████████▌                                                                                   | 40/100 [00:19&lt;00:30,  1.95it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 41%|████████████████████████████████████████████████████████▉                                                                                  | 41/100 [00:20&lt;00:29,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 42%|██████████████████████████████████████████████████████████▍                                                                                | 42/100 [00:20&lt;00:30,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 43%|███████████████████████████████████████████████████████████▊                                                                               | 43/100 [00:21&lt;00:28,  1.99it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 44%|█████████████████████████████████████████████████████████████▏                                                                             | 44/100 [00:21&lt;00:28,  2.00it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 45%|██████████████████████████████████████████████████████████████▌                                                                            | 45/100 [00:22&lt;00:27,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 46%|███████████████████████████████████████████████████████████████▉                                                                           | 46/100 [00:22&lt;00:27,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 47%|█████████████████████████████████████████████████████████████████▎                                                                         | 47/100 [00:23&lt;00:26,  2.00it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 48%|██████████████████████████████████████████████████████████████████▋                                                                        | 48/100 [00:23&lt;00:26,  1.99it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 49%|████████████████████████████████████████████████████████████████████                                                                       | 49/100 [00:24&lt;00:25,  2.02it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 50%|█████████████████████████████████████████████████████████████████████▌                                                                     | 50/100 [00:24&lt;00:24,  2.02it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 51%|██████████████████████████████████████████████████████████████████████▉                                                                    | 51/100 [00:25&lt;00:24,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 52%|████████████████████████████████████████████████████████████████████████▎                                                                  | 52/100 [00:25&lt;00:23,  2.01it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 53%|█████████████████████████████████████████████████████████████████████████▋                                                                 | 53/100 [00:26&lt;00:24,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 54%|███████████████████████████████████████████████████████████████████████████                                                                | 54/100 [00:26&lt;00:22,  2.01it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 55%|████████████████████████████████████████████████████████████████████████████▍                                                              | 55/100 [00:27&lt;00:22,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 56%|█████████████████████████████████████████████████████████████████████████████▊                                                             | 56/100 [00:27&lt;00:22,  2.00it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 57%|███████████████████████████████████████████████████████████████████████████████▏                                                           | 57/100 [00:28&lt;00:21,  2.02it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 58%|████████████████████████████████████████████████████████████████████████████████▌                                                          | 58/100 [00:28&lt;00:21,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 59%|██████████████████████████████████████████████████████████████████████████████████                                                         | 59/100 [00:29&lt;00:20,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 60%|███████████████████████████████████████████████████████████████████████████████████▍                                                       | 60/100 [00:29&lt;00:20,  1.99it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 61%|████████████████████████████████████████████████████████████████████████████████████▊                                                      | 61/100 [00:30&lt;00:19,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 62%|██████████████████████████████████████████████████████████████████████████████████████▏                                                    | 62/100 [00:31&lt;00:19,  1.95it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 63%|███████████████████████████████████████████████████████████████████████████████████████▌                                                   | 63/100 [00:31&lt;00:19,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 64%|████████████████████████████████████████████████████████████████████████████████████████▉                                                  | 64/100 [00:32&lt;00:18,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 65%|██████████████████████████████████████████████████████████████████████████████████████████▎                                                | 65/100 [00:32&lt;00:18,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 66%|███████████████████████████████████████████████████████████████████████████████████████████▋                                               | 66/100 [00:33&lt;00:17,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 67%|█████████████████████████████████████████████████████████████████████████████████████████████▏                                             | 67/100 [00:33&lt;00:17,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 68%|██████████████████████████████████████████████████████████████████████████████████████████████▌                                            | 68/100 [00:34&lt;00:16,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 69%|███████████████████████████████████████████████████████████████████████████████████████████████▉                                           | 69/100 [00:34&lt;00:16,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 70%|█████████████████████████████████████████████████████████████████████████████████████████████████▎                                         | 70/100 [00:35&lt;00:15,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 71%|██████████████████████████████████████████████████████████████████████████████████████████████████▋                                        | 71/100 [00:35&lt;00:15,  1.90it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 72%|████████████████████████████████████████████████████████████████████████████████████████████████████                                       | 72/100 [00:36&lt;00:14,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 73%|█████████████████████████████████████████████████████████████████████████████████████████████████████▍                                     | 73/100 [00:36&lt;00:13,  1.95it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 74%|██████████████████████████████████████████████████████████████████████████████████████████████████████▊                                    | 74/100 [00:37&lt;00:12,  2.03it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 75%|████████████████████████████████████████████████████████████████████████████████████████████████████████▎                                  | 75/100 [00:37&lt;00:13,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 76%|█████████████████████████████████████████████████████████████████████████████████████████████████████████▋                                 | 76/100 [00:38&lt;00:12,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 77%|███████████████████████████████████████████████████████████████████████████████████████████████████████████                                | 77/100 [00:38&lt;00:11,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 78%|████████████████████████████████████████████████████████████████████████████████████████████████████████████▍                              | 78/100 [00:39&lt;00:11,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 79%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                             | 79/100 [00:39&lt;00:10,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 80%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                           | 80/100 [00:40&lt;00:10,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 81%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                          | 81/100 [00:40&lt;00:09,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 82%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                         | 82/100 [00:41&lt;00:09,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 83%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                       | 83/100 [00:41&lt;00:09,  1.87it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 84%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊                      | 84/100 [00:42&lt;00:08,  1.92it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 85%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏                    | 85/100 [00:42&lt;00:07,  1.94it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 86%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌                   | 86/100 [00:43&lt;00:07,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 87%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉                  | 87/100 [00:43&lt;00:06,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 88%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎                | 88/100 [00:44&lt;00:06,  1.93it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 89%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋               | 89/100 [00:44&lt;00:05,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 90%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████              | 90/100 [00:45&lt;00:05,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 91%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍            | 91/100 [00:45&lt;00:04,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 92%|███████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▉           | 92/100 [00:46&lt;00:04,  1.96it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 93%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▎         | 93/100 [00:47&lt;00:03,  1.95it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 94%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▋        | 94/100 [00:47&lt;00:03,  1.91it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 95%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████       | 95/100 [00:48&lt;00:02,  1.95it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 96%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▍     | 96/100 [00:48&lt;00:02,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 97%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▊    | 97/100 [00:49&lt;00:01,  2.02it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 98%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▏  | 98/100 [00:49&lt;00:01,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span> 99%|█████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████▌ | 99/100 [00:50&lt;00:00,  1.99it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:50&lt;00:00,  1.97it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:50&lt;00:00,  1.98it/s]
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</details>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="s2">&quot;Accuracy en entrenamiento=&quot;</span><span class="p">,</span> <span class="mi">100</span> <span class="o">*</span> <span class="n">model</span><span class="o">.</span><span class="n">accuracy</span><span class="p">(</span><span class="n">y_pred</span><span class="p">,</span><span class="n">y</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>(&#39;Accuracy en entrenamiento=&#39;, 85.95)
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="ejecucion-en-modo-perceptron">
<h2>Ejecución en modo Perceptron<a class="headerlink" href="#ejecucion-en-modo-perceptron" title="Link to this heading">#</a></h2>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">X2</span> <span class="o">=</span> <span class="n">np</span><span class="o">.</span><span class="n">reshape</span><span class="p">(</span><span class="n">X</span><span class="p">,(</span><span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">[</span><span class="mi">0</span><span class="p">],</span> <span class="mi">32</span><span class="o">*</span><span class="mi">32</span><span class="p">))</span>
<span class="n">X</span><span class="o">.</span><span class="n">shape</span><span class="p">,</span> <span class="n">X2</span><span class="o">.</span><span class="n">shape</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output text_plain highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>((2000, 1, 32, 32), (2000, 1024))
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="n">model_cd</span> <span class="o">=</span> <span class="n">CNN</span><span class="p">(</span><span class="n">minibatch_size</span><span class="o">=</span><span class="mi">32</span><span class="p">,</span> <span class="n">epocas</span><span class="o">=</span><span class="mi">30</span><span class="p">,</span> <span class="n">eta</span><span class="o">=</span><span class="mf">0.01</span><span class="p">,</span> <span class="n">optimizador</span><span class="o">=</span><span class="s1">&#39;grad&#39;</span><span class="p">,</span> <span class="n">loss</span><span class="o">=</span><span class="s1">&#39;errorCuadratico&#39;</span><span class="p">)</span>
<span class="n">model_cd</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">1024</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model_cd</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">256</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
<span class="n">model_cd</span><span class="o">.</span><span class="n">addLayer</span><span class="p">(</span><span class="n">linealLayer</span><span class="p">(</span><span class="n">neurInp</span><span class="o">=</span><span class="mi">64</span><span class="p">,</span> <span class="n">neurOut</span><span class="o">=</span><span class="mi">10</span><span class="p">,</span> <span class="n">tipActiva</span><span class="o">=</span><span class="s1">&#39;sigmoid&#39;</span><span class="p">))</span>
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span><span class="k">if</span> <span class="n">entrenamiento</span><span class="p">:</span>
    <span class="n">model_cd</span><span class="o">.</span><span class="n">fit</span><span class="p">(</span><span class="n">X</span><span class="p">,</span> <span class="n">y</span><span class="p">)</span>
<span class="k">else</span><span class="p">:</span>
    <span class="nb">print</span><span class="p">(</span><span class="s1">&#39;modo de entrenamiento no activo&#39;</span><span class="p">)</span>
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>modo de entrenamiento no activo
</pre></div>
</div>
</div>
</div>
</section>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            name: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

                </article>
              

              
              
              
              
                <footer class="prev-next-footer">
                  <!-- Previous / next buttons -->
<div class="prev-next-area">
    <a class="left-prev"
       href="03.8_RedesNeurConvolucional_pytorch.html"
       title="previous page">
      <i class="fa-solid fa-angle-left"></i>
      <div class="prev-next-info">
        <p class="prev-next-subtitle">previous</p>
        <p class="prev-next-title">Redes convoluciones con Pytorch</p>
      </div>
    </a>
    <a class="right-next"
       href="03.10_RedesNeurConv_PlantVillage.html"
       title="next page">
      <div class="prev-next-info">
        <p class="prev-next-subtitle">next</p>
        <p class="prev-next-title">PlantVillage. Cultivos con patologías</p>
      </div>
      <i class="fa-solid fa-angle-right"></i>
    </a>
</div>
                </footer>
              
            </div>
            
            
              
                <div class="bd-sidebar-secondary bd-toc"><div class="sidebar-secondary-items sidebar-secondary__inner">

  <div class="sidebar-secondary-item">
  <div class="page-toc tocsection onthispage">
    <i class="fa-solid fa-list"></i> Contents
  </div>
  <nav class="bd-toc-nav page-toc">
    <ul class="visible nav section-nav flex-column">
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#redes-convolucionales">Redes convolucionales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#fundamento-teorico">Fundamento teórico</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#el-proceso-de-retropropagacion">El proceso de retropropagación</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#justificacion-de-la-retropropagacion-en-convolucion">Justificación de la retropropagación en convolución</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#softmax-y-cross-entropy">Softmax y cross-entropy</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#optimizacion-adam">Optimización ADAM</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#funciones-generales">Funciones generales</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-densa">Clase para una capa densa</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-convolucional">Clase para una capa convolucional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-para-una-capa-de-agrupacion-por-maximo-max-pooling">Clase para una capa de agrupación por máximo (max-pooling)</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-de-aplanado">Clase de aplanado</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#clase-red-neuronal-convolucional">Clase red neuronal convolucional</a></li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#prueba-integral-con-la-arquitectura-lenet-5-y-el-conjunto-mnist">Prueba Integral con la arquitectura LeNet-5 y el conjunto MNIST</a><ul class="nav section-nav flex-column">
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#se-define-el-modelo">Se define el modelo</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#se-realiza-el-entrenamiento">Se realiza el entrenamiento</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#serializacion-del-modelo-input-output-segun-estado">Serialización del modelo (input/output según estado)</a></li>
<li class="toc-h3 nav-item toc-entry"><a class="reference internal nav-link" href="#calculo-del-ratio-de-precision-en-el-conjunto-de-entrenamiento">Cálculo del ratio de precisión en el conjunto de entrenamiento</a></li>
</ul>
</li>
<li class="toc-h2 nav-item toc-entry"><a class="reference internal nav-link" href="#ejecucion-en-modo-perceptron">Ejecución en modo Perceptron</a></li>
</ul>
  </nav></div>

</div></div>
              
            
          </div>
          <footer class="bd-footer-content">
            
<div class="bd-footer-content__inner container">
  
  <div class="footer-item">
    
<p class="component-author">
By Valeriano Germán Méndez Fuentes
</p>

  </div>
  
  <div class="footer-item">
    
  <p class="copyright">
    
      © Copyright 2024.
      <br/>
    
  </p>

  </div>
  
  <div class="footer-item">
    
  </div>
  
  <div class="footer-item">
    
  </div>
  
</div>
          </footer>
        

      </main>
    </div>
  </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/bootstrap.js?digest=365ca57ee442770a23c6"></script>
<script src="_static/scripts/pydata-sphinx-theme.js?digest=365ca57ee442770a23c6"></script>

  <footer class="bd-footer">
  </footer>
  </body>
</html>
# Red densa. Procesamiento en lote 

## Activación

__Si la entrada a la red__ está constituida por un bloque de $N$ observaciones almacenadas en la matriz $X$ la activación en una capa $l$ es:

$$ Z^l =   A^{l-1} \cdot W^l  \oplus B^l $$

$$A^l= f(Z^l) $$

Sabiendo que si $l=1$ entonces $A^{l-1} = X$


## Retro-propagación

El _gradiente delta__ en la __capa de salida__ $L$ se calcula para un lote de observaciones de la que se conoce su clasificación real $Y$ por:

__Matriz ${\large \Delta}$__: La tasa de variación de la función de pérdida o coste con respecto a la ponderación lineal de la capa de salida:

$$\frac{\partial{C}}{\partial{Z^L}}=\Delta = (A-Y) \odot f ' (Z)$$

En la obtención de la matriz ${\large \Delta}$ se utiliza el producto de Hadamard ($\odot$).

El __gradiente delta__ se __propaga__ hacia las __capas inferiores__ de acuerdo a:

$$\Delta^l = \Delta^{l+1} \cdot (W^{l+1})^T  \odot f'(Z^l) $$

## Entrenamiento

El __ajuste de la configuración__ de la capa $l$ por el impacto simultaneo de un lote de observaciones es:

$$W^l[t+1] = W^l[t] - \eta \begin{pmatrix} \Delta^l  \end{pmatrix}^T A^{l-1}$$
$$B^l[t+1] = B^l[t] - \eta \begin{pmatrix} \Delta^l  \end{pmatrix}^T   {\Large  1} $$


## Comentarios

La matriz ${\large X}$ tiene dimensiones __(N,n)__: $N$ filas (observaciones) de $n$ características cada una.

Las matrices ${\large Z, A, \Delta}$ tiene dimensiones __(N,m)__, tantas filas como las observaciones del conjunto de entrenamiento y tantas columnas como neuronas de salida $m$. El número de __neuronas de salida__ coincide con el __número de clases__. 


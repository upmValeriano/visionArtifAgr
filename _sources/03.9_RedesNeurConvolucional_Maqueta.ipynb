{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4b47535d",
   "metadata": {},
   "source": [
    "# Maqueta de red convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "632b2f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm \n",
    "import pickle\n",
    "import logging\n",
    "from scipy import signal as sg"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fbbf00b",
   "metadata": {},
   "source": [
    "__Configuración del proceso__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86795f5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "entrenamiento=False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5eeb1122",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Redes convolucionales\n",
    "\n",
    "<img src=\"images/Densa_vs_ConvNet.png\" width=\"800px\" align=\"center\">\n",
    "\n",
    "__Izquierda__: Una red neuronal normal de 3 capas. __Derecha__: Un ConvNet organiza sus neuronas en tres dimensiones (ancho, alto, profundidad), como se visualiza en una de las capas. Cada capa de un ConvNet transforma el volumen de entrada 3D en un volumen de salida 3D de activaciones neuronales. En este ejemplo, la capa de entrada roja contiene la imagen, por lo que su ancho y alto serían las dimensiones de la imagen, y la profundidad sería de 3 (canales rojo, verde, azul)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "589f50bf",
   "metadata": {},
   "source": [
    "Una __Arquitectura Convolucional__ tipo estará compuesta por las capas: [INPUT - CONV - RELU - POOL - FC]. Cuyo detalle es:\n",
    "\n",
    "- __INPUT__ [bxhx3] contendrá los valores de píxeles raw de la imagen, en este caso una imagen de ancho b, altura h, y con tres canales de color R, G, B.\n",
    "- La capa __CONV__ calculará la salida de las neuronas que están conectadas a las regiones locales en la entrada, cada una calculando un producto de punto entre sus pesos y una pequeña región a la que están conectadas en el volumen de entrada. Esto puede resultar en un volumen como [BxHxd] si decidimos usar d filtros. Los pesos que conectan los pixels localmente son compartidos y son __aprendibles__.\n",
    "- La capa __RELU__ aplicará una función de activación por elementos, como el umbral $(max(0,x))$ en cero. Esto deja el tamaño del volumen sin cambios ([BxHxd]).\n",
    "- La capa __POOL__ realizará una operación de __downsampling__ a lo largo de las dimensiones espaciales (ancho, alto), lo que dará como resultado un volumen como [bxhxd], siendo (b,h) inferiores a (B,H). Esta capa no utiliza pesos aprendibles.\n",
    "- La capa __FC o densa__  (es decir, totalmente conectada) calculará los puntajes de clase, lo que resultará en un volumen de tamaño [1x1xK], donde cada uno de los K números corresponde a un puntaje de clase, entre las K categorías del conjunto. Al igual que con las redes neuronales ordinarias y como su nombre indica, cada neurona en esta capa estará conectada a todos los números en el volumen anterior."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9c293bf",
   "metadata": {},
   "source": [
    "## Fundamento teórico\n",
    "\n",
    "Una red neuronal se puede considerar (LeCun et al.; 1998) un sistema construido como una cascada de módulos, cada uno de los cuales implementa una función\n",
    "\n",
    "$$X_l = F_l(W_l, X_{l-1})$$\n",
    "\n",
    "Se toma $l$ como el índicador de número de capa (layer en inglés).\n",
    "\n",
    "Dónde $X_l$ es un vector que representa la salida del módulo, $W_l$ es el vector de los parámetros entrenables del módulo (y que forma parte del conjunto total $W$) y $X_{l-1}$ es el vector de entrada al módulo (así como la salida del módulo previo).\n",
    "\n",
    "Se ha implementado una maqueta naíf que implementa 4 tipos de capas: densa o perceptrón, convolucional, agrupación máxima (max-pooling) y aplanado. La capa densa con una programación matricial en numpy se implementa de acuerdo a la documentación previa. La capa convolucional se restringe a convoluciones 2D sin relleno a ceros y con salto 1. La agrupación máxima usa un paso único en ambas direcciones de la imagen. El aplanado es un simple cambio de formato para pasar de las capas convolucionales a las densas.\n",
    "\n",
    "Al final de la última capa se aplica una función softmax para obtener las probabilidades de cada clase y la pérdida se calcula con una función de croos-entropy.\n",
    "\n",
    "También se __incorpora la optimación Adam__ que provee un ratio de aprendizaje modulado ({cite:p}`kingma2014adam`). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e0bfab",
   "metadata": {},
   "source": [
    "El vector de datos de entrada $X_0$ va a ser un tensor o matriz de 4 dimensiones __(n, c, h, b)__, donde __n__ representará el número de observaciones en el lote tratado, __c__ el número de canales, __h__ es la altura de la imagen y __b__ es el ancho de la imagen.\n",
    "\n",
    "Esta estructura de los vectores $X_l$ se mantiene durante los procesos convolucionales y de agrupación, mientras que las capas densas manejan tensores de la forma __(n, m)__, siendo __n__ el número de observaciones tratadas y __m__ las características de cada observación. Lógicamente, la capa de aplanado efectua reestructuración de las dimensiones con __m = c * h * b__.\n",
    "\n",
    "En una capa convolucional los parámetros entrenables están formados por un filtro $W_l$ cuyas dimensiones son __(co, ci, f, f)__, donde __co__ es el número de canales de salida (los canales del tensor $X_l$), __ci__ es el número de canales de entrada (los canales del tensor $X_{l-1}$) y __f__ es el tamaño del filtro. Además se incluye un parámetro de bias $B_l$, que es un vector de dimensiones __(co)__. \n",
    "\n",
    "Las capas de agrupación y aplanado no tienen parámetros entrenables.\n",
    "\n",
    "Las capas densas tienen una matriz entrenable $W_l$ de dos dimensiones __(m,p)__, siendo __m__ el nº de carácteristicas de la salida de la capa (el vector $X_l$) y __p__ el número de características de la entrada a la capa (el vector $X_{l-1}$)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ddfa904",
   "metadata": {},
   "source": [
    "## El proceso de retropropagación\n",
    "\n",
    "En cuadernos anteriores se ha justificado el proceso de retropropagación de las capas densas que se incluye en la maqueta. Aquí se va a justificar las fórmulas usadas a partir de un ejemplo teórico de dimensión reducida en las condiciones de la convolución expuestas. A continuación se muestra un gráfico del proceso de entrenamiento de un lote de 32 imágenes con la arquitectura LetNet5. En el se indican los pasos forward de cada una de las capas, el final de proceso donde se obtiene las probabilidades de clase con la función softmax y el cálculo de la función de pérdida a partir de la entropía cruzada ($C$).\n",
    "\n",
    "En este proceso final se obtiene el grandiente del coste con respecto a la combinación lineal de la última capa ($\\frac{\\partial C}{\\partial Z ^L}$), este gradiente que en el gráfico y en las rutinas programadas utiliza la nomeclatura __dA__ comparte la dimensión con los tensores __Z__ y __A__ de cada capa. El primer proceso de retropropagación (__BAC1__ en el esquema) arranca con __dA8__ que es el gradiente de coste de la capa 8. Hay que entender __Softmax__ como una activación específica. Por eso la entrada a esta función es __Z8__ la combinación lineal de la última capa. La salida de la función softmax para una observación __x__ es un vector probabilidad __p__ con la probabilidad de cada etiqueta. Para todo el lote es una matriz __P__ donde cada fila es la probabilidad por observación. Más adelante comprobaremos que este gradiente de la capa final cuando se usa __softmax__ unido a la función de coste de entropia cruzada resulta algo tan sintético como:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial Z ^L} = P - Y$$\n",
    "\n",
    "Siendo $P = softmax(Z^L)$ y $Y$ una matriz __one-hot__ de las etiquetas reales.\n",
    "\n",
    "Por tanto los procesos de back-propagación siempre tienen la misma estructura:\n",
    "- Partiendo del gradiente de coste con respecto a la ponderación lineal de su capa ($\\frac{\\partial C}{\\partial Z ^l}$), obtener o propagar el de la capa anterior ($\\frac{\\partial C}{\\partial Z ^{l-1}}$).\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial Z ^l} \\rightarrow \\frac{\\partial C}{\\partial Z ^{l-1}}$$\n",
    "\n",
    "- Obtiener el gradiente del coste con respecto a los pesos y bias de su capa, siempre y cuando sean capas entrenables:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W ^l}; \\frac{\\partial C}{\\partial B ^l}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62693b45",
   "metadata": {},
   "source": [
    "<img src=\"images/Esquema_FWDBAC.png\" width=\"800px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbc5fc3",
   "metadata": {},
   "source": [
    "## Justificación de la retropropagación en convolución\n",
    "\n",
    "Para realizar esta justificación suponemos una única observación y un único canal __(n = c = co = ci = 1)__.\n",
    "\n",
    "Además para simplificar las fórmulas al máximo se toma en $X_{n-1}$ __h=b=4__ en la entrada a la convolución y debido al filtro aplicado la salida se reduce en $X_n$ a __h=b=2__ (por no usar relleno a ceros). Además el tamaño del filtro es __f=3__.\n",
    "\n",
    "Dada una capa convolucional $l$ cuya entrada suponemos definida por una matriz\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  x_{11} & x_{12} & x_{13} & x_{14} \\\\\n",
    "  x_{21} & x_{22} & x_{23} & x_{24} \\\\\n",
    "  x_{31} & x_{32} & x_{33} & x_{34} \\\\\n",
    "  x_{41} & x_{42} & x_{43} & x_{44} \\\\\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Esta matriz puede estar representando los datos de una observación de la capa de entrada a la red neuronal o la salida de la activación de la capa $l-1$, que en su caso vendrá notada por $A^{l-1}$. \n",
    "\n",
    "En la capa $l$ tenemos definida una convolución simple (suponemos un canal de entrada y uno de salida) que requiere una matriz de pesos o filtro $W^l$ y una matriz de bias $B^l$, suponemos un filtro 3x3 y dado que suponemos un sólo canal el bias es una matriz unidimensional:\n",
    "\n",
    "$$W^l = \\begin{bmatrix}\n",
    "  w_{11} & w_{12} & w_{13} \\\\\n",
    "  w_{21} & w_{22} & w_{23} \\\\\n",
    "  w_{31} & w_{32} & w_{33} \\\\\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$B^l = \\begin{bmatrix}\n",
    "  b\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Por comodidad sólo se usa superindice con el orden de la capa en las matrices, pero en los elementos de las matrices no se utiliza superíndice, si no sólo el subindice correspondiente. Así se nota la matriz $W^l$ y sus elementos $w_{ij}$ en lugar de $w^l_{ij}$ por no complicar más la notación."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70cf7ab8",
   "metadata": {},
   "source": [
    "La matriz $Z^l = W^l \\otimes A^{l-1} \\oplus B^l$, resultado de aplicar el filtro convolucional y sumar el bias, viene dada por:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "x_{11} \\cdot w_{11} + x_{12} \\cdot w_{12}+x_{13} \\cdot w_{13}+x_{21} \\cdot w_{21}+x_{22} \\cdot w_{22}+x_{23} \\cdot w_{23}+x_{31} \\cdot w_{31}+x_{32} \\cdot w_{32}+x_{33} \\cdot w_{33}+b &\n",
    "x_{12} \\cdot w_{11} + x_{13} \\cdot w_{12} + x_{14} \\cdot w_{13} + x_{22} \\cdot w_{21} + x_{23} \\cdot w_{22} + x_{24} \\cdot w_{23} + x_{32} \\cdot w_{31} + x_{33} \\cdot w_{32} + x_{34} \\cdot w_{33} + b \\\\\n",
    "x_{21} \\cdot w_{11} + x_{22} \\cdot w_{12} + x_{23} \\cdot w_{13} + x_{31} \\cdot w_{21} + x_{32} \\cdot w_{22} + x_{33} \\cdot w_{23} + x_{41} \\cdot w_{31} + x_{42} \\cdot w_{32} + x_{43} \\cdot w_{33} + b &\n",
    "x_{22} \\cdot w_{11} + x_{23} \\cdot w_{12} + x_{24} \\cdot w_{13} + x_{32} \\cdot w_{21} + x_{33} \\cdot w_{22} + x_{34} \\cdot w_{23} + x_{42} \\cdot w_{31} + x_{43} \\cdot w_{32} + x_{44} \\cdot w_{33} + b\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Si se aplica la función de activación resulta $A^l = f(Z^l) = \\begin{bmatrix} a_{11} & a_{12} \\\\ a_{21}  & a_{22} \\end{bmatrix} $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6cdb89cc",
   "metadata": {},
   "source": [
    "En el proceso de retropropagación, al procesar la capa $l+1$ se habrá retropropagado una matriz con el gradiente del coste con relación a la ponderación lineal $Z^l$, con igual dimensión que $Z^l$ y $A^l$ y que se nota:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial Z^l} = \\begin{bmatrix} \\delta_{11}  & \\delta_{12}  \\\\ \\delta_{21}  & \\delta_{22} \\end{bmatrix} =  \n",
    "\\begin{bmatrix} \\frac{\\partial C}{\\partial z_{11}}  & \\frac{\\partial C}{\\partial z_{12}}  \\\\ \\frac{\\partial C}{\\partial z_{21}}  & \\frac{\\partial C}{\\partial z_{22}} \\end{bmatrix}\n",
    "$$ \n",
    "\n",
    "El objetivo del proceso de retropropagación de la capa $l$ es obtener el gradiente del coste en relación a los pesos $W^l$ y del bias $B^l$, estos gradientes tiene igual dimensión que $W^l$ y $B^l$. Se observa en la notación que es una __matriz jacobiana__ con las derivadas primeras del coste con respecto a cada peso:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial W^l} = \\begin{bmatrix}\n",
    "  \\frac{\\partial C}{\\partial w_{11}} & \\frac{\\partial C}{\\partial w_{12}} & \\frac{\\partial C}{\\partial w_{13}} \\\\\n",
    "  \\frac{\\partial C}{\\partial w_{21}} & \\frac{\\partial C}{\\partial w_{22}} & \\frac{\\partial C}{\\partial w_{23}} \\\\\n",
    "  \\frac{\\partial C}{\\partial w_{31}} & \\frac{\\partial C}{\\partial w_{32}} & \\frac{\\partial C}{\\partial w_{33}} \\\\\n",
    " \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial B^l} = \\begin{bmatrix}\n",
    "    \\frac{\\partial C}{\\partial b}\n",
    "  \\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7214f1c9",
   "metadata": {},
   "source": [
    "Por la regla de la cadena, el término $ \\frac{\\partial C}{\\partial w_{11}}$ se obtiene:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial w_{11}} = \\frac{\\partial C}{\\partial a_{11}} \\frac{\\partial a_{11}}{\\partial w_{11}} +\n",
    "   \\frac{\\partial C}{\\partial a_{12}} \\frac{\\partial a_{12}}{\\partial w_{11}} + \n",
    "   \\frac{\\partial C}{\\partial a_{21}} \\frac{\\partial a_{21}}{\\partial w_{11}} + \n",
    "   \\frac{\\partial C}{\\partial a_{22}} \\frac{\\partial a_{22}}{\\partial w_{11}} \n",
    "$$ \n",
    "\n",
    "Como $\\frac{\\partial C}{\\partial a_{ij}} = \\delta_{ij}$, la matriz $\\frac{\\partial C}{\\partial W^l}$, dónde el resultado anterior aparece en el término 1,1 es:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "  \\delta_{11} \\cdot x_{11} + \\delta_{12} \\cdot x_{12} + \\delta_{21} \\cdot x_{21} + \\delta_{22} \\cdot x_{22} &\n",
    "  \\delta_{11} \\cdot x_{12} + \\delta_{12} \\cdot x_{13} + \\delta_{21} \\cdot x_{22} + \\delta_{22} \\cdot x_{23} &\n",
    "  \\delta_{11} \\cdot x_{13} + \\delta_{12} \\cdot x_{14} + \\delta_{21} \\cdot x_{23} + \\delta_{22} \\cdot x_{24} \\\\\n",
    "  \\delta_{11} \\cdot x_{21} + \\delta_{12} \\cdot x_{22} + \\delta_{21} \\cdot x_{31} + \\delta_{22} \\cdot x_{32} &\n",
    "  \\delta_{11} \\cdot x_{22} + \\delta_{12} \\cdot x_{23} + \\delta_{21} \\cdot x_{32} + \\delta_{22} \\cdot x_{33} &\n",
    "  \\delta_{11} \\cdot x_{23} + \\delta_{12} \\cdot x_{24} + \\delta_{21} \\cdot x_{33} + \\delta_{22} \\cdot x_{34} \\\\\n",
    "  \\delta_{11} \\cdot x_{31} + \\delta_{12} \\cdot x_{32} + \\delta_{21} \\cdot x_{41} + \\delta_{22} \\cdot x_{42} &\n",
    "  \\delta_{11} \\cdot x_{32} + \\delta_{12} \\cdot x_{33} + \\delta_{21} \\cdot x_{42} + \\delta_{22} \\cdot x_{43} &\n",
    "  \\delta_{11} \\cdot x_{33} + \\delta_{12} \\cdot x_{34} + \\delta_{21} \\cdot x_{43} + \\delta_{22} \\cdot x_{44}\n",
    "  \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "El gradiente del coste con respecto a $B^l$, usando la regla de la cadena resulta:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial b} = \\frac{\\partial C}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial b} +\n",
    "   \\frac{\\partial C}{\\partial z_{12}} \\frac{\\partial z_{12}}{\\partial b} + \n",
    "   \\frac{\\partial C}{\\partial z_{21}} \\frac{\\partial z_{21}}{\\partial b} + \n",
    "   \\frac{\\partial C}{\\partial z_{22}} \\frac{\\partial z_{22}}{\\partial b} \n",
    "$$\n",
    "\n",
    "Y como $\\frac{\\partial a_{ij}}{\\partial b} = 1$ se tiene:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial B^l} = \\delta_{11} + \\delta_{12} + \\delta_{21} + \\delta_{22} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ae628",
   "metadata": {},
   "source": [
    "__Se puede observar__ que la matriz $\\frac{\\partial C}{\\partial W^l}$ se construye recorriendo uno a uno los elementos de $\\frac{\\partial C}{\\partial Z^l}$ a la vez que nos movemos usando las dimensiones del filtro $W^l$ sobre la matriz $A^{l-1}$ y los subproductos escalares entre $\\delta_{ij}$ y las submatrices (3x3) se van acumulando en $\\frac{\\partial C}{\\partial W^l}$. El primer subproducto se ve en la siguiente imagen:\n",
    "\n",
    "<img src=\"images/conv_gradW.png\" width=\"300px\" align=\"center\">\n",
    "\n",
    "Y el siguiente subproducto sería:\n",
    "\n",
    "<img src=\"images/conv_gradW2.png\" width=\"300px\" align=\"center\">\n",
    "\n",
    "El código programado aparece en la función __backward__ de la clase __conv2DLayer__ (ajustando la notación):\n",
    "\n",
    "                 dW[co, :] += Aprev[i, :, _h:_h+f, _b:_b+f] * Delta[i, co, _h,_b]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa324d73",
   "metadata": {},
   "source": [
    "La última operación del proceso de retropropagación de la capa $l$ consiste en obtener la matriz jacobiana \n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial Z^{l-1}} $$\n",
    "\n",
    "Que es de la misma dimensión que $A^{l-1}$, en este ejemplo (4x4), siendo el elemento $(i,j)$ el resultado de obtener \n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial x_{ij}} $$\n",
    "\n",
    "Por la regla de la cadena, el término $ \\frac{\\partial C}{\\partial x_{11}}$ se obtiene:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial x_{11}} = \\frac{\\partial C}{\\partial z_{11}} \\frac{\\partial z_{11}}{\\partial x_{11}} +\n",
    "   \\frac{\\partial C}{\\partial z_{12}} \\frac{\\partial z_{12}}{\\partial x_{11}} + \n",
    "   \\frac{\\partial C}{\\partial z_{21}} \\frac{\\partial z_{21}}{\\partial x_{11}} + \n",
    "   \\frac{\\partial C}{\\partial z_{22}} \\frac{\\partial z_{22}}{\\partial x_{11}} = \\delta_{11} \\cdot w_{11}\n",
    "$$ \n",
    "\n",
    "La matriz jacobiana completa\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial Z^{l-1}} = \\begin{bmatrix}\n",
    "  \\frac{\\partial C}{\\partial x_{11}} & \\frac{\\partial C}{\\partial x_{12}} & \\frac{\\partial C}{\\partial x_{13}}  & \\frac{\\partial C}{\\partial x_{14}}\\\\\n",
    "  \\frac{\\partial C}{\\partial x_{21}} & \\frac{\\partial C}{\\partial x_{22}} & \\frac{\\partial C}{\\partial x_{23}} & \\frac{\\partial C}{\\partial x_{24}} \\\\\n",
    "  \\frac{\\partial C}{\\partial x_{31}} & \\frac{\\partial C}{\\partial x_{32}} & \\frac{\\partial C}{\\partial x_{33}}  & \\frac{\\partial C}{\\partial x_{34}}\\\\\n",
    "   \\frac{\\partial C}{\\partial x_{41}} & \\frac{\\partial C}{\\partial x_{42}} & \\frac{\\partial C}{\\partial x_{43}}  & \\frac{\\partial C}{\\partial x_{44}}\\\\\n",
    "   \\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Entonces $\\frac{\\partial C}{\\partial Z^{l-1}}$ resulta:\n",
    "\n",
    "$$\\begin{bmatrix}\n",
    "w_{11} \\cdot \\delta_{11}  &\tw_{12} \\cdot \\delta_{11} + w_{11} \\cdot \\delta_{12} &\tw_{13} \\cdot \\delta_{11} + w_{12} \\cdot \\delta_{12} &\tw_{13} \\cdot \\delta_{12} \\\\\n",
    "w_{21} \\cdot \\delta_{11} + w_{11} \\cdot \\delta_{21} & w_{22} \\cdot \\delta_{11} + w_{21} \\cdot \\delta_{12} + w_{12} \\cdot \\delta_{21} + w_{11} \\cdot \\delta_{22} &  w_{23} \\cdot \\delta_{11} + w_{22} \\cdot \\delta_{12} + w_{13} \\cdot \\delta_{21} + w_{12} \\cdot \\delta_{22} & w_{23} \\cdot \\delta_{12} + w_{13} \\cdot \\delta_{22} \\\\\n",
    "w_{31} \\cdot \\delta_{11} + w_{21} \\cdot \\delta_{22} & w_{32} \\cdot \\delta_{11} + w_{31} \\cdot \\delta_{12} + w_{22} \\cdot \\delta_{21} + w_{21} \\cdot \\delta_{22} & w_{33} \\cdot \\delta_{11} + w_{32} \\cdot \\delta_{12} + w_{23} \\cdot \\delta_{21} + w_{22} \\cdot \\delta_{22} & w_{33} \\cdot \\delta_{12} + w_{23} \\cdot \\delta_{22} \\\\\n",
    "w_{31} \\cdot \\delta_{21} & w_{32} \\cdot \\delta_{21} + w_{31} \\cdot \\delta_{22} & w_{33} \\cdot \\delta_{21} + w_{32} \\cdot \\delta_{22} & w_{33} \\cdot \\delta_{22}\n",
    "\\end{bmatrix} $$\n",
    "\n",
    "La forma de obtener esta matriz es acumulando el producto escalar\n",
    "\n",
    "$$ \\delta_{ij} \\cdot W^l $$\n",
    "\n",
    "En la matriz $\\frac{\\partial C}{\\partial Z^{l-1}}$ cuya dimensión coindicen con $A^{l-1}$, en este ejemplo 4x4, acumulando el resultado matricial, en este caso 3x3, en la submatriz de acuerdo al movimiento del filtro. El primer subproducto escalar se ve en la siguiente imagen:\n",
    "\n",
    "<img src=\"images/conv_gradA.png\" width=\"800px\" align=\"center\">\n",
    "\n",
    "Y el siguiente subproducto en:\n",
    "\n",
    "<img src=\"images/conv_gradA2.png\" width=\"800px\" align=\"center\">\n",
    "\n",
    "El código programado aparece en la función __backward__ de la clase __conv2DLayer__ (ajustando la notación):\n",
    "\n",
    "                 DeltaPrev[i, :, _h:_h+f, _b:_b+f] += self.W[co, :] * Delta[i, co, _h,_b]\n",
    "\n",
    "El código anterior finaliza concatenado el __producto de Hadamard de la derivada primera de la función de activación__ en $Z^{l-1}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab38ae53",
   "metadata": {},
   "source": [
    "## Softmax y cross-entropy\n",
    "\n",
    "A la salida de la última capa se aplica la función softmax. Si el vector $z$ tiene la transformación lineal de la última capa (antes de aplicar la función de activación) la probabilidad de la neurona $j$ de salida es de acuerdo a la función softmax:\n",
    "\n",
    "$$p_j = \\frac{e^{z_j}}{\\sum_j{e^{z_j}}}$$\n",
    "\n",
    "Finalmente la pérdida se obtiene con la función entropia cruzada que es:\n",
    "\n",
    "$$- \\sum_j {y_j \\cdot ln(p_j)}$$\n",
    "\n",
    "Dónde $y_j$ es el valor de la etiqueta en formato __one-hot__ (vector con tantos dígitos como posibles etiquetas hay y que lleva todo ceros y un 1 en la posición que corresponde a la etiqueta).\n",
    "\n",
    "El uso de softmax y cross-entropy a la salida de la última capa ($L$) como se indica hace que el gradiente del coste con respecto a la transformación lineal de esa última capa:\n",
    "\n",
    "$$ \\frac{\\partial C}{\\partial z^l_i} =  p_i - y_i$$\n",
    "\n",
    "Para comprobarlo en un caso sencillo, se supone que una observación que tiene una transformación lineal:\n",
    "\n",
    "$$z=(z_1, z_2, z_3)$$\n",
    "\n",
    "Sabiendo que las etiquetas reales de esa observación en formato __one-hot__ viene dado por\n",
    "\n",
    "$$y=(y_1, y_2, y_3)$$\n",
    "\n",
    "La probabilidad usando la función __softmax__ es:\n",
    "\n",
    "$$p=(p_1, p_2, p_3)$$\n",
    "$$p_1 = \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}$$\n",
    "$$p_2 = \\frac{e^{z_2}}{e^{z_1} + e^{z_2} + e^{z_3}}$$\n",
    "$$p_3 = \\frac{e^{z_3}}{e^{z_1} + e^{z_2} + e^{z_3}}$$\n",
    "\n",
    "La función de coste por entropia cruzada es:\n",
    "\n",
    "$$ C = -y1 \\cdot ln(p_1) -y2 \\cdot ln(p_2) -y3 \\cdot ln(p_3) $$\n",
    "\n",
    "El gradiente del coste con respecto a $z$ será el vector\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial C}{\\partial z_1}, \\frac{\\partial C}{\\partial z_2}, \\frac{\\partial C}{\\partial z_3} \\end{pmatrix}$$\n",
    "\n",
    "Desarrollando la primera componente:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1} = -y1 \\frac{1}{p_1}\\frac{\\partial p_1}{\\partial z_1}  -y2 \\frac{1}{p_2}\\frac{\\partial p_2}{\\partial z_1}  -y3 \\frac{1}{p_3}\\frac{\\partial p_3}{\\partial z_1}$$\n",
    "\n",
    "Quedando:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1} = \\frac{-y_1e^{z_2}-y_1e^{z_3}+y_2e^{z_1}+y_3e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}}$$\n",
    "\n",
    "Sumando y restando el término $y_1e^{z_1}$ la fracción se puede poner \n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1} = \\frac{-y_1 (e^{z_1} + e^{z_2} + e^{z_3}) + e^{z_1}(y_1+y_2+y_3)}{e^{z_1} + e^{z_2} + e^{z_3}}$$\n",
    "\n",
    "Pero como el vector $y$ es un formato __one-hot__, se cumple que $y_1+y_2+y_3=1$ quedado la expresión:\n",
    "\n",
    "$$\\frac{\\partial C}{\\partial z_1} = -y_1 + \\frac{e^{z_1}}{e^{z_1} + e^{z_2} + e^{z_3}} = p_1 - y_1$$\n",
    "\n",
    "Haciendo lo mismo con las otras 2 derivadas parciales se ve que el vector gradiente se obtiene así:\n",
    "\n",
    "$$\\begin{pmatrix} \\frac{\\partial C}{\\partial z_1}, \\frac{\\partial C}{\\partial z_2}, \\frac{\\partial C}{\\partial z_3} \\end{pmatrix} = \\begin{pmatrix}p_1 - y_1, & p_2 - y_2, & p_3 - y_3 \\end{pmatrix} = p - y$$\n",
    "\n",
    "Este gradiente es el que aparece en el esquema anterior de LetNet5 identificado como __dA8__, una matriz de 32 filas y 10 columnas para cada una de las 10 posibles etiquetas. Esta matriz jacobiana es la que arranca el proceso de retro-propagación de los gradientes del coste."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b53de101",
   "metadata": {},
   "source": [
    "## Optimización ADAM\n",
    "\n",
    "Al actualizar los pesos y bias con sus gradientes de coste multiplicados por la tasa de entrenamiento ($\\eta$) en arquitecturas que mezclan capas de diferentes naturaleza surge el inconveniente de que las velocidades de crecimiento de los gradientes no son homogéneas y sería preciso utilizar valores de $\\eta$ diferentes en cada capa.\n",
    "\n",
    "Como sería muy difícil estimar estos valores en cada capa, hay algoritmos, por ejemplo Root Mean Square Propagation (__RMSprop__) que utilizan una media móvil que promedia más los valores actuales que los antiguos. A la vez se utiliza un momento de segundo orden para normalizar los elementos del gradiente (pues la raiz cuadrada de de la estimación del momento del segundo orden equivale a la desviación estándar).\n",
    "\n",
    "__ADAM__ (o estimación adaptativa del momento) añade a __RMSprop__ el cálculo de un momento y es el método más utilizado. Su fórmula es:\n",
    "\n",
    "$$m_{t+1}=\\beta \\cdot m_t + (1 - \\beta) \\bigtriangledown f_i(w_t)$$\n",
    "$$v_{t+1}=\\alpha \\cdot v_t + (1 - \\alpha) \\bigtriangledown f_i(w_t) ^2$$\n",
    "$$w_{t+1} = w_t - \\eta \\frac{m_t}{\\sqrt{v_t + 1} + \\epsilon}$$\n",
    "\n",
    "Donde $\\epsilon$ es un valor muy próximo a cero ($10^{-7}$ por ejemplo) para evitar divisiones por cero"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9632529",
   "metadata": {},
   "source": [
    "## Funciones generales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d4b9c757",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class netFuntion(object):\n",
    "    def __init__(self, seed=1):\n",
    "        \n",
    "        self.random = np.random.RandomState(seed)\n",
    "\n",
    "        return \n",
    "    \n",
    "    def sigmoid(self, x):\n",
    "        #return 1.0/(1.0 + np.exp(-x))  ## versión básica con problemas de desbordamiento en valores x<<<0\n",
    "        #return np.where(x < 0, np.exp(x)/(1.0 + np.exp(x)), 1.0/(1.0 + np.exp(-x)))\n",
    "        #return 1. / (1. + np.exp(-np.clip(x, -250, 250)))\n",
    "        from scipy.special import expit\n",
    "        try:\n",
    "            return expit(x)  ##Función sigmoidea de scipy; algo más lenta\n",
    "        except:\n",
    "            print('Error sigmoid en X=', x)\n",
    "            return x\n",
    "\n",
    "    def sigmoid_derivada(self, x):\n",
    "        return self.sigmoid(x)*(1.0-self.sigmoid(x))\n",
    "\n",
    "    def tanh(self, x):\n",
    "        return np.tanh(x)\n",
    "\n",
    "    def tanh_derivada(self, x):\n",
    "        return 1.0 - np.tanh(x)**2\n",
    "\n",
    "    def ReLU(self, x):\n",
    "        return np.maximum(0, x)\n",
    "\n",
    "    def ReLU_derivada(self, x):\n",
    "        return np.where(x <= 0, 0, 1)\n",
    "    \n",
    "    def softmax(self, raw_preds):\n",
    "        '''\n",
    "        pass raw predictions through softmax activation function\n",
    "        '''\n",
    "        try:\n",
    "            out = np.exp(raw_preds)\n",
    "            soft = out/np.sum(out) # divide the exponentiated vector by its sum. All values in the output sum to 1.\n",
    "        except:\n",
    "            print('ejecutada excepcion en softmax')\n",
    "            soft = self.stable_softmax(raw_preds)\n",
    " \n",
    "        return soft\n",
    "\n",
    "    def stable_softmax(self, x):\n",
    "        z = x - np.max(x, axis=-1, keepdims=True)\n",
    "        numerator = np.exp(z)\n",
    "        denominator = np.sum(numerator, axis=-1, keepdims=True)\n",
    "        softmax = numerator / denominator\n",
    "        return softmax\n",
    "\n",
    "    def categoricalCrossEntropy(self, probs, label):\n",
    "        '''\n",
    "        Calcula la pérdida de entropía cruzada categórica de las predicciones\n",
    "        Multiplica la etiqueta de salida deseada por el registro de la predicción, y se suman todos los valores en el vector        \n",
    "        Se evita calcular el logaritmo de cero sumando un epsilon = 1e-7\n",
    "        '''\n",
    "        \n",
    "        entr = 0\n",
    "        for v_pro, v_lab in zip(probs, label):\n",
    "            for _pro, _lab in zip(v_pro, v_lab):\n",
    "                entr -= _lab * np.log(_pro + 1e-7)\n",
    "        \n",
    "        return entr\n",
    "        #return  -np.sum(label * np.log(probs + 1e-7))\n",
    "\n",
    "    def listToString(self, list):\n",
    "        cadena = ''\n",
    "        for ele in list:\n",
    "            cadena = cadena + ele + '; '\n",
    "        return cadena\n",
    "    \n",
    "    def validarOpciones(self, valOpcion, tipOpcion):\n",
    "        if tipOpcion == 'ACTIVACION':\n",
    "            lstOpciones = ['RELU', 'SIGMOID', 'TANH', 'IDENTITY', 'SOFTMAX']\n",
    "            literal = 'Tipo de activación'\n",
    "        elif tipOpcion == 'OPTIMIZADOR':\n",
    "            lstOpciones = ['ADAM', 'GRAD']\n",
    "            literal = 'Optimizador'\n",
    "        else: ## 'PERDIDA'\n",
    "            lstOpciones = ['CROSSENTROPY', 'ERRORCUADRATICO']\n",
    "            literal = 'Función de pérdida'\n",
    "        \n",
    "        valOpcion = valOpcion.upper()\n",
    "        \n",
    "        assert valOpcion in lstOpciones, literal + ' <' + valOpcion + '> no contemplada. Usar: ' + self.listToString(lstOpciones)\n",
    "        \n",
    "        return valOpcion\n",
    "        \n",
    "    \n",
    "    def activacionPrima(self, tipActiva, A, B):\n",
    "        if (tipActiva == 'RELU'):\n",
    "            A[B<0] = 0\n",
    "            return A\n",
    "        elif (tipActiva == 'SIGMOID'):\n",
    "            return A * self.sigmoid_derivada(B)\n",
    "        elif (tipActiva == 'TANH'):\n",
    "            return A * self.tanh_derivada(B)\n",
    "        else:  ## No cambia si es Identity y Softmax (porque sólo se usa en la última capa)\n",
    "            return A\n",
    "\n",
    "    def activacion(self, tipActiva, X):\n",
    "        if (tipActiva == 'RELU'):\n",
    "            return self.ReLU(X)\n",
    "        elif (tipActiva == 'SIGMOID'):\n",
    "            return self.sigmoid(X)\n",
    "        elif (tipActiva == 'TANH'):\n",
    "            return self.tanh(X)\n",
    "        elif (tipActiva == 'SOFTMAX'):\n",
    "            return np.asarray([self.softmax(sp) for sp in X])\n",
    "        else:\n",
    "            return X\n",
    "\n",
    "    def onehot(self, y, n_classes):\n",
    "        \"\"\"Convierte las etiquetas en una representación de vectores de base canónica R^K siendo K el total de etiquetas\n",
    "        Parameters\n",
    "        ------------\n",
    "        y : array, shape = [n_samples]\n",
    "           Valores objetivo.\n",
    "        Returns\n",
    "        -----------\n",
    "        onehot : array, shape = (n_samples, n_labels)\n",
    "        \"\"\"\n",
    "        if n_classes == 2:\n",
    "            onehot = np.zeros((1, y.shape[0]))\n",
    "            for idx, val in enumerate(y.astype(int)):\n",
    "                ilabel = self.Clases_y.tolist().index(val)\n",
    "                onehot[0, idx] = int(ilabel)\n",
    "        else:\n",
    "            onehot = np.zeros((n_classes, y.shape[0]))\n",
    "            for idx, val in enumerate(y.astype(int)):\n",
    "                ilabel = self.Clases_y.tolist().index(val)\n",
    "                onehot[ilabel, idx] = 1.\n",
    "        return onehot.T\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9a1587",
   "metadata": {},
   "source": [
    "## Clase para una capa densa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a878ee33",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class linealLayer(netFuntion):\n",
    "    def __init__(self, neurInp, neurOut, tipActiva):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.neurInp = neurInp\n",
    "        self.neurOut = neurOut\n",
    "        self.tipActiva = self.validarOpciones(tipActiva, 'ACTIVACION')\n",
    "        self.W = self.random.standard_normal(size=(neurOut, neurInp)) * 0.01\n",
    "        self.B = np.zeros((neurOut))\n",
    "        self.pesos=True\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self,  X): \n",
    "        '''\n",
    "        Calcula la transformación lineal por la matriz de pesos W\n",
    "        Hay c_i canales de entrada. La dimensión de X ha de ser c_i mapas \n",
    "\n",
    "        Argumentos:\n",
    "        p -- Tamaño de Pool\n",
    "        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada\n",
    "\n",
    "        Retornos:\n",
    "        H -- Salida agrupada con dimensión (c_i, h/p, b/p)\n",
    "        '''\n",
    "        assert len(X.shape)==2 or len(X.shape)==4, \"La matriz de entrada X deben ser listas de 2 o 4 dimensiones\"\n",
    "\n",
    "        # Si la entrada es un mapa que ha devuelto una convolución se aplanan las dimensiones canal, alto, ancho en una\n",
    "        if len(X.shape)==4:\n",
    "            X = np.reshape(X,(X.shape[0], int(X.shape[1]*X.shape[2]*X.shape[3])))\n",
    "\n",
    "        #print('forwardLineal - X.shape=', X.shape)\n",
    "        # Se recogen las dimensiones de los mapas de entrada\n",
    "        (nf_x, nc_x) = X.shape\n",
    "        (nf_w, nc_w) = self.W.shape\n",
    "        (nc_b,) = self.B.shape\n",
    "\n",
    "        assert nc_x == nc_w, 'El numero de columnas de las matrices X y W deben ser iguales'\n",
    "        assert nc_b == nf_w, 'La dimensión del Bias y el nº de filas de W debe ser iguales nc_b='+ str(nc_b) + '_nf_w=' + str(nf_w)\n",
    "\n",
    "        Z = np.dot(X, np.transpose(self.W)) + np.transpose(self.B)\n",
    "        A = self.activacion(self.tipActiva, Z)\n",
    "        return Z, A\n",
    "    \n",
    "    def backward(self, dA, Aprev, l):\n",
    "        '''\n",
    "        El cálculo hacia atrás para una capa Densa\n",
    "\n",
    "         Argumentos:\n",
    "         dA: gradiente del costo con respecto a la salida de la capa densa, \n",
    "             matriz numpy de forma (n_H, n_W) asumiendo canales = 1\n",
    "         Apre -- Salida de la activación de la capa previa\n",
    "         W -- Matriz de pesos de la capa actual\n",
    "         l: identifica el nº de capa. Si l==0 es la primera capa oculta\n",
    "\n",
    "         Retornos:\n",
    "         dAprev: gradiente del costo con respecto a la entrada de la capa anterior. Gradiente que se propaga\n",
    "         dW: gradiente de la matriz de peso de la capa actual\n",
    "         dB: gradiente de la matriz de bias de la capa actual\n",
    "        '''\n",
    "\n",
    "        if len(Aprev.shape)==4:\n",
    "            Aprev = np.reshape(Aprev,(Aprev.shape[0], int(Aprev.shape[1]*Aprev.shape[2]*Aprev.shape[3])))\n",
    "\n",
    "        dW = np.dot(dA.T, Aprev)\n",
    "        dB = np.sum(dA, axis=0)\n",
    "\n",
    "        ## Propagación del delta dA sobre la capa previa. En la primera capa no tiene sentido la propagación\n",
    "        if l==0:\n",
    "            dAprev = None\n",
    "        else:\n",
    "            dAprev = np.dot(dA, self.W)\n",
    "\n",
    "        return dAprev, dW, dB"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75e34660",
   "metadata": {},
   "source": [
    "## Clase para una capa convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a627e490",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class conv2DLayer(netFuntion):\n",
    "    def __init__(self, canalInp, canalOut, fSize, tipActiva):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.canalInp = canalInp\n",
    "        self.canalOut = canalOut\n",
    "        self.fSize = fSize\n",
    "        self.tipActiva = self.validarOpciones(tipActiva, 'ACTIVACION')\n",
    "        stddev = 1.0/np.sqrt(np.prod((canalOut, canalInp, fSize, fSize)))\n",
    "        self.W = self.random.normal(loc = 0, scale = stddev, size = (canalOut, canalInp, fSize, fSize))\n",
    "        self.B = np.zeros((canalOut))\n",
    "        self.pesos=True\n",
    "        \n",
    "        return \n",
    "    \n",
    "    def forward(self, X): \n",
    "        '''\n",
    "        Calcula una convolución suponiendo que no hay padding (relleno a ceros) y strike 1 (sin salto)\n",
    "        El filtro es dimension f x f\n",
    "        Por lo tanto el mapa de salida tiene una dimensión que se reduce en f-1 con respecto al de entrada\n",
    "        Hay c_i canales de entrada y c_o canales de salida. La dimensión de X ha de ser c_i mapas \n",
    "        Y los pesos han de ser c_o lista de c_i matrices de dimension f\n",
    "\n",
    "        Argumentos:\n",
    "        W -- Pesos, numpy array con dimension (c_o, c_i, f, f), siendo un filtro individual (f, f) \n",
    "        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada\n",
    "\n",
    "        Retornos:\n",
    "        A -- Salida convolucionada, numpy array de tamaño (n, c_o, h, b), se asume salto 1 y siempre relleno a ceros\n",
    "        '''\n",
    "\n",
    "        #W = np.asarray(W)\n",
    "        assert len(X.shape)==4, \"La matriz de entrada X deben ser listas de 4 dimensiones. X.shape=\" + str(X.shape)\n",
    "        assert len(self.W.shape)==4, \"La matriz de pesos W deben ser listas de 4 dimensiones. W.shape=\" + str(W.shape)\n",
    "        # Se recogen las dimensiones de los mapas de entrada\n",
    "        (n, c_i, h, b) = X.shape\n",
    "        # Se recogen las dimensiones de los pesos \n",
    "        (c_o, _c_i, f, f) = self.W.shape\n",
    "        # Se recoge las dimensiones de los Bias\n",
    "        assert len(self.B.shape)==1 and self.B.shape[0]==c_o, \"Bias debe ser de dimensión y con nº igual a canal salida=\"  + str(self.B.shape)\n",
    "\n",
    "        assert _c_i == c_i, \"El nº de canales de entrada en X debe ser igual que al nº canales de entrada en W\"\n",
    "\n",
    "        # Inicializa la salida H con ceros\n",
    "        Z = np.zeros((n, c_o, h-(f-1), b-(f-1)))\n",
    "\n",
    "        # Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida\n",
    "        #for i in range(n):\n",
    "        #    for co in range(c_o):\n",
    "        #        for ci in range(c_i):\n",
    "        #            for _h in range(h-(f-1)):\n",
    "        #                for _b in range(b-(f-1)):\n",
    "        #                    x_slice = X[i, ci, _h:_h+f, _b:_b+f]\n",
    "        #                    Z[i, co, _h,_b] += np.sum(x_slice * self.W[co, ci]) + self.B[co]\n",
    "\n",
    "        # Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida\n",
    "        for co in range(c_o):\n",
    "            for ci in range(c_i):\n",
    "                for _h in range(h-(f-1)):\n",
    "                    for _b in range(b-(f-1)):\n",
    "                        x_slice = X[:, ci, _h:_h+f, _b:_b+f]\n",
    "                        Z[:, co, _h,_b] += np.tensordot(x_slice, self.W[co, ci]) + self.B[co]\n",
    "        \n",
    "        A = self.activacion(self.tipActiva, Z)\n",
    "        return Z, A\n",
    "\n",
    "    def backward(self, dA, Aprev, l):\n",
    "        '''\n",
    "        El cálculo hacia atrás para una función de convolución\n",
    "\n",
    "         Argumentos:\n",
    "         dA: gradiente del costo con respecto a la salida de la capa de conv (A), \n",
    "             matriz numpy de forma (n_H, n_W) asumiendo canales = 1\n",
    "         Aprev: Activación de la capa previa\n",
    "         W, B: matrices de peso y bias de la capa actual\n",
    "         shape_A : Tamaño de la activación en la capa actual\n",
    "\n",
    "         Retornos:\n",
    "         dAprev: gradiente del costo con respecto a la entrada de la capa anterior\n",
    "         dW: gradiente del costo con respecto a los pesos del filtro de la capa actual \n",
    "         dB: gradiente dle costo con respecto a los bias del filgro de la capa actual\n",
    "         l: identifica el nº de capa. Si l==0 es la primera capa oculta\n",
    "        '''\n",
    "        \n",
    "        #W = np.asarray(W)\n",
    "        assert len(dA.shape)==4, \"La matriz de entrada dA deben ser listas de 4 dimensiones. dA.shape=\" + str(dA.shape)\n",
    "        assert len(Aprev.shape)==4, \"La matriz de entrada Aprev deben ser listas de 4 dimensiones. Aprev.shape=\" + str(Aprev.shape)\n",
    "        assert len(self.W.shape)==4, \"La matriz de pesos W deben ser listas de 4 dimensiones=\"  + str(self.W.shape)\n",
    "        \n",
    "        # Se recogen las dimensiones de los mapas de entrada\n",
    "        (n, co, h, b) = dA.shape\n",
    "        (n2, c_i, h2, b2) = Aprev.shape\n",
    "        # Se recogen las dimensiones de los pesos \n",
    "        (c_o, _c_i, f, f) = self.W.shape\n",
    "        assert _c_i == c_i, \"El nº de canales de entrada en Aprev debe ser igual que al nº canales de entrada en W\"\n",
    "\n",
    "        assert n==n2, \"El nº de observaciones en Aprev y dA debe ser igual n=\" + str(n) + \" n2=\" + str(n2)\n",
    "        assert h2==h+(f-1), \"La altura del mapa previo y actual deben diferir en el filtro. h=\" + str(h) + \" h2=\" + str(h2)\n",
    "        assert b2==b+(f-1), \"La anchua del mapa previo y actual deben diferir en el filtro. b=\" + str(h) + \" b2=\" + str(h2)\n",
    "\n",
    "        # Initializing dAprev, dW with the correct shapes\n",
    "        dAprev = np.zeros_like(Aprev)\n",
    "        dW = np.zeros_like(self.W)\n",
    "        dB = np.zeros_like(self.B)\n",
    "\n",
    "        # Inicializamos dAprev con la dimensión de Aprev\n",
    "        dAprev = np.zeros_like(Aprev)\n",
    "\n",
    "        # Bucle sobre los ejes vertical(h) y la horizontal(b) de dA\n",
    "        for i in range(n):\n",
    "            for _h in range(h):\n",
    "                for _b in range(b):\n",
    "                    for co in range(c_o):\n",
    "                            ##Full Convolution 180º rotate Filter\n",
    "                            dAprev[i, :, _h:_h+f, _b:_b+f] += self.W[co, :] * dA[i, co, _h,_b]\n",
    "                            ##Convolution\n",
    "                            dW[co, :] += Aprev[i, :, _h:_h+f, _b:_b+f] * dA[i, co, _h,_b]\n",
    "                            dB[co] += dA[i, co, _h, _b]\n",
    "        \n",
    "        return dAprev, dW, dB\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff281a1c",
   "metadata": {},
   "source": [
    "## Clase para una capa de agrupación por máximo (max-pooling)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "e39cb368",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class maxPooling(netFuntion):\n",
    "    def __init__(self, pSize, tipActiva):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        self.pSize = pSize\n",
    "        self.tipActiva = self.validarOpciones(tipActiva, 'ACTIVACION')\n",
    "        self.pesos=False\n",
    "        \n",
    "        return \n",
    "\n",
    "    def forward(self, X): \n",
    "        '''\n",
    "        Calcula la agrupación por pool, suponiendo un tamaño de pool p\n",
    "        Hay c_i canales de entrada. La dimensión de X ha de ser c_i mapas \n",
    "\n",
    "        Argumentos:\n",
    "        p -- Tamaño de Pool\n",
    "        X -- Salida de la activación de la capa anterior, numpy array de dimension (n, c_i, h, b) asumiendo c_i canales de entrada\n",
    "\n",
    "        Retornos:\n",
    "        H -- Salida agrupada con dimensión (c_i, h/p, b/p)\n",
    "        '''\n",
    "        assert len(X.shape)==4, \"La matriz de entrada X deben ser listas de 4 dimensiones\"\n",
    "        # Se recogen las dimensiones de los mapas de entrada\n",
    "        (n, c_i, h, b) = X.shape\n",
    "\n",
    "        ## Se calcula la nueva dimensión una vez aplicado el pool\n",
    "        n_h = int(h/self.pSize)\n",
    "        n_b = int(b/self.pSize)\n",
    "        # Inicializa la salida H con ceros\n",
    "        H = np.zeros((n, c_i, n_h, n_b))\n",
    "\n",
    "        # Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida\n",
    "        for i in range(n):\n",
    "            for ci in range(c_i):\n",
    "                for _h in range(n_h):\n",
    "                    for _b in range(n_b):\n",
    "                        x_slice = X[i, ci, self.pSize*_h:self.pSize*(_h+1), self.pSize*_b:self.pSize*(_b+1)]\n",
    "                        H[i, ci, _h,_b] = np.max(x_slice)\n",
    "\n",
    "        return H, self.activacion(self.tipActiva, H)\n",
    "    \n",
    "    def backward(self, dA, Aprev, l):\n",
    "        '''\n",
    "        El cálculo hacia atrás para una función de convolución\n",
    "\n",
    "         Argumentos:\n",
    "         dA: gradiente del costo con respecto a la salida de la capa de pool (A), \n",
    "             matriz numpy de forma (n_H, n_W) asumiendo canales = 1\n",
    "         Aprev, A: Activación de la capa previa y de la capa actual. Sus dimensiones permiten calcular el pooling\n",
    "\n",
    "         Retornos:\n",
    "         dAprev: gradiente del costo con respecto a la entrada de la capa de conv (Aprev), \n",
    "             matriz numpy de forma (n_H_prev, n_W_prev) asumiendo canales = 1\n",
    "         l: identifica el nº de capa. Si l==0 es la primera capa oculta\n",
    "        '''\n",
    "        ##print(\"backwardPool - dA.shape=\" + str(dA.shape))\n",
    "        ##print(\"backwardPool - Aprev.shape=\" + str(Aprev.shape))\n",
    "        assert len(Aprev.shape)==4, \"La matriz de entrada Aprev deben ser listas de 4 dimensiones. Aprev.shape=\" + str(Aprev.shape)\n",
    "\n",
    "        assert len(dA.shape)==4, \"La matriz de entrada dA deben ser listas de 4 dimensiones. dA.shape=\" + str(dA.shape)\n",
    "\n",
    "        # Se recoge las dimensiones del gradiente del costo con respecto a la salida, dA\n",
    "        (n, c_o, h, b) = dA.shape\n",
    "\n",
    "        (n2, c_o2, h2, b2) = Aprev.shape\n",
    "        assert n==n2 and c_o==c_o2, \"El nº de observaciones y canales en dA y Aprev deben ser iguales. n=\" + str(n) + \"; n2=\" \n",
    "                                     #str(n2) + \";c_o=\" + str(c_o) + \";c_o2=\" + str(c_o2)\n",
    "        p = int(h2/h)\n",
    "        p2 = int(b2/b)\n",
    "        assert p==p2 and p>1 and p==self.pSize, \"Padding incorrecto p=\" + str(p) + \";p2=\" + str(p2) + \";self.pSize=\" + str(self.pSize)\n",
    "\n",
    "        # Initializing dX, dW with the correct shapes\n",
    "        dAprev = np.zeros_like(Aprev)\n",
    "\n",
    "        # Bucle sobre los ejes vertical(h) y la horizontal(b) del mapa de salida\n",
    "        for i in range(n):\n",
    "            for co in range(c_o):\n",
    "                for _h in range(h):\n",
    "                    for _b in range(b):\n",
    "                        mapa = Aprev[i, co, _h*self.pSize:(_h+1)*self.pSize, _b*self.pSize:(_b+1)*self.pSize]\n",
    "                        idx = np.nanargmax(mapa)\n",
    "                        (hm, bm) = np.unravel_index(idx, mapa.shape)\n",
    "                        dAprev[i, co, _h*self.pSize + hm, _b*self.pSize + bm] = dA[i, co, _h, _b]\n",
    "                        ## Forma alternativa de hacerlo\n",
    "                        ###dAprev[i, co, _h*p:(_h+1)*p, _b*p:(_b+1)*p] = (1/p**2)*dA[i, co, _h, _b]\n",
    "\n",
    "\n",
    "        return dAprev, None, None\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af92e0f4",
   "metadata": {},
   "source": [
    "## Clase de aplanado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d05694c0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class flatten(netFuntion):\n",
    "    def __init__(self, shapeInp, sizeOut):\n",
    "        \n",
    "        super().__init__()\n",
    "        \n",
    "        assert len(shapeInp) == 3, 'shapeInp debe ser de longitud 3. len(shapeInp)=' + str(len(shapeInp))\n",
    "        self.shapeInp = shapeInp\n",
    "        self.sizeOut = sizeOut\n",
    "        self.tipActiva = 'IDENTITY'\n",
    "        self.pesos=False\n",
    "        \n",
    "        return \n",
    "\n",
    "    def forward(self, X): \n",
    "        \n",
    "        H = np.reshape(X,(X.shape[0], self.sizeOut))\n",
    "        \n",
    "        return H, H\n",
    "    \n",
    "    def backward(self, dA, Aprev, l):\n",
    "        \n",
    "        dAprev = np.reshape(dA,(dA.shape[0], self.shapeInp[0], self.shapeInp[1], self.shapeInp[2]))\n",
    "        \n",
    "        return dAprev, None, None"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0512f08",
   "metadata": {},
   "source": [
    "## Clase red neuronal convolucional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5d533cf6",
   "metadata": {
    "tags": [
     "hide-cell"
    ]
   },
   "outputs": [],
   "source": [
    "class CNN(netFuntion):\n",
    "    \"\"\"MultiLayer LInear NEuron classifier.\n",
    "    Parametros\n",
    "    ------------\n",
    "    eta : float Ratio de aprendizaje (entre 0.0 y 1.0)\n",
    "    n_iter : int Pasos sobre el conjunto de datos de entrenamiento.\n",
    "    capas: capas[0] las neuronas de entrada, capas[1] las neuronas de salida\n",
    "    random_state : int Generador de semillas de números aleatorios para inicializar los pesos.\n",
    "    shuffle : boolean. Hace una mezcla o barajado aleatorio del conjunto de entrenamiento en los minibatch\n",
    "    minibatch_size: tamaño del minibatch o subconjuntos en que se divide el conjunto de entrada para el entrenamiento\n",
    "    hiddenLayers: Neuronas en cada capa oculta. Aquí se excluye la capa de entrada y la de salida que la obtiene\n",
    "                  automáticamente el proceso fit() de X e y\n",
    "    Atributos\n",
    "    -----------\n",
    "    w_ : Array de dimensión 1 con los pesos después del ajuste.\n",
    "    cost_ : lista de Suma-de-cuadrados de los valores de la función coste en cada Paso del algoritmo.\n",
    "    \"\"\"\n",
    "    def __init__(self, eta=0.01, epocas=50, shuffle=False, minibatch_size=0, seed=1, optimizador='ADAM', loss='CrossEntropy'):\n",
    "        \n",
    "        super().__init__(seed=seed)\n",
    "        \n",
    "        self.activarTrazaTensor=False ## Activa o inhibe las trazas de la función trazaTensor\n",
    "        self.eta = eta\n",
    "        self.epocas = epocas\n",
    "        self.shuffle = shuffle\n",
    "        self.minibatch_size = minibatch_size\n",
    "        self.optimizador = self.validarOpciones(optimizador, 'OPTIMIZADOR')\n",
    "        self.loss  = self.validarOpciones(loss, 'PERDIDA')\n",
    "        self.capas = []\n",
    "        self.L = 0\n",
    "        \n",
    "        if self.activarTrazaTensor == True:\n",
    "            self.epocas = 1 #Se fuerzan condiciones de mínima actividad\n",
    "            self.minibatch_size = min(1, self.minibatch_size) #Se fuerzan condiciones de mínima actividad\n",
    "            f = open('trazaCNN.txt', 'w')\n",
    "            f.write('Inicio de la traza\\n')\n",
    "            f.close()\n",
    "\n",
    "    def addLayer(self, layer):\n",
    "        assert type(layer) in [linealLayer, conv2DLayer, maxPooling, flatten], \"Objeto layer no permitido\"\n",
    "        \n",
    "        self.capas.append(layer)\n",
    "        self.L += 1\n",
    "        \n",
    "        return\n",
    "    \n",
    "    def validaLayers(self):\n",
    "        assert self.L>0, \"Debe tener alguna capa definida en la red neuronal\"\n",
    "        if self.capas[-1].tipActiva == 'SOFTMAX':\n",
    "            assert self.optimizador == 'ADAM' and self.loss == 'CROSSENTROPY', \"Optimizidor Adam debe ir con perdida CrossEntropy y en la última capa softmax\"\n",
    "        else:\n",
    "            assert self.optimizador == 'GRAD' and self.loss == 'ERRORCUADRATICO', \"Optimizidor GRAD (gradiente descenso) debe ir con perdida errorCuadratico y en la última capa distinto de softmax\"\n",
    "        \n",
    "        return\n",
    "\n",
    "\n",
    "    def fit(self, X, y_inicial):\n",
    "        \"\"\" Ajuste con los datos de entrenamiento.\n",
    "        Parametros\n",
    "        ----------\n",
    "        X : {Tipo array}, shape = [n_ejemplo, n_caracteristicas]\n",
    "        Vectores de entrenamiento, donde n_ejemplo es el numero de ejemplos y \n",
    "        n_caracteristicas es el número de características.\n",
    "        y : tipo array, shape = [n_ejemplo] Valores Objetivo.\n",
    "        Retorno\n",
    "        -------\n",
    "        self : objecto\n",
    "        \"\"\"\n",
    "        self.minibatch_size = len(X) if (self.minibatch_size == 0) else self.minibatch_size\n",
    "        self.Clases_y = np.unique(y_inicial)\n",
    "        y = self.onehot(y_inicial, len(self.Clases_y))\n",
    "        self.coste = []\n",
    "        self.validaLayers()\n",
    "        if self.optimizador == 'ADAM': self.initAdam()\n",
    "                \n",
    "        for t in range(self.epocas):\n",
    "            # iterate over minibatches\n",
    "            cost=0\n",
    "            indices = np.arange(X.shape[0])\n",
    "            if self.shuffle:\n",
    "                self.random.shuffle(indices)\n",
    "            lstIndices = tqdm([i for i in range(0, indices.shape[0] - self.minibatch_size + 1, self.minibatch_size)])\n",
    "            for start_idx in lstIndices:\n",
    "                batch_idx = indices[start_idx:start_idx + self.minibatch_size]\n",
    "                self.Z = []  ## lista donde guarda las salidas del sumatorio en cada capa\n",
    "                self.A = [X[batch_idx]] ## lista para guardar la activación de cada capa. En la capa 1 es la X, en resto f(W(X))\n",
    "                ### Primero se hace el avance hacia adelante\n",
    "                self.trazaTensor(\"IMAGEN INICIAL\", self.A[-1])\n",
    "                for l in range(self.L):\n",
    "                    sumatorioPesos, activacion = self.capas[l].forward(self.A[l])\n",
    "                    self.Z.append(sumatorioPesos)  ## Se guarda la lista de sumatorios en A\n",
    "                    self.A.append(activacion)  ## La activación de esta capa es la entrada de la próxima\n",
    "                    self.trazaTensor(\"ACTIVACION[\" + str(l) + \"]\", self.A[-1])\n",
    "                ## Se obtiene la matriz Delta en la capa L con la tasa de cambio del error\n",
    "                \n",
    "                #probs = np.asarray([self.softmax(sp) for sp in sumatorioPesos]) # predict class probabilities with the softmax activation function\n",
    "                probs = activacion\n",
    "                errors = probs - y[batch_idx]   ## Los errores y - la activacion de la ultima capa\n",
    "                if self.loss == 'CROSSENTROPY':\n",
    "                    loss = self.categoricalCrossEntropy(probs, y[batch_idx]) # categorical cross-entropy loss\n",
    "                    delta= errors  ## Delta de la función CrossEntropy + SoftMax\n",
    "                else: ## 'errorCuadratico'\n",
    "                    loss = (errors**2).sum() / (2.0 * X[batch_idx].shape[0])\n",
    "                    delta = self.activacionPrima(self.capas[-1].tipActiva, errors, self.Z[-1])\n",
    "                \n",
    "                self.trazaTensor(\"PROBS\", probs)\n",
    "                self.trazaTensor(\"LOSS\", loss)\n",
    "                \n",
    "                self.trazaTensor(\"DELTA ULTIMA\", delta)\n",
    "                self.deltas = [delta] ## delta de la capa última, la capa L\n",
    "\n",
    "                ### Retropropagación. Calcula desde la capa L-1 hacia atrás la matriz Delta \n",
    "                ## propagando la tasa de cambio del error obtenido en la capa L\n",
    "                for l in reversed(range(self.L)):\n",
    "                    self.backwardProceso(l, t+1)\n",
    "                    \n",
    "                cost = loss / X[batch_idx].shape[0]\n",
    "                lstIndices.set_description(\"Época: %d Coste: %.4f\" % (t, cost))\n",
    "                self.coste.append(cost)\n",
    "                if self.activarTrazaTensor == True:\n",
    "                    return ## Se fuerzan condiciones de mínima actividad\n",
    "                \n",
    "        return\n",
    "\n",
    "    def backwardProceso(self, l, t):\n",
    "        dA, dW, dB = self.capas[l].backward(dA=self.deltas[-1], Aprev=self.A[l], l=l)\n",
    "        self.trazaTensor(\"dA[\" + str(l) + \"]\", dA, nDecimal = 4)\n",
    "        self.trazaTensor(\"dW[\" + str(l) + \"]\", dW, nDecimal = 4)\n",
    "        self.trazaTensor(\"dB[\" + str(l) + \"]\", dB, nDecimal = 4)\n",
    "        if l>0:  ## Se añade la activación derivada de la capa anterior para completar la retro-progración de dA\n",
    "            dA = self.capas[l-1].activacionPrima(self.capas[l-1].tipActiva, dA, self.Z[l-1]) ## self.Z[l-1] es Zprev\n",
    "        self.deltas.append(dA)\n",
    "\n",
    "        if self.capas[l].pesos==True:  ## La capa tiene pesos (W y B) que actualizar\n",
    "            if self.optimizador == 'ADAM': ## Optimización por el algorimo de Adam\n",
    "                beta1=0.95\n",
    "                beta2=0.99\n",
    "                epsilon=1e-7\n",
    "                self.capas[l].W_moment = beta1*self.capas[l].W_moment + (1-beta1)*dW/self.minibatch_size\n",
    "                self.capas[l].W_RMS = beta2*self.capas[l].W_RMS + (1-beta2)*(dW/self.minibatch_size)**2\n",
    "                self.capas[l].B_moment = beta1*self.capas[l].B_moment + (1-beta1)*dB/self.minibatch_size\n",
    "                self.capas[l].B_RMS = beta2*self.capas[l].B_RMS + (1-beta2)*(dB/self.minibatch_size)**2\n",
    "                self.capas[l].W -= self.eta * self.capas[l].W_moment/(np.sqrt(self.capas[l].W_RMS)+epsilon) \n",
    "                self.capas[l].B -= self.eta * self.capas[l].B_moment/(np.sqrt(self.capas[l].B_RMS)+epsilon)\n",
    "            else:  ## Gradiente Descenso estándar\n",
    "                self.capas[l].W -= self.eta * dW\n",
    "                self.capas[l].B -= self.eta * dB\n",
    "            self.trazaTensor(\"UPDATED W[\" + str(l) + \"]\", self.capas[l].W, nDecimal = 4)\n",
    "            self.trazaTensor(\"UPDATED B[\" + str(l) + \"]\", self.capas[l].B, nDecimal = 4)\n",
    "\n",
    "    def initAdam(self):\n",
    "        for l in range(self.L):\n",
    "            if self.capas[l].pesos==True:  ## La capa tiene pesos (W y B) que actualizar\n",
    "                self.capas[l].W_moment = np.zeros_like(self.capas[l].W) \n",
    "                self.capas[l].W_RMS = np.zeros_like(self.capas[l].W) \n",
    "                self.capas[l].B_moment = np.zeros_like(self.capas[l].B) \n",
    "                self.capas[l].B_RMS = np.zeros_like(self.capas[l].B) \n",
    "        \n",
    "        return\n",
    "    \n",
    "    def predict(self, X):\n",
    "        \"\"\"Return class label after unit step\"\"\"\n",
    "        # Se aplica un forward a través de todas las capas\n",
    "        A_=X\n",
    "        for l in range(self.L):\n",
    "            Z, A_ = self.capas[l].forward(A_)\n",
    "\n",
    "        i_labels = np.argmax(A_, axis=1)\n",
    "        y_pred = [self.Clases_y[i] for i in i_labels] \n",
    "        \n",
    "        return np.array(y_pred)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"Return la probabilidad de cada clase\"\"\"\n",
    "        # Se aplica un forward a través de todas las capas\n",
    "        A_=X\n",
    "        for l in range(self.L):\n",
    "            Z, A_ = self.capas[l].forward(A_)\n",
    "        \n",
    "        probs = np.asarray([self.softmax(sp) for sp in Z])\n",
    "        \n",
    "        return probs\n",
    "    def accuracy(self, ypred, yok):\n",
    "        \n",
    "        assert len(ypred)==len(yok) and len(ypred.shape)==1 and len(yok.shape)==1, \"ypre, yok no son array 1D de igual longitud. ypred=\" + str(ypred.shape) + \"yok=\" + str(yok.shape) \n",
    "\n",
    "        return np.sum(ypred==yok)/len(ypred)   \n",
    "    \n",
    "    def writeModel(self, fileName):\n",
    "        W = []\n",
    "        B = []\n",
    "        for l in range(self.L):\n",
    "            if self.capas[l].pesos==True:  ## La capa tiene pesos (W y B) que actualizar\n",
    "                W.append(self.capas[l].W)\n",
    "                B.append(self.capas[l].B)\n",
    "        \n",
    "        with open(fileName, 'wb') as file:\n",
    "            pickle.dump([W,B], file)\n",
    "        \n",
    "        return\n",
    "\n",
    "    def readModel(self, fileName):\n",
    "        with open(fileName, 'rb') as file:\n",
    "            [W,B] = pickle.load(file)\n",
    "\n",
    "        i=0\n",
    "        for l in range(self.L):\n",
    "            if self.capas[l].pesos==True:  ## La capa tiene pesos (W y B) que actualizar\n",
    "                self.capas[l].W = W[i]\n",
    "                self.capas[l].B = B[i]\n",
    "                i+=1\n",
    "        \n",
    "        return\n",
    "\n",
    "    def trazaTensor(self, literal, T, nDecimal = 2):\n",
    "        if self.activarTrazaTensor == False:\n",
    "            return\n",
    "        \n",
    "        f = open('trazaCNN.txt', 'a')\n",
    "        \n",
    "        f.write(literal + '\\n')\n",
    "        \n",
    "        if not type(T)  in [np.ndarray, np.float64, np.float32]:\n",
    "            f.write(str(T))\n",
    "            f.close()\n",
    "            return\n",
    "        \n",
    "        n = len(T.shape)\n",
    "        if (n==4):\n",
    "            for i in range(T.shape[0]):\n",
    "                for j in range(T.shape[1]):\n",
    "                    for k in range(T.shape[2]):\n",
    "                        f.write(\"[\")\n",
    "                        for m in range(T.shape[3]):\n",
    "                            f.write(str(round(T[i][j][k][m], nDecimal)) + \" \")\n",
    "                        f.write(\"] \\n\")\n",
    "                    f.write(\"-----------------------------------------\\n\")\n",
    "        elif (n==3):\n",
    "            for i in range(T.shape[0]):\n",
    "                for j in range(T.shape[1]):\n",
    "                    f.write(\"[\")\n",
    "                    for k in range(T.shape[2]):\n",
    "                        f.write(str(round(T[i][j][k], nDecimal)) + \" \")\n",
    "                    f.write(\"] \\n\")\n",
    "                f.write(\"-----------------------------------------\\n\")\n",
    "        elif (n==2):\n",
    "            for i in range(T.shape[0]):\n",
    "                f.write(\"[\")\n",
    "                for j in range(T.shape[1]):\n",
    "                    f.write(str(round(T[i][j], nDecimal)) + \" \")\n",
    "                f.write(\"] \\n\")\n",
    "        elif (n==1):\n",
    "            f.write(\"[\")\n",
    "            for i in range(T.shape[0]):\n",
    "                f.write(str(round(T[i], nDecimal)) + \" \")\n",
    "            f.write(\"] \\n\")\n",
    "        else:\n",
    "            f.write(str(T))\n",
    "        \n",
    "        f.write(\"==========================================\\n\")\n",
    "        f.close()\n",
    "        return\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31d43198",
   "metadata": {},
   "source": [
    "## Prueba Integral con la arquitectura LeNet-5 y el conjunto MNIST\n",
    "\n",
    "Se implementa la arquitectura LeNet-5 (LeCun et al.; 1998)\n",
    "\n",
    "<img src=\"images/LeNet5.png\" width=\"750px\" align=\"center\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9859ce27",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000, 32, 32)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision import datasets, transforms\n",
    "import torchvision.transforms.functional as fn\n",
    "\n",
    "# Download and load the test data\n",
    "w_dataset = datasets.MNIST('data', download=True, train=False, transform=transforms.ToTensor())\n",
    "dataResized = fn.resize(w_dataset.data, 32,antialias=True)\n",
    "dataCifar = dataResized.data.numpy()\n",
    "dataCifar.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d01f089b",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X.shape= (2000, 1, 32, 32)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Se añade una dimensión para el canal\n",
    "nTotal = 2000\n",
    "X = np.empty((nTotal, dataCifar.shape[1], dataCifar.shape[2]), dtype=np.float32)\n",
    "#X = np.empty(dataCifar.shape)\n",
    "\n",
    "\n",
    "## Se crea la matriz de labels\n",
    "y = []\n",
    "#for i in range(w_dataset.data.size()[0]):\n",
    "for i in range(nTotal):\n",
    "    _x, t = w_dataset.__getitem__(i)\n",
    "    y.append(t)\n",
    "    X[i] = dataCifar[i]\n",
    "y = np.asarray(y)\n",
    "X = np.asarray([ [_x] for _x in X ], dtype=np.float32)\n",
    "print('X.shape=', X.shape)\n",
    "np.unique(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4357af2d",
   "metadata": {},
   "source": [
    "### Se define el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "919a3339",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = CNN(minibatch_size=32, epocas=1, eta=0.01, optimizador='Adam', loss='crossEntropy')\n",
    "model.addLayer(conv2DLayer(canalInp=1, canalOut=6, fSize=5, tipActiva='ReLU'))\n",
    "model.addLayer(maxPooling(pSize=2, tipActiva='ReLU'))\n",
    "model.addLayer(conv2DLayer(canalInp=6, canalOut=16, fSize=5, tipActiva='ReLU'))\n",
    "model.addLayer(maxPooling(pSize=2, tipActiva='ReLU'))\n",
    "model.addLayer(flatten(shapeInp=(16,5,5), sizeOut=400))\n",
    "model.addLayer(linealLayer(neurInp=400, neurOut=120, tipActiva='ReLU'))\n",
    "model.addLayer(linealLayer(neurInp=120, neurOut=84, tipActiva='ReLU'))\n",
    "model.addLayer(linealLayer(neurInp=84, neurOut=10, tipActiva='softmax'))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23c3b828",
   "metadata": {},
   "source": [
    "### Se realiza el entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5fdf0bb0",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modo de entrenamiento no activo\n"
     ]
    }
   ],
   "source": [
    "if entrenamiento:\n",
    "    model.fit(X, y)\n",
    "else:\n",
    "    print('modo de entrenamiento no activo')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90186b93",
   "metadata": {},
   "source": [
    "### Serialización del modelo (input/output según estado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "33e034ba",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modelo recuperado del fichero\n"
     ]
    }
   ],
   "source": [
    "if entrenamiento:\n",
    "    model.writeModel('./data/modeloLetNet5.pt')\n",
    "    print('modelo guardado en fichero')\n",
    "else:\n",
    "    model.readModel('./data/modeloLetNet5.pt')\n",
    "    print('modelo recuperado del fichero')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d3e5f7",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "source": [
    "### Cálculo del ratio de precisión en el conjunto de entrenamiento"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ba378125",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-output"
    ]
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [00:54<00:00,  1.85it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "('Accuracy en entrenamiento=', 85.95)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "minibatch_size = 20\n",
    "indices = np.arange(X.shape[0])\n",
    "first=True\n",
    "lstIndices = tqdm([i for i in range(0, indices.shape[0] - minibatch_size + 1, minibatch_size)])\n",
    "model.Clases_y = np.unique(y)\n",
    "for start_idx in lstIndices:\n",
    "    batch_idx = indices[start_idx:start_idx + minibatch_size]\n",
    "    y_pred2 = model.predict(X[batch_idx])\n",
    "    y_pred = y_pred2 if first else np.concatenate((y_pred,y_pred2),axis=0)\n",
    "    first=False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "852f6373-5106-493a-924f-b6f4f15d9f8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"Accuracy en entrenamiento=\", 100 * model.accuracy(y_pred,y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "000dd646",
   "metadata": {},
   "source": [
    "## Ejecución en modo Perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b5001076",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((2000, 1, 32, 32), (2000, 1024))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X2 = np.reshape(X,(X.shape[0], 32*32))\n",
    "X.shape, X2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "41387034",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_cd = CNN(minibatch_size=32, epocas=30, eta=0.01, optimizador='grad', loss='errorCuadratico')\n",
    "model_cd.addLayer(linealLayer(neurInp=1024, neurOut=256, tipActiva='sigmoid'))\n",
    "model_cd.addLayer(linealLayer(neurInp=256, neurOut=64, tipActiva='sigmoid'))\n",
    "model_cd.addLayer(linealLayer(neurInp=64, neurOut=10, tipActiva='sigmoid'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "f13d5e65",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "modo de entrenamiento no activo\n"
     ]
    }
   ],
   "source": [
    "if entrenamiento:\n",
    "    model_cd.fit(X, y)\n",
    "else:\n",
    "    print('modo de entrenamiento no activo')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4cab6b7-3e3e-488e-b360-0f0d9c13b01d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "internal-cathedral",
   "metadata": {},
   "source": [
    "# Análisis de HMM\n",
    "\n",
    "Tenemos tres fases:\n",
    "\n",
    "1. [__Aprendizaje__](content:markov:hmm:aprendizaje): dada una lista de secuencias de sı́mbolos, ¿podemos __aprender__ los parámetros $(\\pi^{(0)} , P, E )$ de un HMM? __Algoritmo de Baum-Welch__.\n",
    "\n",
    "\n",
    "2. [__Evaluación__](content:markov:hmm:evaluacion): una vez que el modelo está ajustado, ¿cuál es la verosimilitud de una secuencia dada? __Algoritmo “forward”__.\n",
    "\n",
    "\n",
    "3. [__Descodificación__](content:markov:hmm:descodificacion): una vez que el modelo está ajustado, ¿cuál es el la secuencia de estados __más probable__ para generar de una secuencia de sı́mbolos dada? __Algoritmo de Viterbi__."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "noticed-trader",
   "metadata": {},
   "source": [
    "(content:markov:hmm:aprendizaje)=\n",
    "## 4.5.1 Aprendizaje\n",
    "\n",
    "Dos casos:\n",
    "1. Si conocemos las secuencias de estados asociadas a cada secuencia de sı́mbolos\n",
    "2. Si no los conocemos\n",
    "\n",
    "### Caso 1: conocemos las secuencias de estados asociadas a los símbolos\n",
    "\n",
    "En proteı́nas, esto equivale a conocer la estructura secundaria de cada secuencia de aminoácidos.\n",
    "\n",
    "+ La verosimilitud de que una secuencia de estados $\\mathcal{est}=\\{x_1,\\ldots,x_n\\}$ genere una secuencia de símbolos $\\mathcal{sim}=\\{s_1,\\ldots,s_n\\}$ es:\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{\\mathcal{est},\\mathcal{sim}\\}=\n",
    "\\mathbb{P}\\{X_1=x_1\\}\\mathbb{P}\\{S_1=s_1|X_1=x_1\\}\\times\n",
    "\\prod_{t=2}^{n}\\mathbb{P}\\{X_t=x_t|X_{t-1}=x_{t-1}\\}\\mathbb{P}\\{S_t=s_t|X_t=x_t\\}\n",
    "$$\n",
    "\n",
    "+ Entonces el método de __máxima verosimilitud__ permite estimar los parámetros como frecuencias relativas en el conjunto de entrenamiento.\n",
    "\n",
    "```{note}\n",
    "En el __conjunto de entrenamiento__\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "+ Probabilidades de transición:\n",
    "\n",
    "$$\n",
    "p_{ij}=\\frac{\\text{número de transiciones desde $X=x_i$ hasta $X=x_j$}}{\\text{número de transiciones que parten de $X=x_i$}}\n",
    "$$\n",
    "\n",
    "+ Probabilidades de emisión:\n",
    "\n",
    "$$\n",
    "e_{ij}=\n",
    "\\frac{\\text{número de veces que el estado $X=x_i$ emite el símbolo $S=s_j$}}{\\text{número de veces que el estado es $X=x_i$}}\n",
    "$$\n",
    "\n",
    "\n",
    "+ Vector inicial:\n",
    "\n",
    "$$\n",
    "\\pi^{(0)}_{i}=\n",
    "\\frac{\\text{número de veces que el estado inicial es $X=x_i$}}\n",
    "{\\text{número total de instancias}}\n",
    "$$\n",
    "\n",
    "\n",
    "### Caso 2: no conocemos las secuencias de estados asociadas a los símbolos\n",
    "\n",
    "En proteı́nas, esto equivale a conocer solamente las secuencias de aminoácidos.\n",
    "\n",
    "+ Este es el caso más frecuente en la práctica.\n",
    "\n",
    "\n",
    "+ En este caso se puede aplicar el __algoritmo de Baum-Welch__, que estima también las probabilidades de transición aunque no conozcamos las secuencias de estados.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "polyphonic-secret",
   "metadata": {},
   "source": [
    "(content:markov:hmm:evaluacion)=\n",
    "## 4.5.2 Evaluación\n",
    "\n",
    "\n",
    "```{warning}\n",
    "¡Tras la fase de aprendizaje!\n",
    "```\n",
    "\n",
    "\n",
    "Dada una secuencia de __sı́mbolos__ (aminoácidos), ¿puedo calcular la probabilidad de observarla de acuerdo con el HMM?\n",
    "\n",
    "+ Equivale a calcular la probabilidad marginal\n",
    "\n",
    "$$\n",
    "\\mathbb{P}\\{\\mathcal{sim}\\}=\n",
    "\\sum_{\\{x_1,\\ldots,x_n\\}}\n",
    "\\mathbb{P}\\{X_1=x_1\\}\\mathbb{P}\\{S_1=s_1|X_1=x_1\\}\\prod_{t=2}^{n}\\mathbb{P}\\{X_t=x_t|X_{t-1}=x_{t-1}\\}\\mathbb{P}\\{S_t=s_t|X_t=x_t\\}\n",
    "$$\n",
    "\n",
    "donde tenemos que sumar sobre todos los posibles caminos que generan la secuencia.\n",
    "\n",
    "\n",
    "+ El número de posibilidades crece __exponencialmente__ con $n$.\n",
    "\n",
    "\n",
    "+ Se usa __programación dinámica__ para calcular la suma anterior de manera __eficiente__. El resultado es el __algoritmo “forward”__.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "infrared-conflict",
   "metadata": {},
   "source": [
    "(content:markov:hmm:descodificacion)=\n",
    "## 4.5.3 Descodificación\n",
    "\n",
    "```{warning}\n",
    "¡Tras la fase de aprendizaje!\n",
    "```\n",
    "\n",
    "Dada una secuencia de sı́mbolos (aminoácidos), ¿puedo calcular la secuencia de estados ocultos (estructuras secundarias) que maximiza la verosimilitud $\\mathbb{P}\\{\\mathcal{est}, \\mathcal{sim} \\}$?\n",
    "\n",
    "+ Es un proceso de máxima verosimilitud.\n",
    "\n",
    "\n",
    "+  Se usa __programación dinámica__ eligiendo la rama de __mayor probabilidad__ en cada paso de la secuencia.\n",
    "\n",
    "\n",
    "+ El resultado es el __algoritmo de Viterbi__.\n",
    "\n",
    "### Algoritmo de Viterbi\n",
    "\n",
    "Para el ejemplo anterior:\n",
    "\n",
    "$$\n",
    "\\mathbf{v}_0=\\left(\\stackrel{H}{0.6},\\quad \n",
    "\\stackrel{C}{0.4}\n",
    "\\right),\\quad\n",
    "P=\n",
    "\\stackrel{H\\quad C}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.4 \\\\\n",
    "    0.3 & 0.7 \n",
    "    \\end{bmatrix}%\n",
    "  },\\quad\n",
    "E=\n",
    "\\begin{matrix}\n",
    "H \\\\C\n",
    "\\end{matrix}\n",
    "\\stackrel{G\\quad W\\quad M}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.3 & 0.1 \\\\\n",
    "    0.2 & 0.4 & 0.4 \n",
    "    \\end{bmatrix}%\n",
    "  }\n",
    "$$\n",
    "\n",
    "Tenemos una cadena con tres símbolos observados: $G-W-M$ (en ese orden). Queremos conocer la cadena de estados subyacente __más probable__.\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.1\n",
    "<img src=\"./images/hmm1.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "registered-center",
   "metadata": {},
   "source": [
    "$$\n",
    "\\pi^{(0)}=\\left(\\stackrel{\\color{red}{H}}{\\color{red}{0.6}},\\quad \n",
    "\\stackrel{\\color{blue}{C}}{\\color{blue}{0.4}}\n",
    "\\right),\\quad\n",
    "P=\n",
    "\\stackrel{H\\quad C}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.4 \\\\\n",
    "    0.3 & 0.7 \n",
    "    \\end{bmatrix}%\n",
    "  },\\quad\n",
    "E=\n",
    "\\begin{matrix}\n",
    "H \\\\C\n",
    "\\end{matrix}\n",
    "\\stackrel{\\color{orange}{G}\\quad W\\quad M}{%\n",
    "    \\begin{bmatrix}\n",
    "    \\color{red}{0.6} & 0.3 & 0.1 \\\\\n",
    "    \\color{blue}{0.2} & 0.4 & 0.4 \n",
    "    \\end{bmatrix}%\n",
    "  }\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(H)=\\pi^{(0)}(H)\\cdot E(H\\rightarrow G)= 0.6 \\times 0.6 = 0.36\n",
    "$$\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(C)=\\pi^{(0)}(C)\\cdot E(C\\rightarrow G)= 0.4 \\times 0.2 = 0.08\n",
    "$$\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.2\n",
    "<img src=\"./images/hmm2.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "rental-cooler",
   "metadata": {},
   "source": [
    "Siguiente paso: calculamos las probabilidades de las 4 cadenas posibles. \n",
    "\n",
    "```{admonition} __Importante__\n",
    ":class: note\n",
    "Partimos de \n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1=\\left(\n",
    "\\stackrel{\\color{red}{H}}{\\color{red}{0.36}}\n",
    ",\\quad \n",
    "\\stackrel{\\color{blue}{C}}{\\color{blue}{0.08}}\n",
    "\\right)\n",
    ",\\quad\n",
    "P=\n",
    "\\stackrel{H\\quad C}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.4 \\\\\n",
    "    0.3 & 0.7 \n",
    "    \\end{bmatrix}%\n",
    "  },\\quad\n",
    "E=\n",
    "\\begin{matrix}\n",
    "H \\\\C\n",
    "\\end{matrix}\n",
    "\\stackrel{G\\quad \\color{orange}{W}\\quad M}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.3 & 0.1 \\\\\n",
    "    0.2 & 0.4 & 0.4 \n",
    "    \\end{bmatrix}%\n",
    "  }\n",
    "$$\n",
    "```\n",
    "\n",
    "\n",
    "+ Probabilidad de la cadena $H-H$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(H)\\cdot P(H\\rightarrow H)\\cdot E(H\\rightarrow W) = 0.36 \\times 0.6 \\times 0.3 = 0.0648\n",
    "$$\n",
    "\n",
    "\n",
    "+ Probabilidad de la cadena $C-H$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(C)\\cdot P(C\\rightarrow H)\\cdot E(H\\rightarrow W) = 0.08 \\times 0.3 \\times 0.3 = 0.0072\n",
    "$$\n",
    "\n",
    "```{admonition} __Importante__\n",
    ":class: tip\n",
    "En este momento podemos rechazar la cadena $C-H$ frente a $H-H$, ya que la primera tiene menos probabilidad de ocurrir. Aquí radica la potencia del algoritmo de Viterbi: dejamos de calcular las probabilidades de todas las cadenas sucesivas a $C-H$.\n",
    "\n",
    "```\n",
    "\n",
    "\n",
    "```{note}\n",
    "Esta selección se hace a __nivel de nodo__: rechazamos uno de los caminos que __llegan a un nodo__.\n",
    "```\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.3\n",
    "<img src=\"./images/hmm3.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "improving-leone",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "+ Probabilidad de la cadena $H-C$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(H)\\cdot P(H\\rightarrow C)\\cdot E(C\\rightarrow W) = 0.36 \\times 0.4 \\times 0.4 = 0.0576\n",
    "$$\n",
    "\n",
    "\n",
    "+ Probabilidad de la cadena $C-C$:\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_1(C)\\cdot P(C\\rightarrow C)\\cdot E(C\\rightarrow W) = 0.08 \\times 0.7 \\times 0.4 = 0.0224\n",
    "$$\n",
    "\n",
    "\n",
    "> Igualmente, aquí podemos descartar la cadena $C-C$ frente a $H-C$ (y todas las sucesivas cadenas de $C-C$).\n",
    "\n",
    "Hemos rechazado los caminos $C-C$ y $C-H$, por tanto podemos rechazar que la cadena pasa por nodo $C$ en la primera etapa. \n",
    "\n",
    "> En cada caso: tomamos el camino de __mayor probabilidad__ y marcamos el nodo.\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.4\n",
    "<img src=\"./images/hmm4.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "latter-anger",
   "metadata": {},
   "source": [
    "```{admonition} __Importante__\n",
    ":class: warning\n",
    "__NO__ podemos rechazar, no obstante, el nodo $C$ en la segunda etapa de la cadena (emisión $=W$). En este momento disponemos de la probabilidad que la cadena sea $H-C$ ó $H-H$, pero la cadena de Markov __continua__.\n",
    "```\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "> Continuamos el proceso hasta la última etapa de la cadena: calculamos las probabilidades de las cadenas restantes, que son:\n",
    "+ $H-H-H$\n",
    "+ $H-H-C$\n",
    "+ $H-C-H$\n",
    "+ $H-C-C$\n",
    "\n",
    "\n",
    "```{note}  \n",
    "En la tercera etapa el estado observado es $M$\n",
    "```\n",
    "\n",
    "Partimos de esta situación\n",
    "\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2=\\left(\n",
    "\\stackrel{\\color{red}{H}}{\\color{red}{0.065}}\n",
    ",\\quad \n",
    "\\stackrel{\\color{blue}{C}}{\\color{blue}{0.0058}}\n",
    "\\right)\n",
    ",\\quad\n",
    "P=\n",
    "\\stackrel{H\\quad C}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.4 \\\\\n",
    "    0.3 & 0.7 \n",
    "    \\end{bmatrix}%\n",
    "  },\\quad\n",
    "E=\n",
    "\\begin{matrix}\n",
    "H \\\\C\n",
    "\\end{matrix}\n",
    "\\stackrel{G\\quad \\color{orange}{W}\\quad M}{%\n",
    "    \\begin{bmatrix}\n",
    "    0.6 & 0.3 & 0.1 \\\\\n",
    "    0.2 & 0.4 & 0.4 \n",
    "    \\end{bmatrix}%\n",
    "  }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "sized-rhythm",
   "metadata": {},
   "source": [
    "+ Probabilidad de la cadena $H-H-H$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2(H)\\cdot P(H\\rightarrow H)\\cdot E(H\\rightarrow M) = 0.065\\times 0.6 \\times 0.1 = 0.0039\n",
    "$$\n",
    "\n",
    "+ Probabilidad de la cadena $H-H-C$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2(H)\\cdot P(H\\rightarrow C)\\cdot E(C\\rightarrow M) = 0.065\\times 0.4 \\times 0.4 = 0.0104\n",
    "$$\n",
    "\n",
    "\n",
    "+ Probabilidad de la cadena $H-C-H$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2(C)\\cdot P(C\\rightarrow H)\\cdot E(H\\rightarrow M) = 0.058 \\times 0.3 \\times 0.1 = 0.00174\n",
    "$$\n",
    "\n",
    "+ Probabilidad de la cadena $H-C-C$\n",
    "\n",
    "$$\n",
    "\\mathcal{L}_2(C)\\cdot P(C\\rightarrow C)\\cdot E(C\\rightarrow M) = 0.058\\times 0.7\\times 0.4 = 0.01624\n",
    "$$\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.5\n",
    "<img src=\"./images/hmm5.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "occasional-masters",
   "metadata": {},
   "source": [
    "> Observaciones\n",
    "\n",
    "+ De los dos caminos que llegan a $H$ en la tercera etapa, el más probable es el que viene de $C$, por tanto descartamos el camino $H-H-H$.\n",
    "\n",
    "\n",
    "+ De los dos caminos que llegan a $C$ en la tercera etapa, el más probable es el que proviene de $C$ en la segunda. Descartamos el camino $H-H-C$.\n",
    "\n",
    "\n",
    "+ Las dos observaciones anteriores hacen que descartemos el estado $H$ en la segunda etapa. Esto no es trivial, pues si hubiésemos detenido el proceso en dicha etapa, el último estado más probable hubiera sido $H$!\n",
    "\n",
    "\n",
    "+ Los dos posibles caminos son\n",
    "    + $H-C-H$ con una probabilidad de 0.001728\n",
    "    + $H-C-C$ con una probabilidad de 0.0161\n",
    "    \n",
    "    \n",
    "El último estado es el de mayor verosimilitud: $H-C-C$.\n",
    "\n",
    ":::{figure-md} markdown-fig.4.05.6\n",
    "<img src=\"./images/hmm6.png\" width=\"600px\">\n",
    "\n",
    "Viterbi\n",
    ":::"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "registered-quilt",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

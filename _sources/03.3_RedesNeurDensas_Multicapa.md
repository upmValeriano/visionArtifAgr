# Red densa multicapa

## Introducción

El __algoritmo de retropropagación que permite entrenar una red multicapa__ se introduce en 1970, pero no es hasta 1986 con el artículo de {cite:p}`rumelhart1986learning` cuando se aprecia su potencial. Permitiendo, por ejemplo, clasificar conjuntos donde las clases no están linealmente separadas. 

### Definición de un perceptrón multicapa o red densa

Se dispone de un conjunto de capas $\{0,..., l, ..., L\}$ donde __0__ es la capa de entrada, __L__ la de salida y el resto de capas $0<l<L$ son las capas intermedias u ocultas. Una capa $l-1$ tiene conectadas todas sus neuronas con cada una de las neuronas de la capa siguiente $l$, por tanto harán falta __L__ matrices de Pesos y Bias para configurar la red:

$$\{W^1, ..., W^l,...,W^L\}; \{B^1, ..., B^l,...,B^L\}$$

_En general bastarán con 1 o 2 capas ocultas para definir un perceptrón_.

<img src="images/RedMultiCapa.png" width="800px" align="center">

## Activación (forward)

El proceso de activación transforma una observación $\vec{x}$ en la probabilidad final de la red. Implica concatenar procesos de ponderación lineal y activación aplicados a la salida de la capa anterior:

$$\begin{pmatrix} \bar{z}^l \end{pmatrix}^T = W^l \cdot \begin{pmatrix} \bar{a}^{(l-1)} \end{pmatrix}^T + B^l$$
$$\bar{a}^l = f \begin{pmatrix} \bar{z}^l \end{pmatrix} $$


Siendo $\bar{a}^{(l-1)} = \bar{x}$ en la primera capa si $l=1$.

## Entrenamiento

El entrenamiento de un perceptrón multicapa requiere ajustar la configuración de cada capa por el gradiente de la función de pérdida o coste con respecto a los pesos y bias.

__Para justificar__ de una manera sencilla este proceso de __retropropagación__ se parte de un ejemplo lo más sencillo posible de red multicapa:

<img src="images/RedTriCapa.png" width="700px" align="center">

__Cuya configuración es__

$$  W^1 = \begin{pmatrix} w^1_{11} & w^1_{12} \\  w^1_{21} & w^1_{22} \end{pmatrix}   
  \phantom{abc} 
    B^1 = \begin{pmatrix} b^1_1 \\ b^1_2 \end{pmatrix}    \phantom{abc} 
    W^2 = \begin{pmatrix} w^2_{11} & w^2_{12} \\  w^2_{21} & w^2_{22} \end{pmatrix}    \phantom{abc} 
    B^2 = \begin{pmatrix} b^2_1 \\ b^2_2 \end{pmatrix} 
    $$

El __superindice__ se utiliza aquí en la notación para identificar el número de capa. Para no confudir, donde se emplee como elevado al cuadrado, aparecerá en negrita y con tamaño mayor.

### Entrenamiento en la capa 2

El entrenamiento que produce una observación $\bar{x}=(x_1, x_2)$ en los pesos de la segunda capa conduce a resultados similares a los obtenidos en el modelo bicapa. Para seguir la aplicación de la regla de la cadena las derivadas de la __función de pérdida__

$$C = \frac{1}{2} \begin{bmatrix} (a^2_1 - y_1)^ {\Large \textbf 2} + (a^2_2 - y_2)^ {\Large \textbf 2} \end{bmatrix}$$

se puede seguir hacia atrás el grafo de la red hasta las variables que se está derivando. 

<img src="images/RedTriCapa2.png" width="500px" align="center">

Para obtener $\frac{\partial C}{\partial w^2_{11}}$ se sigue el camino amarillo:

$$\frac{\partial C}{\partial w^2_{11}} = \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial z^2_1}\frac{\partial z^2_1}{\partial w^2_{11}} =  (a^2_1 - y_1) f'(z^2_1) a^1_1 = \delta^2_1 a^1_1 $$

De igual forma se obtendría

$$\frac{\partial C}{\partial w^2_{12}} = \delta^2_1 a^1_2 \phantom{abc} \frac{\partial C}{\partial b^2_1} = \delta^2_1$$

Y para obtener $\frac{\partial C}{\partial w^2_{21}}$ se sigue el camino verde:

$$\frac{\partial C}{\partial w^2_{21}} = \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial z^2_2}\frac{\partial z^2_2}{\partial w^2_{21}} =  (a^2_2 - y_2) f'(z^2_2) a^1_1 = \delta^2_2 a^1_1 $$

De igual forma se obtendría

$$\frac{\partial C}{\partial w^2_{22}} = \delta^2_2 a^1_2 \phantom{abc} \frac{\partial C}{\partial b^2_2} = \delta^2_2$$

__Lo que permite obtener los gradientes de la capa 2__:

$$\frac{\partial C}{\partial \bar{z}^2} = \bar{\delta}^2 = (\delta^2_1,\delta^2_2) = (\bar{a}^2 - \bar{y}) \odot f'(\bar{z}^2)$$ 

$$\frac{\partial C}{\partial W^2} = (\bar{\delta}^2)^T \cdot \bar{a}_1$$
$$\frac{\partial C}{\partial B^2} = (\bar{\delta}^2)^T $$

### Entrenamiento de la capa 1

__El entrenamiento de la capa 1__, por ejemplo para optimizar el peso $w^1_{11}$ requiere dos caminos simultáneos (verde y amarillo) como se puede comprobar en el siguiente gráfico:

<img src="images/RedTriCapa3.png" width="700px" align="center">


$$\frac{\partial C}{\partial w^1_{11}} = \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{11}} +  \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{11}} $$

Desarrollando cada uno de los sumandos:


$$ \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{11}} = 
 \frac{\partial C}{\partial a^2_1} 
 \frac{\partial a^2_1}{\partial z^2_1}
 \frac{\partial z^2_1}{\partial a^1_1}
 \frac{\partial a^1_1}{\partial z^1_1}
 \frac{\partial z^1_1}{\partial  w^1_{11}} $$

$$ \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{11}} = 
 \frac{\partial C}{\partial a^2_2} 
 \frac{\partial a^2_2}{\partial z^2_2}
 \frac{\partial z^2_2}{\partial a^1_1}
 \frac{\partial a^1_1}{\partial z^1_1}
 \frac{\partial z^1_1}{\partial  w^1_{11}} $$

Permite poner:

$$\frac{\partial C}{\partial w^1_{11}} = \delta^2_1 w^2_{11} f'(z^1_1) x_1 + \delta^2_2 w^2_{21} f'(z^1_1) x_1 = 
(\delta^2_1 w^2_{11} + \delta^2_2 w^2_{21}) f'(z^1_1)  x_1 = \delta^1_1 x_1$$

De igual forma se tiene:

$$\frac{\partial C}{\partial w^1_{12}} = \delta^1_1 x_2 \phantom{abc}  \frac{\partial C}{\partial b^1_1} = \delta^1_1$$

__Para optimizar el peso__ $w^1_{21}$ requiere también dos caminos simultáneos (verde y amarillo) como se puede comprobar en el siguiente gráfico:

<img src="images/RedTriCapa4.png" width="700px" align="center">


$$\frac{\partial C}{\partial w^1_{21}} = \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{21}} +  \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{21}} $$

Desarrollando cada uno de los sumandos:


$$ \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{21}} = 
 \frac{\partial C}{\partial a^2_1} 
 \frac{\partial a^2_1}{\partial z^2_1}
 \frac{\partial z^2_1}{\partial a^1_2}
 \frac{\partial a^1_2}{\partial z^1_2}
 \frac{\partial z^1_2}{\partial  w^1_{21}} $$

$$ \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{21}} = 
 \frac{\partial C}{\partial a^2_2} 
 \frac{\partial a^2_2}{\partial z^2_2}
 \frac{\partial z^2_2}{\partial a^1_2}
 \frac{\partial a^1_2}{\partial z^1_2}
 \frac{\partial z^1_2}{\partial  w^1_{21}} $$

Permite poner:

$$\frac{\partial C}{\partial w^1_{21}} = \delta^2_1 w^2_{12} f'(z^1_2) x_1 + \delta^2_2 w^2_{22} f'(z^1_2) x_1 = 
(\delta^2_1 w^2_{12} + \delta^2_2 w^2_{22}) f'(z^1_2)  x_1 = \delta^1_2 x_1$$

De igual forma se tiene:

$$\frac{\partial C}{\partial w^1_{22}} = \delta^1_2 x_2 \phantom{abc}  \frac{\partial C}{\partial b^1_2} = \delta^1_2$$

__Para optimizar el peso__ $w^1_{21}$ requiere también dos caminos simultáneos (verde y amarillo) como se puede comprobar en el siguiente gráfico:

<img src="images/RedTriCapa4.png" width="700px" align="center">


$$\frac{\partial C}{\partial w^1_{21}} = \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{21}} +  \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{21}} $$

Desarrollando cada uno de los sumandos:


$$ \frac{\partial C}{\partial a^2_1} \frac{\partial a^2_1}{\partial w^1_{21}} = 
 \frac{\partial C}{\partial a^2_1} 
 \frac{\partial a^2_1}{\partial z^2_1}
 \frac{\partial z^2_1}{\partial a^1_2}
 \frac{\partial a^1_2}{\partial z^1_2}
 \frac{\partial z^1_2}{\partial  w^1_{21}} $$

$$ \frac{\partial C}{\partial a^2_2} \frac{\partial a^2_2}{\partial w^1_{21}} = 
 \frac{\partial C}{\partial a^2_2} 
 \frac{\partial a^2_2}{\partial z^2_2}
 \frac{\partial z^2_2}{\partial a^1_2}
 \frac{\partial a^1_2}{\partial z^1_2}
 \frac{\partial z^1_2}{\partial  w^1_{21}} $$

Permite poner:

$$\frac{\partial C}{\partial w^1_{21}} = \delta^2_1 w^2_{12} f'(z^1_2) x_1 + \delta^2_2 w^2_{22} f'(z^1_2) x_1 = 
(\delta^2_1 w^2_{12} + \delta^2_2 w^2_{22}) f'(z^1_2)  x_1 = \delta^1_2 x_1$$

De igual forma se tiene:

$$\frac{\partial C}{\partial w^1_{22}} = \delta^1_2 x_2 \phantom{abc}  \frac{\partial C}{\partial b^1_2} = \delta^1_2$$

## Retropropagación

__Resumiendo, el gradiente $\bar{\delta}^1$ se puede poner__:

$$\bar{\delta}^1 = (\delta^1_1, \delta^1_2) = (\delta^2_1 w^2_{11} + \delta^2_2 w^2_{21},  \delta^2_1 w^2_{12} + \delta^2_2 w^2_{22}) \odot f'(z^1_1, z^1_2)
$$

$$ \begin{bmatrix} \bar{\delta}^1 \end{bmatrix}^T= \begin{bmatrix}  \begin{pmatrix} w^2_{11} & w^2_{21} \\  w^2_{12} & w^2_{22} \end{pmatrix} \begin{pmatrix}\delta^2_1 \\ \delta^2_2 \end{pmatrix} \end{bmatrix}  \odot f'\begin{pmatrix} \begin{bmatrix} \bar{z}^1 \end{bmatrix}^T \end{pmatrix} = 
(W^2)^T \cdot \begin{bmatrix} \bar{\delta}^2     \end{bmatrix} ^T \odot f'\begin{pmatrix} \begin{bmatrix} \bar{z}^1 \end{bmatrix}^T \end{pmatrix}$$

Que también se puede poner como:

$$ \bar{\delta}^1 =  
\begin{bmatrix} \bar{\delta}^2 \cdot  W^2  \end{bmatrix} \odot f'\begin{pmatrix}\bar{z}^1\end{pmatrix}$$

__Graficamente__ se puede observar que el gradiente $\bar{\delta}$ se transmite desde la última capa hasta la capa 1 __funcionando la red densa de forma inversa__. Está transformación lineal para pasar de $\bar{\delta}^2$ a $\bar{\delta}^1$ tiene lugar con una matriz $W^2$ __traspuesta__ con respecto al proceso de activación.

<img src="images/RedTriCapa5.png" width="600px" align="center">

__Finalmente__ las formulas que permiten obtener los __gradientes__ $\bar{\delta}$ en una capa $l < L$ para una observación $\bar{x}$ o para un bloque de observaciones $X$ es:

$$ ( \bar{\delta}^l )^T =  (W^{l+1})^T \cdot (\bar{\delta}^{l+1} )^T \odot f'\begin{bmatrix} (z^l)^T \end{bmatrix} $$

que puede adoptar también la siguiente forma:

$$\bar{\delta}^l = \bar{\delta}^{l+1} \cdot W^{l+1} \odot f'(z^l) $$



```{note}

Los gradientes delta se transmiten desde la capa superior a partir de la matriz de pesos transpuesta. La transposición hace que el sentido de transmisión en la red sea el opuesto.
```

::::{grid}
:gutter: 3

:::{grid-item-card} Activación (forward)
$$\begin{pmatrix} \bar{z}^l \end{pmatrix}^T = W^l \cdot \begin{pmatrix} \bar{a}^{(l-1)} \end{pmatrix}^T + B^l$$

$$\bar{a}^l = f \begin{pmatrix} \bar{z}^l \end{pmatrix}$$
:::

:::{grid-item-card} Retro-Propagación (backward)
$$( \bar{\delta}^l )^T =  (W^{l+1})^T \cdot (\bar{\delta}^{l+1} )^T \odot f'\begin{bmatrix} (z^l)^T \end{bmatrix}$$
$$\bar{\delta}^l = \bar{\delta}^{l+1} \cdot W^{l+1} \odot f'(z^l) $$
:::

:::{grid-item-card} Entrenamiento
$$  W^l[t+1] = W^l[t] - \eta \cdot    \begin{pmatrix} \bar{\delta}^l \end{pmatrix} ^T \cdot \bar{a}^{(l-1)} $$
$$  B^l[t+1] = B^l[t] - \eta  \cdot \begin{pmatrix} \bar{\delta}^l \end{pmatrix} ^T  $$
:::


::::




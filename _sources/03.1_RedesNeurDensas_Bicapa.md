# Red densa simple

## Neurona de McCulloch y Pitts

{cite:p}`McCulloch_Pitts_1943` propusieron el primer modelo de neurona artificial denominado __TLU__ (Threshold Logic Unit) o LTU (Linear Threshold Unit), unidad de umbral lineal. A esta neurona artificial también se le denomina __Perceptrón__.

El modelo parte de $n$ entradas $(x_1, x_2, ..., x_j, ..., x_n)$ a la que se aplica una ponderación lineal más un umbral (que suele denominarse sesgo o bias, $b$):

$$z=b+\displaystyle\sum_{i=1}^n w_{i}x_i$$

A la que se aplica una función de activación no lineal, que en el modelo de McCulloch y Pitts es una función escalonada del tipo 

$$a=f(z) \left \{ \begin{array}{c} 1 &  z \ge 0 \\ 0 & z <0 \end{array} \right .  $$

Resultando un modelo con salida digital o binaria. Las entradas $(x_1, x_2, ..., x_j, ..., x_n)$  equivalen a las dendritas, el parámetro $b$ se denomina umbral o bias y la salida  $a$ es el axón. Durante el entrenamiento se compara $a$ con los valores reales que se encuentran en $y$ 

<img src="images/McCulloch-Pitts.png" width="600px" align="center">


_____

## El perceptrón simple

{cite:p}`rosenblatt1958perceptron` introdujo el __perceptrón simple__ formado por dos capas, una de __entrada con n neuronas__ y una de __salida con m neuronas__. {cite:p}`widrow1960adaptive` introdujeron el modelo __ADAptative LInear Neuron (Adaline)__ tambien bicapa. 

### Activación de una observación $\bar{x}$ por efecto de la red

Dada __1 observación__ $\bar{x}=(x_1,...,x_j, ..., x_n)$ de la que se conoce su __clase__ en formato _one-hote_ $\bar{y}=(y_1,...,y_i, ...y_m)$ 

Primero se efectua una __ponderación lineal__ $\bar{z}=(z_1,...,z_i, ..., z_m)$ de la que se obtiene la __salida__ $\bar{a}$ aplicando una __función de activación__: 

$$\bar{a}=f(\bar{z})=(a_1,...,a_i, ..., a_m)$$

Se puede decir que existen $n$ __neuronas de entrada__ y $m$ __neuronas de salida__. La ponderación lineal en las neuronas de salida a partir de los pesos se calcula:

$$z_i = b_i + \displaystyle\sum_{j=1}^n w_{ij}x_j \quad (i=1...m)$$

Hacen falta $n \times m$ pesos y $m$ bias para definir la red, que puesto en forma matricial:

$$ W = \begin{bmatrix}{w_{11}}&{w_{12}}&{...}&{w_{1m}}\\{w_{21}}&{w_{22}}&{...}&{w_{2m}}\\{...}&{...}&{...}&{...}\\{w_{n1}}&{w_{n2}}&{...}&{w_{nm}}\end{bmatrix}  \quad B = \begin{bmatrix}{b_{1}}&{b_{2}}&{...}&{b_{m}}\end{bmatrix} $$

__Siendo__

$$ \bar{z} = \bar{x} \cdot W + B $$ 

_____

<img src="images/Neurona-2Capas.png" width="800px" align="center">

_____

### Entrenamiento que produce en la red una observación $\bar{x}$

Se supone el siguiente __ejemplo sencillo__ de red bicapa

<img src="images/RedBiCapa.png" width="400px" align="center">

La red se puede configurar con las __matrices de pesos y bias__ siguientes:

$$W = \begin{bmatrix} w_{11} & w_{12} \\ w_{21} & w_{22}  \\ w_{31} & w_{32} \end{bmatrix}  \phantom{abc}  B = \begin{bmatrix} b_1 & b_2 \end{bmatrix} $$

Se conoce __una observación__ $\bar{x}=(x_1, x_2, x_3)$ de la que se sabe su __clase en formato one-hot__ $\bar{y}=(y_1, y_2)$. Una vez __activada__ la red se obtiene $\bar{a}=(a_1, a_2)$. Usando el error al cuadrado, la __pérdida__ de esta observación será:

$$C = \frac{1}{2} \begin{bmatrix} (a_1-y_1)^2 + (a_2-y_2)^2 \end{bmatrix}$$

Se sabe que:

$$\begin{cases} z_1 = w_{11} x_1 + w_{12} x_2 + w_{13} x_3 + b_1  \\ z_2  = w_{21} x_1 + w_{22} x_2 + w_{23} x_3 + b_2 \end{cases} 
\rightarrow \begin{cases} a_1 = f(z_1) \\ a_2 = f(z_2) \end{cases}  $$

La __tasa de variación de la función de pérdida__ con respecto al peso $w_{11}$ afecta a la __primera neurona de salida__, por lo tanto se calcula aplicando la regla de la cadena así:

$$\frac{\partial{C}}{\partial{w_{11}}} = \frac{\partial{C}}{\partial{a_1}} \frac{\partial{a_1}}{\partial{z_1}}  \frac{\partial{z_1}}{\partial{w_{11}}} $$

Por tanto

$$\frac{\partial{C}}{\partial{w_{11}}} = (a_1 - y_1) f'(z_1) x_1$$

Llamando $\delta_1 = (a_1 - y_1) f'(z_1)$ __las tasas de variación__ respecto a la configuración que aplica sobre $z_1$ es:

 $$ \frac{\partial{C}}{\partial{w_{11}}} = \delta_1 x_1; \phantom{abc} \frac{\partial{C}}{\partial{w_{12}}} = \delta_1 x_2; 
      \phantom{abc} \frac{\partial{C}}{\partial{w_{13}}} = \delta_1 x_3; \phantom{abc} \frac{\partial{C}}{\partial{b_1}} = \delta_1$$

__Repitiendo el proceso__ con respecto al peso $w_{21}$ afecta a la __segunda neurona de salida__ (término $a_2$):

$$\frac{\partial{C}}{\partial{w_{21}}} = \frac{\partial{C}}{\partial{a_2}} \frac{\partial{a_2}}{\partial{z_2}}  \frac{\partial{z_2}}{\partial{w_{21}}} $$
$$\frac{\partial{C}}{\partial{w_{21}}} = (a_2 - y_2) f'(z_2) x_1$$

Llamando $\delta_2 = (a_2 - y_2) f'(z_2)$ __las tasas de variación__ respecto a la configuración que aplica sobre $z_2$ es:

 $$ \frac{\partial{C}}{\partial{w_{21}}} = \delta_2 x_1; \phantom{abc} \frac{\partial{C}}{\partial{w_{22}}} = \delta_2 x_2; 
      \phantom{abc} \frac{\partial{C}}{\partial{w_{23}}} = \delta_2 x_3; \phantom{abc} \frac{\partial{C}}{\partial{b_2}} = \delta_2$$

__El gradiente__ que optimiza la configuración de la red por el impacto de la observación $\bar{x}$ será:

$$ \frac{\partial{C}}{\partial{W}}  = 
    \begin{bmatrix}  
    \frac{\partial{C}}{\partial{w_{11}}} & \frac{\partial{C}}{\partial{w_{12}}} \\
    \frac{\partial{C}}{\partial{w_{21}}} & \frac{\partial{C}}{\partial{w_{22}}} \\
    \frac{\partial{C}}{\partial{w_{31}}} & \frac{\partial{C}}{\partial{w_{32}}} 
    \end{bmatrix} =
    \begin{bmatrix} \delta_1 x_1 & \delta_2 x_1  \\
                    \delta_1 x_2 & \delta_2 x_2 \\
                    \delta_1 x_3 & \delta_2 x_3  \end{bmatrix} = 
    \begin{bmatrix} \delta_1 & \delta_2  \end{bmatrix} \begin{bmatrix} x_1 \\ x_2 \\ x_3  \end{bmatrix}   
    =  \bar{\delta}  \cdot \begin{pmatrix}\bar{x} \end{pmatrix} ^T
$$

Y __también__

$$ \frac{\partial{C}}{\partial{B}} = \begin{bmatrix}  \frac{\partial{C}}{\partial{b_1}}  &  \frac{\partial{C}}{\partial{b_2}}   \end{bmatrix} = 
\begin{bmatrix} \delta_1 & \delta_2 \end{bmatrix} = \bar{\delta} $$ 

__Además__ $\bar{\delta}$ se puede poner vectorialmente:

$$\bar{\delta} = (\delta_1, \delta_2) = \begin{pmatrix} (a_1 - y_1)f'(z_1), (a_2 - y_2)f'(z_2) \end{pmatrix}$$

De la que se obtiene la __expresión final__:

$$\bar{\delta} = (\bar{a} - \bar{y}) \odot f'(\bar{z})$$

Donde $\bar{a} - \bar{y}$ es el __error neto__ y $\odot$ es el __producto de hadamard__ que obtiene a partir de 2 vectores un nuevo vector donde sus compenentes son el producto componente a componente.

__Indicar como comentario final__ que además los gradientes $ \frac{\partial{C}}{\partial{W}}$ y $ \frac{\partial{C}}{\partial{B}}$ que optimizan la configuración de la red, se tiene que $\bar{\delta}$ es un gradiente intermedio de $C$ con respecto a $\bar{z}$. Efectivamente:

$$\bar{\delta}=(\delta_1, \delta_2) = (\frac{\partial{C}}{\partial{z_1}}, \frac{\partial{C}}{\partial{z_2}}) = \frac{\partial{C}}{\partial{\bar{z}}} $$


## Resumen del modelo

<h3 style="color:blue;">Resumen del entrenamiento de 1 observación</h3>

$$  W[t+1] = W[t] - \eta \cdot    \begin{pmatrix} \bar{\delta} \end{pmatrix} ^T \cdot \bar{x} $$
$$  B[t+1] = B[t] - \eta  \cdot \begin{pmatrix} \bar{\delta} \end{pmatrix} ^T  $$

<h4 style="color:green;">Gradientes asociados</h4>

$$\frac{\partial C}{\partial W}=\begin{pmatrix} \bar{\delta} \end{pmatrix} ^T \cdot \bar{x};   \phantom{abc}  \frac{\partial C}{\partial B}=\begin{pmatrix} \bar{\delta} \end{pmatrix} ^T ;  \phantom{abc} \frac{\partial C}{\partial Z}=\begin{pmatrix} \bar{\delta} \end{pmatrix} ^T  $$

## Comentarios Finales

Una de las funciones de activación primeras que se usaron fuero la sigmoidea o lógistica 

$$f(z)=\sigma(z)=\frac{1}{1+e^{-z}}$$

El __gradiente descenso__ se basa en que el __vector gradiente__ define en el dominio de una función de varias variables $f(\bar{x})$ la dirección de máximo incremento de la pendiente. De forma que se toma __signo negativo__ para ir hacia un mínimo local y un valor de magnitud reducida $\eta$ que evite saltar el mínimo por un avance excesivo.

Así partiendo de un valor aleatorio $\bar{x} = \bar{x}_0$ se busca el mínimo local a través de sucesivas iteraciones:

$$\bar{x}_{k+1} = \bar{x}_{k} - \eta \nabla f(\bar{x}_{k})$$


<img src="images/gradiente-descenso.png" width="400px" align="center">

